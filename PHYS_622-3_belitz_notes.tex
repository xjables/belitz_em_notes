%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[oneside,english]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose,tmargin=2.5cm,bmargin=2cm,lmargin=2.5cm,rmargin=2.5cm}
\setcounter{secnumdepth}{3}
\usepackage{babel}
\usepackage{array}
\usepackage{varioref}
\usepackage{float}
\usepackage{fancybox}
\usepackage{calc}
\usepackage{units}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cancel}
\usepackage{wasysym}
\usepackage{esint}
\usepackage[unicode=true]
 {hyperref}
\usepackage{breakurl}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
  \theoremstyle{definition}
  \newtheorem{example}{\protect\examplename}
  \theoremstyle{remark}
  \newtheorem{rem}{\protect\remarkname}
  \theoremstyle{remark}
  \newtheorem{claim}{\protect\claimname}
 \ifx\proof\undefined\
   \newenvironment{proof}[1][\proofname]{\par
     \normalfont\topsep6\p@\@plus6\p@\relax
     \trivlist
     \itemindent\parindent
     \item[\hskip\labelsep
           \scshape
       #1]\ignorespaces
   }{%
     \endtrivlist\@endpefalse
   }
   \providecommand{\proofname}{Proof}
 \fi
  \theoremstyle{definition}
  \newtheorem{defn}{\protect\definitionname}
  \theoremstyle{plain}
  \newtheorem{prop}{\protect\propositionname}
  \theoremstyle{plain}
  \newtheorem{thm}{\protect\theoremname}
  \theoremstyle{plain}
  \newtheorem{cor}{\protect\corollaryname}
  \theoremstyle{plain}
  \newtheorem{lem}{\protect\lemmaname}
\newenvironment{lyxlist}[1]
{\begin{list}{}
{\settowidth{\labelwidth}{#1}
 \setlength{\leftmargin}{\labelwidth}
 \addtolength{\leftmargin}{\labelsep}
 \renewcommand{\makelabel}[1]{##1\hfil}}}
{\end{list}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{mdframed}
\usepackage{xcolor}
\usepackage{pstricks}
\usepackage{bbm}
\usepackage{tensor}
\usepackage{refstyle}
\usepackage{arydshln,leftidx,mathtools}
\usepackage{siunitx}
\usepackage{cleveref}

\crefname{thm}{Theorem}{Theorems}
\Crefname{thm}{Theorem}{Theorems}
\crefname{lem}{Lemma}{Lemmas}
\Crefname{lem}{Lemma}{Lemmas}
\crefname{rem}{Remark}{Remarks}
\Crefname{rem}{Remark}{Remarks}
\crefname{claim}{Claim}{Claims}
\Crefname{claim}{Claim}{Claims}
\crefname{example}{Example}{Examples}
\Crefname{example}{Example}{Examples}
\crefname{cor}{Corollary}{Corollaries}
\Crefname{cor}{Corollary}{Corollaries}
\crefname{prop}{Proposition}{Propositions}
\Crefname{prop}{Proposition}{Propositions}
\crefname{chapter}{Ch.}{Chapters}
\Crefname{chapter}{Ch.}{Chapters}
\crefname{section}{\S}{\S\S}
\Crefname{section}{\S}{\S\S}
\crefname{subsection}{\S}{\S\S}
\Crefname{subsection}{\S}{\S\S}

\renewcommand*\thesection{\arabic{section}}

\surroundwithmdframed[
linewidth	=	6pt,
linecolor	=	gray!40,
topline	=	false,
bottomline	=	false,
rightline	=	false,
]{example}

\surroundwithmdframed[
linewidth	=	1.25pt,
shadow=true,
shadowcolor=gray!50,
]{prop}

\surroundwithmdframed[
linewidth	=	2pt,
roundcorner 	= 	10pt,
shadow=true,
shadowcolor=black,
]{thm}

\surroundwithmdframed[
linewidth	=	1.25pt,
linecolor	=	gray!75,
shadow=true,
shadowcolor=gray!25,
]{claim}

\surroundwithmdframed[
linewidth	=	1.25pt,
shadow=true,
shadowcolor=gray!50,
]{cor}

\surroundwithmdframed[
linewidth	=	1.25pt,
linecolor	=	gray!75,
shadow=true,
shadowcolor=gray!25,
]{lem}

\surroundwithmdframed[
linecolor	=	gray!75,
linewidth	=	1pt,
topline	=	false,
]{proof}

\surroundwithmdframed[
backgroundcolor	= 	gray!20,
linewidth		=	0pt,
]{defn}



\makeatletter
\@addtoreset{thm}{subsection}
\makeatother

\makeatletter
\@addtoreset{example}{subsection}
\makeatother

\makeatletter
\@addtoreset{prop}{subsection}
\makeatother

\makeatletter
\@addtoreset{defn}{subsection}
\makeatother

\makeatletter
\@addtoreset{lem}{subsection}
\makeatother

\makeatletter
\@addtoreset{cor}{subsection}
\makeatother

\makeatletter
\@addtoreset{claim}{subsection}
\makeatother

\makeatletter
\@addtoreset{rem}{subsection}
\makeatother

\makeatother

\providecommand{\claimname}{Claim}
\providecommand{\corollaryname}{Corollary}
\providecommand{\definitionname}{Definition}
\providecommand{\examplename}{Example}
\providecommand{\lemmaname}{Lemma}
\providecommand{\propositionname}{Proposition}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}

\begin{document}
\global\long\def\understuff#1#2{\underset{#2}{\underbrace{#1}}}
\global\long\def\summation#1#2{\overset{#2}{\underset{#1}{\sum}}}
\global\long\def\infinitesimal#1{d#1\:}
\global\long\def\indefintfrac#1#2{\int\frac{d#1}{#2}\:}
\global\long\def\indefint#1{\int\infinitesimal{#1}}
\global\long\def\defint#1#2#3{\int_{#2}^{#3}\infinitesimal{#1}}



\title{Physics 622}


\title{Classical Electrodynamics}


\author{Transcribed lecture notes of Dr. Dietrich Belitz}

\maketitle
\tableofcontents{}


\chapter{Mathematical preliminaries\label{chap:Mathematical-preliminaries}}


\section{Vector spaces and tensor spaces}


\subsection{Vector spaces}

Let $V$ be an $n$-dimensional vector space over $\mathbb{R}$.

We say that $V$ has a set of \emph{basis vectors} 
\[
\left\{ \boldsymbol{e}_{j};j=1,\ldots,n\right\} 
\]
 and \emph{elements} (that is, \emph{vectors}) $\boldsymbol{x}$ expanded
in this basis as\footnote{I have adopted the notation that vectors are bold.}
\[
\boldsymbol{x}=\overset{n}{\underset{j=1}{\sum}}x^{j}\boldsymbol{e}_{j}=:x^{j}\boldsymbol{e}_{j}\quad,\quad x^{j}\in\mathbb{R},
\]
 where the scalars $x^{j}$ are called the \emph{coordinates }or \emph{components}
of $\boldsymbol{x}$.
\begin{example}
Define the set
\[
\mathbb{R}^{n}:=\mathbb{R}\times\cdots\times\mathbb{R}=\left\{ \left(x^{1},\ldots,x^{n}\right);x^{j}\in\mathbb{R}\right\} .
\]
 $\mathbb{R}^{n}$ constitutes a vector space over $\mathbb{R}$ if
vector addition and scalar multiplication are defined to be the standard
real vector addition and real scalar multiplication.

Furthermore, the \emph{Cartesian basis} is
\[
\left\{ \boldsymbol{e}_{1}=\left(1,0,\ldots,0\right),\boldsymbol{e}_{2}=\left(0,1,0,\ldots,0\right),\ldots,\boldsymbol{e}_{n}=\left(0,\ldots,0,1\right)\right\} .
\]
\end{example}
\begin{rem}
Two vector spaces $V$ and $W$ over the same field $F$ are said
to be \emph{isomorphic}, denoted $V\cong W$,\emph{ }iff there exists
a bijection $T:V\rightarrow W$ that preserves addition and scalar
multiplication. That is, 
\[
\begin{alignedat}{1}T\left(\boldsymbol{x}\boldsymbol{+}\boldsymbol{y}\right)= & T\left(\boldsymbol{x}\right)+T\left(\boldsymbol{y}\right)\text{, and}\\
T\left(c\boldsymbol{x}\right)= & cT(\boldsymbol{x})
\end{alignedat}
\]
 for all $\boldsymbol{x},\boldsymbol{y}\in V$ and all $c\in F$.\footnote{Here, and throughout this document, one must be mindful of what type
of variable and what type of operation is written, because often the
same symbols are used for addition between vectors and addition between
scalars. In this case, $\boldsymbol{x}+\boldsymbol{y}$ is vector
addition, $c\boldsymbol{x}$ is scalar-vector multiplication, $T\left(\boldsymbol{x}\right)+T\left(\boldsymbol{y}\right)$
is scalar addition, and $cT(\boldsymbol{x})$ is scalar-scalar multiplication.}\end{rem}
\begin{claim}
All $n$-dimensional vector spaces over $\mathbb{R}$ are isomorphic
to $\mathbb{R}^{n}$.\end{claim}
\begin{proof}
In fact, \emph{all} finite-dimensional vector spaces of the same dimension
and over the same field are isomorphic to one another. \href{http://math.kennesaw.edu/~sellerme/sfehtml/classes/math3260/isomorphicvectorspaces.pdf}{See Theorem 9 of this document}.
\end{proof}

\subsection{Tensor spaces\label{sub:ch1sec1_2Tensor-spaces}}

Let $V$ be an $n$-dimensional vector space over $\mathbb{R}$ with
basis $\left\{ \boldsymbol{e}_{j}\right\} $.
\begin{defn}
\textbf{\emph{Linear forms.}} A mapping $f:V\rightarrow\mathbb{R}$
is called a \emph{linear form} iff
\[
\begin{alignedat}{2}\left(\textrm{i}\right)\quad & f\left(\boldsymbol{x}\boldsymbol{+}\boldsymbol{y}\right) & = & f\left(\boldsymbol{x}\right)+f\left(\boldsymbol{y}\right)\\
\left(\text{ii}\right)\quad & f\left(c\boldsymbol{x}\right) & = & cf\left(\boldsymbol{x}\right)
\end{alignedat}
\]
 for all $\boldsymbol{x},\boldsymbol{y}\in V$ and all $c\in\mathbb{R}$.
\end{defn}
~
\begin{defn}
\textbf{\emph{Bilinear forms.}}\textbf{ }A mapping $f:V\times V\rightarrow\mathbb{R}$
is called a \emph{bilinear form} iff

\[
\begin{alignedat}{2}\left(\textrm{i}\right)\quad & f\left(\boldsymbol{x+y},\boldsymbol{z}\right) & = & f\left(\boldsymbol{x},\boldsymbol{z}\right)+f\left(\boldsymbol{y},\boldsymbol{z}\right)\\
\left(\textrm{ii}\right)\quad & f\left(\boldsymbol{x},\boldsymbol{y+z}\right) & = & f\left(\boldsymbol{x},\boldsymbol{y}\right)+f\left(\boldsymbol{x},\boldsymbol{z}\right)\\
\left(\text{iii}\right)\quad & f\left(c\boldsymbol{x},\boldsymbol{y}\right) & = & cf\left(\boldsymbol{x},\boldsymbol{y}\right)\\
\left(\text{iv}\right)\quad & f\left(\boldsymbol{x},c\boldsymbol{y}\right) & = & cf\left(\boldsymbol{x},\boldsymbol{y}\right)
\end{alignedat}
\]
 for all $\boldsymbol{x},\boldsymbol{y},\boldsymbol{z}\in V$ and
all $c\in\mathbb{R}$.
\end{defn}
~
\begin{defn}
\label{definition:1_3_covarianttensors}\textbf{\emph{Bilinear form
components.}} The scalars $t_{jk}:=f\left(\boldsymbol{e}_{j},\boldsymbol{e}_{k}\right)$
are called the \emph{coordinates} or \emph{components} of the bilinear
form $f$ in the basis $\left\{ \boldsymbol{e}_{j}\right\} $.\end{defn}
\begin{prop}
\label{prop:1The-coordinates-completely}The coordinates completely
determine a bilinear form.\end{prop}
\begin{proof}
Let $\boldsymbol{x},\boldsymbol{y}\in V$. Then
\[
f\left(\boldsymbol{x},\boldsymbol{y}\right)=f\left(x^{j}\boldsymbol{e}_{j},y^{k}\boldsymbol{e}_{k}\right)=x^{j}y^{k}f\left(\boldsymbol{e}_{j},\boldsymbol{e}_{k}\right)=t_{jk}x^{j}y^{k}.
\]
and we see that knowledge of $\left\{ t_{jk}\right\} $ implies knowledge
of $f\left(\boldsymbol{x},\boldsymbol{y}\right)$.\footnote{Notice the importance of $f$ obeying properties (i)-(iv) of a bilinear
form.}\end{proof}
\begin{defn}
\textbf{\emph{2-tensors.}} The $n^{2}$ scalars $t_{jk}$ are called
the coordinates of the \emph{rank-2 tensor }(or \emph{2-tensor})\emph{
}$t$ (which is equivalent to the bilinear form $f$).\end{defn}
\begin{claim}
\textbf{\emph{Symmetric forms.}} A bilinear form, $f$, is symmetric
if and only if the components of the tensor with respect to the given
basis are symmetric; that is, 
\[
f\left(\boldsymbol{x},\boldsymbol{y}\right)=f\left(\boldsymbol{y},\boldsymbol{x}\right)\,\forall\boldsymbol{x},\boldsymbol{y}\in V\quad\Leftrightarrow\quad t_{jk}=t_{kj}\,\forall j,k=1,\ldots,n
\]
\end{claim}
\begin{proof}
Assume $f\left(\boldsymbol{x},\boldsymbol{y}\right)=f\left(\boldsymbol{y},\boldsymbol{x}\right)\,\forall\boldsymbol{x},\boldsymbol{y}\in V$.
Then
\[
t_{jk}:=f\left(\boldsymbol{e}_{j},\boldsymbol{e}_{k}\right)=f\left(\boldsymbol{e}_{k},\boldsymbol{e}_{j}\right)=t_{kj}\,\forall j,k=1,\ldots,n.
\]
Now assume $t_{jk}=t_{kj}\,\forall j,k=1,\ldots,n$. Then 
\[
f\left(e_{j},e_{k}\right)=f\left(e_{k},e_{j}\right)\,\forall j,k=1,\ldots,n.
\]
 Let $\boldsymbol{x},\boldsymbol{y}\in V$. These can be expanded
as $\boldsymbol{x}=x^{j}\boldsymbol{e}_{j}$ and $\boldsymbol{y}=y^{j}\boldsymbol{e}_{j}$.
Thus,
\[
\begin{alignedat}{1}x^{j}y^{k}f\left(\boldsymbol{e}_{j},\boldsymbol{e}_{k}\right)= & x^{j}y^{k}f\left(\boldsymbol{e}_{k},\boldsymbol{e}_{j}\right)\\
\implies f\left(x^{j}\boldsymbol{e}_{j},y^{k}\boldsymbol{e}_{k}\right)= & f\left(y^{k}\boldsymbol{e}_{k},x^{j}\boldsymbol{e}_{j}\right)\\
\implies f\left(\boldsymbol{x},\boldsymbol{y}\right)= & f\left(\boldsymbol{y},\boldsymbol{x}\right).
\end{alignedat}
\]
\end{proof}
\begin{thm}
The set of rank-2 tensors forms a vector space of dimension $n^{2}$
over $\mathbb{R}$.\end{thm}
\begin{proof}
(Problem \#3)
\end{proof}
In a similar manner to how we constructed 2-tensors, one can consider
\emph{multilinear forms} $f:V\times V\times V\rightarrow\mathbb{R}$,
$f:V\times V\times V\times V\rightarrow\mathbb{R}$, etc. to construct
tensors of rank 3, 4, etc. with coordinates $t_{jkl}$, $t_{jklm}$,
etc. Having defined tensors in this manner, let us consider some commonly
encountered tensors.
\begin{example}
\label{example:def_levicivita}\textbf{\emph{The Levi-Civita tensor.}}
Consider $\mathbb{R}^{3}$ with its Cartesian basis $\left\{ e_{1},e_{2},e_{3}\right\} $.
The \emph{Levi-Civita tensor} (or \emph{completely antisymmetric tensor})
is the rank-3 tensor $\varepsilon:\mathbb{R}^{3}\times\mathbb{R}^{3}\times\mathbb{R}^{3}\rightarrow\mathbb{R}$
defined by
\[
\varepsilon\left(\boldsymbol{e}_{j},\boldsymbol{e}_{k},\boldsymbol{e}_{l}\right)=:\varepsilon_{jkl}=\begin{cases}
+1 & \text{if \ensuremath{\left(jkl\right)} is an even permutation of \ensuremath{\left(123\right)}}\\
-1 & \text{if \ensuremath{\left(jkl\right)} is an odd permutation of \ensuremath{\left(123\right)}}\\
0 & \text{if \ensuremath{\left(jkl\right)} is not a permutation of \ensuremath{\left(123\right)}}.
\end{cases}
\]
One example of its use is in representing the cross product $x\times y$
in Einstein notation:
\[
\left(\boldsymbol{x}\times\boldsymbol{y}\right)_{j}=\varepsilon_{jkl}x^{k}y^{l}.
\]

\end{example}
~
\begin{example}
\label{example:1_3_kronecker}\textbf{\emph{The Euclidean Kronecker
delta.}} Consider $\mathbb{R}^{n}$ with its Cartesian basis $\left\{ \boldsymbol{e}_{1},\ldots,\boldsymbol{e}_{n}\right\} $.
The \emph{Euclidean Kronecker delta }is the rank-2 tensor $\delta:\mathbb{R}^{n}\times\mathbb{R}^{n}\rightarrow\mathbb{R}$,
where
\[
\delta\left(\boldsymbol{e}_{j},\boldsymbol{e}_{k}\right)=:\delta_{jk}=\begin{cases}
1 & \text{if \ensuremath{j=k}}\\
0 & \text{otherwise.}
\end{cases}
\]
Note that $\delta_{jk}$ has the values $0$ and $1$ in the particular
case of the Cartesian basis, but generally this is not so. This is
because the Kronecker delta is typically defined in terms of the mixed
tensor, $\delta_{k}^{j}$, which we discuss in the next section.
\end{example}

\subsection{Dual spaces}

Let $V$ be an $n$-dimensional vector space over $\mathbb{R}$, and
let $f$ be a linear form thereon. Let $\boldsymbol{x}\in V$, and
expand $\boldsymbol{x}$ in a basis: $\boldsymbol{x}=x^{j}\boldsymbol{e}_{j}$.
Now consider
\[
\begin{alignedat}{1}f\left(\boldsymbol{x}\right)=f\left(x^{1}\boldsymbol{e}_{1}+\cdots+x^{n}\boldsymbol{e}_{n}\right) & =f\left(\boldsymbol{e}_{1}\right)x^{1}+\cdots+f\left(\boldsymbol{e}_{n}\right)x^{n}\\
 & =:u_{1}x^{1}+\cdots+u_{n}x^{n}=u_{j}x^{j}
\end{alignedat}
\]
where $u_{j}:=f\left(\boldsymbol{e}_{j}\right)\in\mathbb{R}$. Every
linear form on $V$ can be written in this way; the scalars $u_{j}$
uniquely determine the form $f$.\footnote{The proof of this is analogous to that of \cref{prop:1The-coordinates-completely}. }
Furthermore, the set of all $\boldsymbol{u}:=\left(u_{1},\ldots u_{n}\right)$,
and thus the set of linear forms $f$, constitutes a vector space,
denoted $V^{*}$. Since $V^{*}$ is of dimension $n$, it is isomorphic
to $\mathbb{R}^{n}$, and by extension, to $V$.
\begin{defn}
\textbf{\emph{Dual spaces.}}~

(a) The space $V^{*}$ of linear forms on $V$ is called the space
\emph{dual} to $V$.

(b) The elements of $V^{*}$ are called \emph{co-vectors}.\footnote{Co-vectors are also called \emph{covariant vectors}, in which case
vectors are called \emph{contravariant vectors}.} They are one-to-one correspondent to the vector elements of $V$.
\end{defn}
Since the co-vectors are defined via linear forms, and rank-$n$ tensors
are defined by $n$-linear forms,\footnote{As per \cref{sub:ch1sec1_2Tensor-spaces}}
we can consider co-vectors as tensors of rank 1. 
\begin{defn}
\textbf{\emph{Natural pairing.}} The scalar $f\left(\boldsymbol{x}\right)\in\mathbb{R}$
is called the \emph{natural pairing }or \emph{dual pairing}\footnote{According to Dr. Belitz, this is called the \emph{scalar product}
and denoted $\boldsymbol{u}\cdot\boldsymbol{x}$, though I have been
unable to verify this.} of the co-vector \textbf{\emph{u}} (corresponding to $f$) and the
vector $\boldsymbol{x}$. We write
\[
\left\langle \boldsymbol{u},\boldsymbol{x}\right\rangle :=f\left(\boldsymbol{x}\right)=u_{j}x^{j}.
\]

\end{defn}
If $\left\{ \boldsymbol{e}_{j}\right\} $ is a basis of $V$, there
exists a canonical \emph{dual basis} or \emph{co-basis} $\left\{ \boldsymbol{e}^{j}\right\} $
of $V^{*}$,\footnote{It can be proven that, for finite dimensional $V$, the co-basis is
a basis of $V^{*}$.} defined by
\[
\left\langle \boldsymbol{e}^{j},\boldsymbol{e}_{k}\right\rangle =\left(\boldsymbol{e}^{j}\right)_{l}\left(\boldsymbol{e}_{k}\right)^{l}=\delta_{k}^{j},
\]
where 
\[
\boxed{\delta_{k}^{j}:=\begin{cases}
1 & \text{if \ensuremath{j=k}}\\
0 & \text{otherwise}
\end{cases}}
\]
 is called the \emph{Kronecker delta}. The basis $\left\{ \boldsymbol{e}_{j}\right\} $
and co-basis $\left\{ \boldsymbol{e}^{j}\right\} $ are said to be
\emph{biorthogonal.}\footnote{If $\left\{ \boldsymbol{e}_{j}\right\} $ is the Cartesian basis,
it can be proven that $\boldsymbol{e}_{j}=\boldsymbol{e}^{j}\,\forall j\in\left[n\right]$} Any element $u\in V^{*}$ can be expanded in terms of the dual basis
as 
\[
\boldsymbol{u}=u_{j}\boldsymbol{e}^{j}.
\]

\begin{defn}
\textbf{\emph{Contra-/co-variant and mixed tensors.}}~

(a) Bilinear forms $f:V^{*}\times V^{*}\rightarrow\mathbb{R}$ acting
on the co-basis define \emph{contravariant tensors }of rank 2,
\[
f\left(\boldsymbol{e}^{j},\boldsymbol{e}^{k}\right)=t^{jk},
\]
 and analogously for higher rank tensors.\footnote{In this manner, vectors can be considered contravariant tensors of
rank 1.} The tensors of \cref{example:1_3_kronecker} are then called \emph{covariant
tensors}.

(b) Multilinear forms acting on mixtures of basis and co-basis vectors
define \emph{mixed tensors}. For example, $f:V^{*}\times V\times V^{*}\rightarrow\mathbb{R}$
defines $t_{k}^{jl}=f\left(\boldsymbol{e}^{j},\boldsymbol{e}_{k},\boldsymbol{e}^{l}\right).$\footnote{The Kronecker delta, $\delta_{k}^{j}$, is thus a mixed tensor of
rank 2.}
\end{defn}
~
\begin{defn}
\textbf{\emph{Tensor product.}} The contravariant tensor whose components
are given by the product of the components of two contravariant vectors
$\boldsymbol{x}$ and $\boldsymbol{y}$ is called the \emph{tensor
product} of $\boldsymbol{x}$ and $\boldsymbol{y}$, denoted by
\[
\boxed{t=\boldsymbol{x}\otimes\boldsymbol{y},\quad t^{jk}=x^{j}y^{k}}
\]
 Analogously, $t_{jk}=x_{j}y_{k}$, $t{}_{j}{}^{k}=x_{j}y^{k}$, and
$t{}^{j}{}_{k}=x^{j}y_{k}$.
\end{defn}

\section{Minkowski space}


\subsection{The metric tensor}
\begin{defn}
\textbf{\emph{Metric tensor.}} Let $V$ be an $n$-dimensional vector
space over $\mathbb{R}$ with basis $\left\{ \boldsymbol{e}_{j}\right\} $,
and let $g:V\times V\rightarrow\mathbb{R}$ be a symmetric bilinear
form.\footnote{That is, $g\left(\boldsymbol{x},\boldsymbol{y}\right)=g\left(\boldsymbol{y},\boldsymbol{x}\right)\,\forall\boldsymbol{x},\boldsymbol{y}\in V$.}
Then $g$ defines a symmetric 2-tensor: 
\[
\boxed{g_{jk}=g\left(\boldsymbol{e}_{j},\boldsymbol{e}_{k}\right)=g_{kj}}
\]
Let $g$ have an inverse $g^{-1}$, corresponding to a tensor $g^{jk}$,
in the sense 
\[
\boxed{g_{jk}g^{kl}=\delta_{j}^{l}}
\]
 Then we call the scalar
\[
\boxed{g\left(\boldsymbol{x},\boldsymbol{y}\right)=x^{j}g_{jk}y^{k}}
\]
 the \emph{generalized scalar product of $x$ and $y$}, with $g_{jk}$
called the \emph{metric tensor}, denoted
\[
\boxed{g\left(\boldsymbol{x},\boldsymbol{y}\right)=:\boldsymbol{x}\cdot\boldsymbol{y}=:\boldsymbol{x}\boldsymbol{y}}
\]

\end{defn}
Since $V$ is isomorphic to $\mathbb{R}^{n}$, we can consider $\mathbb{R}^{n}$
in what follows.
\begin{defn}
\textbf{\emph{Co-basis.}} Consider $\mathbb{R}^{n}$ endowed with
a metric tensor, $g$, and let $\left\{ \boldsymbol{e}_{j}\right\} $
be a basis. We define an \emph{adjoint basis }or \emph{co-basis $\left\{ \boldsymbol{e}^{j}\right\} $
by 
\[
\boxed{\boldsymbol{e}^{j}:=g^{jk}\boldsymbol{e}_{k}}
\]
}

It readily follows that\footnote{$\boldsymbol{e}_{j}=\delta_{j}^{l}\boldsymbol{e}_{l}=g_{jk}g^{kl}\boldsymbol{e}_{l}=g_{jk}\boldsymbol{e}^{k}$}
\[
\boxed{\boldsymbol{e}_{j}=g_{jk}\boldsymbol{e}^{k}}
\]
\end{defn}
\begin{prop}
The contravariant and covariant components of a vector are related
by
\[
\boxed{x_{j}=g_{jk}x^{k},\quad x^{j}=g^{jk}x_{k}}
\]
\end{prop}
\begin{proof}
$\boldsymbol{x}=x^{j}\boldsymbol{e}_{j}=x^{j}g_{jk}\boldsymbol{e}^{k}=x_{k}\boldsymbol{e}^{k}$,
since $x_{j}$ are defined to be the components of $\boldsymbol{x}$
in basis $\left\{ \boldsymbol{e}^{j}\right\} $. But $g$ is symmetric:

$g_{jk}=g_{kj}\implies x_{k}=g_{kj}x^{j}$. Therefore, $x^{j}=\delta_{j}^{k}x^{k}=g_{kl}g^{lj}x^{k}=x_{l}g^{lj}$.\end{proof}
\begin{rem}
It can be proven\footnote{We proved it in class lol} that the metric
tensor, operating on a general $n+1$ rank tensor, has the effect
that it lowers or raises the index being summed over:
\[
g_{jk}t^{kl_{1}\cdots l_{n}}=t{}_{j}{}^{l_{1}\cdots l_{n}}.
\]
 
\end{rem}
~
\begin{rem}
Note that $\delta_{jk}=g_{jl}\delta_{l}^{k}=g_{jk}$, which in general
is \emph{not }equal to $\delta_{k}^{j}$. However, $g_{j}^{l}=g_{jk}g^{kl}=\delta_{j}^{l}$
is always true. Only in Euclidean space is $\delta_{jk}=\delta_{k}^{j}$.
\end{rem}

\subsection{Basis transformations}
\begin{defn}
\textbf{\emph{Matrices.}}\textbf{~}

(a) An $n\times n$ array of real numbers $D{}^{j}{}_{k}$ (corresponding
to the $j^{th}$ row and $k^{th}$ column) we call an $n\times n$
\emph{matrix }$D$ with \emph{elements }$ $$D{}^{j}{}_{k}$.

(b) A matrix $D$ is \emph{invertible} if a matrix $D^{-1}$ exists
such that 
\[
\boxed{D{}^{j}{}_{k}(D^{-1}){}^{k}{}_{l}=(D^{-1}){}^{j}{}_{k}D{}^{k}{}_{l}=\delta_{l}^{j}}
\]
 or, $DD^{-1}=\mathbbm{1}_{n}$ with $\mathbbm{1}_{n}$ the $n\times n$
unit matrix with $(\mathbbm{1}_{n}){}^{j}{}_{k}=\delta_{k}^{j}$.

(c) The matrix $D^{T}$ with elements 
\[
\boxed{(D^{T}){}^{j}{}_{k}=D{}_{k}{}^{j}}
\]
 is called the \emph{transpose} of $D$.\end{defn}
\begin{prop}
The transpose of a product is the product of the transposes, in reverse
order: 
\[
\boxed{\left(AB\right)^{T}=B^{T}A^{T}}
\]
\end{prop}
\begin{proof}
$(\left(AB\right)^{T}){}^{j}{}_{k}=(AB){}_{k}{}^{j}=A{}_{k}{}^{l}B{}_{l}{}^{j}=(A^{T}){}^{l}{}_{k}(B^{T}){}^{j}{}_{l}=(B^{T}){}^{j}{}_{l}(A^{T}){}^{l}{}_{k}=(B^{T}A^{T}){}^{j}{}_{k}.$\end{proof}
\begin{prop}
The inverse of a transpose is the transpose of the inverse: 
\[
\boxed{\left(D^{-1}\right)^{T}=\left(D^{T}\right)^{-1}}
\]
\end{prop}
\begin{proof}
$D^{T}\left(D^{-1}\right)^{T}=\left(D^{-1}D\right)^{T}=\left(\mathbbm{1}_{n}\right)^{T}=\mathbbm{1}_{n}$.\end{proof}
\begin{defn}
\textbf{\emph{Basis transformation.}} Consider $\mathbb{R}^{n}$ with
a metric tensor $g$; let $\left\{ \boldsymbol{e}_{j}\right\} $ be
a basis and let $D$ be an invertible matrix. Then we define a new
basis\footnote{For proof that it is indeed a basis, see Problem 5.}
$\left\{ \tilde{\boldsymbol{e}}_{j}\right\} $ by the \emph{basis
transformation}
\[
\boxed{\tilde{\boldsymbol{e}}_{j}=\boldsymbol{e}_{k}(D^{-1}){}^{k}{}_{j}},
\]
 whose inverse transformation yields\footnote{$\boldsymbol{e}_{j}=\boldsymbol{e}_{k}\delta_{j}^{k}=\boldsymbol{e}_{k}(D^{-1}){}^{k}{}_{l}D{}^{l}{}_{j}=\tilde{\boldsymbol{e}}_{l}D{}^{l}{}_{j}$}
\[
\boxed{\boldsymbol{e}_{j}=\tilde{\boldsymbol{e}}_{k}D{}^{k}{}_{j}}
\]
\end{defn}
\begin{prop}
\label{prop:2_2_contra_trafo}Let $\boldsymbol{x}\in\mathbb{R}^{n}$
be a vector whose contravariant coordinates with respect to a basis
$\left\{ \boldsymbol{e}_{j}\right\} $ are $x^{j}$. Then its coordinates
with respect to the basis $\left\{ \tilde{\boldsymbol{e}}_{j}\right\} $,
denoted $\tilde{x}^{j}$ and called a \emph{coordinate transformation,
}are
\[
\boxed{\tilde{x}^{j}=D{}^{j}{}_{k}x^{k}}\quad\textrm{or}\quad\boxed{\tilde{\boldsymbol{x}}=D\boldsymbol{x}}
\]


with inverse transformation
\[
\boxed{x^{j}=(D^{-1}){}^{j}{}_{k}\tilde{x}^{k}=}\quad\textrm{or}\quad\boxed{\boldsymbol{x}=D^{-1}\tilde{\boldsymbol{x}}}
\]
\end{prop}
\begin{proof}
$\boldsymbol{x}=x^{j}\boldsymbol{e}_{j}=x^{j}\tilde{\boldsymbol{e}}_{k}D{}^{k}{}_{j}=x^{j}D{}^{k}{}_{j}\tilde{\boldsymbol{e}}_{k}=\tilde{x}^{k}\tilde{\boldsymbol{e}}_{k}\implies\tilde{x}^{k}=x^{j}D{}^{k}{}_{j}$\end{proof}
\begin{prop}
Let $g_{jk}=\boldsymbol{e}_{j}\cdot\boldsymbol{e}_{k}$ be the metric
in the basis $\left\{ \boldsymbol{e}_{j}\right\} $, and let $D^{-1}$
be a basis transformation such that $ $\textup{$\tilde{\boldsymbol{e}}_{j}=\boldsymbol{e}_{k}(D^{-1}){}^{k}{}_{j}$.
}\textup{\emph{Then the metric $\tilde{g}$ corresponding to $g$
expressed in the transformed basis $\left\{ \tilde{\boldsymbol{e}}_{j}\right\} $,
defined by coordinates
\[
\boxed{\tilde{g}:=g\left(\tilde{\boldsymbol{e}}_{j},\tilde{\boldsymbol{e}}_{k}\right)}
\]
 is given by}}
\[
\boxed{\tilde{g}_{jk}=((D^{-1})^{T}){}_{j}{}^{m}g_{ml}(D^{-1}){}^{l}{}_{k}}\quad\textrm{or}\quad\boxed{\tilde{g}=\left(D^{-1}\right)^{T}gD^{-1}}\quad\textrm{or}\quad\boxed{g=D^{T}\tilde{g}D}
\]
\end{prop}
\begin{proof}
Problem 6\end{proof}
\begin{cor}
\label{cor:2_2_covar_coord_trafo}The covariant coordinates transform
according to
\[
\boxed{\tilde{x}_{j}=(D^{-1}){}^{k}{}_{j}x_{k}}
\]
 with inverse transformation
\[
\boxed{x_{j}=D{}^{k}{}_{j}\tilde{x}_{k}}
\]
\end{cor}
\begin{proof}
$\tilde{x}_{j}=\tilde{g}_{jk}\tilde{x}^{k}=\tilde{g}_{jk}D{}^{k}{}_{l}x^{l}=\tilde{g}_{jk}D{}^{k}{}_{l}g^{lm}x_{m}=(\tilde{g}Dg){}_{j}{}^{m}x_{m}$

$=(\left(D^{-1}\right)^{T}gD^{-1}Dg){}_{j}{}^{m}x_{m}\overset{g^{2}=\mathbbm{1}}{=}(\left(D^{-1}\right)^{T}){}_{j}{}^{m}x_{m}=(D^{-1}){}^{m}{}_{j}x_{m}$
\end{proof}

\subsection{Normal coordinate systems}
\begin{lem}
\label{lem:symmetric_implies_diagonalizable}For every symmetric $n\times n$
matrix $M{}^{j}{}_{k}=M{}^{k}{}_{j}$ that has an inverse, there exists
a transformation $D$ such that\footnote{When a sub- or superscript is in parentheses, no summation is implied.}
\[
\tilde{M}{}^{j}{}_{k}=(D^{T}MD){}^{j}{}_{k}=m_{(j)}\delta_{k}^{j}.
\]
 That is to say, there exists a transformation that diagonalizes $M$.\end{lem}
\begin{proof}
This is called the \emph{spectral decomposition theorem}, and is proven
elsewhere.\end{proof}
\begin{cor}
\label{cor:diag_metric}Let $g_{jk}$ be a metric on $\mathbb{R}^{n}$.
There exists a coordinate transformation $D$ such that\footnote{Here, for whatever reason, the Kronecker delta is the Euclidean one.}
\[
\boxed{\tilde{g}_{jk}=\lambda_{\left(j\right)}\delta_{jk}}
\]
\end{cor}
\begin{proof}
$g$ can be considered a real symmetric matrix; by \cref{lem:symmetric_implies_diagonalizable},
it can be diagonalized in this form.\end{proof}
\begin{thm}
\label{thm:diag_metric_2}There exists a coordinate transformation
$D$ that diagonalizes $g$ such that
\[
\boxed{\tilde{g}=\begin{pmatrix}1\\
 & \ddots\\
 &  & 1\\
 &  &  & -1\\
 &  &  &  & \ddots\\
 &  &  &  &  & -1
\end{pmatrix}}
\]
with $m$ elements of $+1$ and $n-m$ elements of $-1$, where $0\leq m\leq n$.\end{thm}
\begin{proof}
From \cref{cor:diag_metric}, we can write ...\end{proof}
\begin{defn}
\textbf{\emph{Normed coordinate systems.}} Basis sets in which the
metric has the form of \cref{thm:diag_metric_2} are called \emph{normed
coordinate systems}. The number $m$ is characteristic of the space;
this is sometimes called \emph{Sylvester's Rigidity Theorem}.\end{defn}
\begin{example}
\textbf{\emph{Normed Euclidean space.}} Let $m=n$. Then
\[
g=\begin{pmatrix}1\\
 & \ddots\\
 &  & 1
\end{pmatrix}
\]
 and we see
\[
g_{jk}=\delta_{k}^{j}.
\]
 $\mathbb{R}^{n}$ endowed with this metric is called $n-$\emph{dim
Euclidean Space, $E_{n}$. }The normal coordinate systems are called
\emph{Cartesian}. In the space $E_{n}$, we have $x_{j}=g_{jk}x^{k}=\delta_{k}^{j}x^{k}=x^{j}$.
In this case, positive semi-definiteness holds, and so also the Pythagorean
Theorem.
\end{example}
~
\begin{example}
\textbf{\emph{Normed Minkowski space.}} Let $m=1$; $\left(n\geq2\right)$.
Then
\[
g=\begin{pmatrix}1\\
 & -1\\
 &  & \ddots\\
 &  &  & -1
\end{pmatrix}
\]
$\mathbb{R}^{n}$ endowed with this metric is called \emph{Minkowski
space}, $M_{n}$. The normal coordinate systems are called \emph{inertial
frames}. In the space $M_{n}$, we have $x_{1}=x^{1}$ and $x_{j}=-x^{j}$
for $j=2,\ldots,n$. In physics we label $\boldsymbol{x}=\left(x^{0},x^{1},x^{2},x^{3}\right)$
with $x^{0}=x_{0}=ct$ where $t$ is called \emph{time} and $\left(x^{1},x^{2},x^{3}\right)$
is called space. $c$ is a characteristic velocity, namely the speed
of light in vacuum.
\end{example}

\subsection{Normal coordinate transformations}
\begin{defn}
\label{def:2_4_normal_coord_trafo}\textbf{\emph{Normal coordinate
transformation.}} A \emph{normal coordinate transformation} is one
that transforms a normal coordinate system into another normal coordinate
system. That is,
\[
\boxed{g=\left(D^{-1}\right)^{T}gD^{-1}},
\]
 from which it follows
\[
\boxed{g=D^{T}gD}.
\]
\end{defn}
\begin{example}
For $E_{n}$, these transformations are called \emph{orthogonal} and
are a subset of \emph{unitary transformations:
\[
g=\mathbbm{1}_{n}=\left(D^{-1}\right)^{T}\mathbbm{1}_{n}D^{-1}=\left(D^{-1}\right)^{T}D^{-1}
\]
\[
\implies D^{T}=D^{-1}
\]
}
\end{example}
~
\begin{example}
For $M_{4}$, these transformations are called \emph{Lorentz transformations}.\end{example}
\begin{lem}
~

(i) If $D$ is a normal transformation, then so is $D^{-1}$

(ii) If $D_{1}$, $D_{2}$ are normal transformations, then so is
the successive transformation $D_{1}D_{2}$.\end{lem}
\begin{proof}
Problem 7\end{proof}
\begin{thm}
The set of normal transformations forms a group (not necessarily abelian)
under matrix multiplication.\end{thm}
\begin{proof}
Problem 7\end{proof}
\begin{example}
In $E_{n}$, the group of normal transformations is called the \emph{orthogonal
group} $O\left(n\right)$. 
\end{example}
~
\begin{example}
In $M_{n}$, the group of normal transformations is called the \emph{pseudo-orthogonal
group} $O\left(1,n-1\right)$.\end{example}
\begin{prop}
Let $D$ be a normal coordinate transformation. Then
\[
\boxed{\det D=\pm1}
\]
\end{prop}
\begin{proof}
From \cref{def:2_4_normal_coord_trafo}, $\det g=\det\left(D^{T}gD\right)=\det g$$\left(\det D\right)^{2}$
$\implies\det D=\pm1$. 
\end{proof}

\section{Tensor Fields}


\subsection{The concept of a tensor field}

Let $V$ be $\mathbb{R}^{n}$ endowed with metric tensor $g$, and
let $D$ be a normal coordinate transformation (say, from coordinate
system $CS$ to $\widetilde{CS}$). That is, transformed coordinates
take the form $\tilde{x}^{j}=D{}^{j}{}_{k}x^{k}$.
\begin{defn}
\label{definition:3_1_tensor_field}\textbf{\emph{(Pseudo-)tensor
fields.}} $\forall\boldsymbol{x}\in V$, consider assigning a rank-$N$
tensor $t^{j_{1}\cdots j_{N}}\left(\boldsymbol{x}\right)$ to $\boldsymbol{x}$.
The set of assigned tensors,\footnote{A tensor field $t$ can be considered a tensor-valued function with
domain $V$. That is, $t:V\rightarrow V^{N}$.} $\left\{ t^{j_{1}\cdots j_{N}}\left(\boldsymbol{x}\right);\boldsymbol{x}\in V\right\} $,
is called a \emph{tensor field} iff, under a coordinate transformation,
\[
\boxed{\tilde{t}^{j_{1}\cdots j_{N}}\left(\boldsymbol{x}\right)=D{}^{j_{1}}{}_{k_{1}}\cdots D{}^{j_{N}}{}_{k_{N}}t^{j_{1}\cdots j_{N}}\left(\boldsymbol{x}\right)}
\]
and is called a \emph{pseudo-tensor field} iff, under a coordinate
transformation,
\[
\boxed{\tilde{t}^{j_{1}\cdots j_{N}}\left(\boldsymbol{x}\right)=\left(\det D\right)D{}^{j_{1}}{}_{k_{1}}\cdots D{}^{j_{N}}{}_{k_{N}}t^{j_{1}\cdots j_{N}}\left(\boldsymbol{x}\right)}
\]
\end{defn}
\begin{example}
Is the Levi-Civita tensor a tensor or pseudo-tensor? Recall that by
\cref{example:def_levicivita}, the Levi-Civita is independent of
$\boldsymbol{x}$; that is,\footnote{This is the only justification I could come up with for this equation.
I am not sure if the middle two parts of this equation are the reason
you can say this.} 
\[
\tilde{\varepsilon}^{jkl}:=\varepsilon\left(\tilde{\boldsymbol{e}}^{j},\tilde{\boldsymbol{e}}^{k},\tilde{\boldsymbol{e}}^{l}\right)=\varepsilon\left(\boldsymbol{e}^{j},\boldsymbol{e}^{k},\boldsymbol{e}^{l}\right)=\varepsilon^{jkl}.
\]
 Let $D$ be a normal coordinate transformation. Then\footnote{A few remarks on the notation used here:
\begin{itemize}
\item $S_{3}$ denotes the ``symmetric group on $3$ letters'', and represents
the set of possible permutations of the set $\left\{ 1,2,3\right\} $.
Thus, $\pi\in S_{3}$ means $\pi$ is some permutation of the numbers
$\left\{ 1,2,3\right\} $ such as $312$, and $\overset{}{\underset{\pi\in S_{3}}{\sum}}\left(\cdots\right)$
represents a sum over all possible permutations of $\left\{ 1,2,3\right\} $.
\item $\textnormal{sgn}\left(\pi\right)$ represents the ``sign'' of the
permutation; if it is an even permutation, $\textnormal{sgn}\left(\pi\right)=1$,
and if an odd permutation, $\textnormal{sgn}\left(\pi\right)=-1$.
\item $\pi\left(1\right)$ is the first number in the permutation, $\pi\left(2\right)$
is the second, etc. That is, if the permutation is $312$, $\pi\left(1\right)=3$,
$\pi\left(2\right)=1$, etc.
\item We can represent a permutation, say $312$, with the notation $\begin{pmatrix}1 & 2 & 3\\
\pi\left(1\right) & \pi\left(2\right) & \pi\left(3\right)
\end{pmatrix}=\begin{pmatrix}1 & 2 & 3\\
3 & 1 & 2
\end{pmatrix}$ . The order of the columns doesn't matter: $\begin{pmatrix}1 & 2 & 3\\
3 & 1 & 2
\end{pmatrix}=\begin{pmatrix}2 & 3 & 1\\
1 & 2 & 3
\end{pmatrix}$. However, the bottom row always represents $\pi\left(1\right)$,
$\pi\left(2\right)$, etc.\end{itemize}
}
\begin{eqnarray*}
D{}^{j}{}_{\alpha}D{}^{k}{}_{\beta}D{}^{l}{}_{\gamma}\varepsilon^{\alpha\beta\gamma} & = & \overset{}{\underset{\pi\in S_{3}}{\sum}}\textrm{sgn}\left(\pi\right)D{}^{j}{}_{\pi\left(1\right)}D{}^{k}{}_{\pi\left(2\right)}D{}^{l}{}_{\pi\left(3\right)}\\
 & = & \begin{vmatrix}D{}^{j}{}_{1} & D{}^{j}{}_{2} & D{}^{j}{}_{3}\\
D{}^{k}{}_{1} & D{}^{k}{}_{2} & D{}^{k}{}_{3}\\
D{}^{l}{}_{1} & D{}^{l}{}_{2} & D{}^{l}{}_{3}
\end{vmatrix}\\
 & = & \textrm{sgn}\begin{pmatrix}j & k & l\\
1 & 2 & 3
\end{pmatrix}\det D=\varepsilon^{jkl}\det D\\
 & = & \varepsilon^{jkl}\frac{1}{\det D}=\tilde{\varepsilon}^{jkl}\frac{1}{\det D}.
\end{eqnarray*}
 As per \cref{definition:3_1_tensor_field}, the Levi-Civita tensor
constitutes a pseudo-tensor field:
\[
\boxed{\tilde{\varepsilon}^{jkl}=\left(\det D\right)D{}^{j}{}_{\alpha}D{}^{k}{}_{\beta}D{}^{l}{}_{\gamma}\varepsilon^{\alpha\beta\gamma}}
\]

\end{example}

\subsection{Gradient, curl, divergence}

Let $f\left(\boldsymbol{x}\right)$ be a scalar-valued function $f:V\rightarrow\mathbb{R}$
(that is, a scalar field).
\begin{claim}
\label{claim:3_2_trafo_partial}Let $D$ be a coordinate transformation.
Then
\[
\boxed{\left(D^{-1}\right){}^{j}{}_{k}=\frac{\partial x^{j}}{\partial\tilde{x}^{k}}}
\]
\end{claim}
\begin{proof}
Take the partial derivative of $x^{j}=\left(D^{-1}\right){}^{j}{}_{k}\tilde{x}^{k}$
with respect to $\tilde{x}^{k}$.\end{proof}
\begin{defn}
\textbf{\emph{Gradient.}} The \emph{gradient }of\emph{ $f$, }denoted
$\left(\nabla f\right)\left(\boldsymbol{x}\right)$ or $\left(\textnormal{grad }f\right)\left(\boldsymbol{x}\right)$,
is the vector field defined by components\footnote{Note that when we begin discussing Minkowski space, the $\nabla$
symbol will be reserved for Euclidean vectors.}
\[
\boxed{\left(\nabla f\right)_{j}\left(\boldsymbol{x}\right):=\frac{\partial}{\partial x^{j}}f\left(\boldsymbol{x}\right)}
\]
which is often also written\footnote{A subscript is used because, as we will prove, the gradient transforms
covariantly.} 
\[
\boxed{\partial_{j}f\left(\boldsymbol{x}\right):=\frac{\partial}{\partial x^{j}}f\left(\boldsymbol{x}\right)}.
\]
 Analogously,\footnote{The superscript reflects the fact that this derivative transforms
contravariantly.}
\[
\boxed{\partial^{j}f\left(\boldsymbol{x}\right):=\frac{\partial}{\partial x_{j}}f\left(\boldsymbol{x}\right)}.
\]
\end{defn}
\begin{prop}
\label{prop:3_2_del_trafo}The gradient of a scalar field transforms
as a covariant vector:
\[
\boxed{\tilde{\partial}_{j}\tilde{f}\left(\tilde{\boldsymbol{x}}\right)=\left(D^{-1}\right){}^{k}{}_{j}\partial_{k}f\left(\boldsymbol{x}\right)}
\]
\end{prop}
\begin{proof}
Let $D$ be a coordinate transformation. Then\footnote{Since $f$ is a scalar-valued function, $\tilde{f}\left(\tilde{\boldsymbol{x}}\right)=f\left(\boldsymbol{x}\right)$.
The second line follows from the chain rule.}
\begin{eqnarray*}
\tilde{\partial}_{j}\tilde{f}\left(\tilde{\boldsymbol{x}}\right) & = & \frac{\partial}{\partial\tilde{x}^{j}}\tilde{f}\left(\tilde{\boldsymbol{x}}\right)=\frac{\partial}{\partial\tilde{x}^{j}}f\left(\boldsymbol{x}\right)\\
 & = & \frac{\partial x^{k}}{\partial\tilde{x}^{j}}\frac{\partial}{\partial x^{k}}f\left(\boldsymbol{x}\right)\\
 & = & \left(D^{-1}\right){}^{k}{}_{j}\partial_{k}f\left(\boldsymbol{x}\right).
\end{eqnarray*}
\end{proof}
\begin{defn}
\textbf{\emph{Curl.}} The \emph{curl }of a vector field \textbf{$\boldsymbol{v}\left(\boldsymbol{x}\right)$},
denoted $\left(\nabla\times\boldsymbol{v}\right)\left(\boldsymbol{x}\right)$
or $\left(\textnormal{curl }\boldsymbol{v}\right)\left(\boldsymbol{x}\right)$,
is the vector field whose $j^{\textnormal{th}}$ component is\footnote{The superscript reflects the fact that the curl transforms as a pseudovector.}
\[
\boxed{\left(\nabla\times\boldsymbol{v}\right)^{j}\left(\boldsymbol{x}\right):=\varepsilon^{jkl}\partial_{k}v_{l}\left(\boldsymbol{x}\right)}
\]
\end{defn}
\begin{prop}
The curl of a vector field transforms as a pseudovector:
\[
\boxed{\widetilde{\left(\nabla\times\boldsymbol{v}\right)}^{j}\left(\tilde{\boldsymbol{x}}\right)=\left(\det D\right)D{}^{j}{}_{k}\left(\nabla\times\boldsymbol{v}\right)^{k}\left(\boldsymbol{x}\right)}
\]
\end{prop}
\begin{proof}
Let $D$ be a coordinate transformation. By \cref{prop:3_2_del_trafo}
and \cref{cor:2_2_covar_coord_trafo}, 
\begin{eqnarray*}
\widetilde{\left(\nabla\times\boldsymbol{v}\right)}^{j}\left(\tilde{\boldsymbol{x}}\right) & = & \tilde{\varepsilon}^{jkl}\tilde{\partial}_{k}\tilde{v}_{l}\left(\tilde{\boldsymbol{x}}\right)\\
 & = & \tilde{\varepsilon}^{jkl}\left(D^{-1}\right){}^{m}{}_{k}\partial_{m}\left(D^{-1}\right){}^{\alpha}{}_{l}v_{\alpha}\left(\boldsymbol{x}\right)\\
 & = & \delta_{\beta}^{j}\tilde{\varepsilon}^{\beta kl}\left(D^{-1}\right){}^{m}{}_{k}\left(D^{-1}\right){}^{\alpha}{}_{l}\partial_{m}v_{\alpha}\left(\boldsymbol{x}\right)\\
 & = & D{}^{j}{}_{\gamma}\underset{\left(\det D\right)\varepsilon^{\gamma m\alpha}}{\underbrace{\left(D^{-1}\right){}^{\gamma}{}_{\beta}\tilde{\varepsilon}^{\beta kl}\left(D^{-1}\right){}^{m}{}_{k}\left(D^{-1}\right){}^{\alpha}{}_{l}}}\partial_{m}v_{\alpha}\left(\boldsymbol{x}\right)\\
 & = & \left(\det D\right)D{}^{j}{}_{\gamma}\varepsilon^{\gamma m\alpha}\partial_{m}v_{\alpha}\left(\boldsymbol{x}\right)\\
 & = & \left(\det D\right)D{}^{j}{}_{\gamma}\left(\nabla\times\boldsymbol{v}\right)^{\gamma}\left(\boldsymbol{x}\right)
\end{eqnarray*}
\end{proof}
\begin{defn}
\textbf{\emph{Divergence.}}\textbf{ }The \emph{divergence} of a vector
field $\boldsymbol{v}\left(\boldsymbol{x}\right)$, denoted $\left(\nabla\cdot\boldsymbol{v}\right)\left(\boldsymbol{x}\right)$
or $\left(\textnormal{div }\boldsymbol{v}\right)\left(\boldsymbol{x}\right)$,
is the scalar field defined by
\[
\boxed{\left(\nabla\cdot\boldsymbol{v}\right)\left(\boldsymbol{x}\right):=\partial_{j}v^{j}\left(\boldsymbol{x}\right)}
\]
\end{defn}
\begin{prop}
The divergence of a vector field transforms as a scalar:
\[
\boxed{\widetilde{\left(\nabla\cdot\boldsymbol{v}\right)}\left(\tilde{\boldsymbol{x}}\right)=\left(\nabla\cdot\boldsymbol{v}\right)\left(\boldsymbol{x}\right)}
\]
\end{prop}
\begin{proof}
By Propositions \vref{prop:2_2_contra_trafo} and \vref{prop:3_2_del_trafo},
\begin{eqnarray*}
\widetilde{\left(\nabla\cdot\boldsymbol{v}\right)}\left(\tilde{\boldsymbol{x}}\right) & = & \tilde{\partial}_{j}\tilde{v}^{j}\left(\tilde{\boldsymbol{x}}\right)\\
 & = & \left(D^{-1}\right){}^{l}{}_{j}\partial_{l}D{}^{j}{}_{k}v^{k}\left(\boldsymbol{x}\right)\\
 & = & \left(D^{-1}\right){}^{l}{}_{j}D{}^{j}{}_{k}\partial_{l}v^{k}\left(\boldsymbol{x}\right)\\
 & = & \delta_{k}^{l}\partial_{l}v^{k}\left(\boldsymbol{x}\right)=\partial_{k}v^{k}\left(\boldsymbol{x}\right)\\
 & = & \left(\nabla\cdot\boldsymbol{v}\right)\left(\boldsymbol{x}\right)
\end{eqnarray*}

\end{proof}

\subsection{Tensor products and traces}

We can generalize the concepts of the tensor product defined in \cref{chap:Mathematical-preliminaries}
and the trace of a matrix.
\begin{defn}
\textbf{\emph{(General) tensor product.}} Let $s$, $t$ be tensors
of ranks $N$ and $M$, respectively. The \emph{tensor product} of
$s$ and $t$, denoted $s\otimes t$, is the rank $N+M$ tensor defined
by coordinates
\[
\boxed{\left(s\otimes t\right)^{j_{1}\cdots j_{N+M}}=s^{j_{1}\cdots j_{N}}t^{j_{N+1}\cdots j_{N+M}}}
\]
\end{defn}
\begin{prop}
The tensor product of two tensors or pseudotensors is tensor, while
the tensor product of a tensor with a pseudotensor is a pseudotensor.\end{prop}
\begin{proof}
Easy (apparently)\end{proof}
\begin{defn}
\textbf{\emph{Contraction.}} Let $t$ be a tensor or pseudotensor
of rank $N+2$. We define the $\left(1,2\right)-$\emph{trace} or
$\left(1,2\right)-$\emph{contraction} of $t$ as the rank $N$ tensor
or pseudotensor $u$ with components
\[
\boxed{u^{l_{1}\cdots l_{N}}:=g_{jk}t^{jkl_{1}\cdots l_{N}}=t{}^{j}{}_{j}{}^{l_{1}\cdots l_{N}}=t{}_{j}{}^{j}{}^{l_{1}\cdots l_{N}}}.
\]
 Note that the $1^{\textnormal{st}}$ and $2^{\textnormal{nd}}$ indices
were summed over; in general the $\left(j,k\right)-$\emph{contraction
}will instead sum over the $j^{\textnormal{th}}$ and $k^{\textnormal{th}}$
indices, respectively.\end{defn}
\begin{example}
The curl of a vector field can be considered a $\left(2,4\right)-$
and $\left(3,5\right)-$contraction of the rank 5 pseudotensor $\varepsilon^{jkl}\partial^{m}v^{\alpha}\left(\boldsymbol{x}\right)$.
\end{example}

\subsection{Minkowski tensors\label{sub:ch1sec3_4Minkowski-tensors}}

Consider $M_{4}$; that is, $\mathbb{R}^{4}$ endowed with the Minkowski
metric tensor $g=\left(+,-,-,-\right)$. 

Let $A^{\mu}\in M_{4}$. We adopt the following conventions:
\begin{enumerate}
\item We will often refer to the entire vector as $A^{\mu}=\left(A^{0},A^{1},A^{2},A^{3}\right)=:\left(A^{0},\boldsymbol{A}\right)$.\footnote{Note that we also label the first index with a $0$ instead of a $1$. } 
\item In sums, lowercase Greek indices run over all four indices: $\mu=0,1,2,3$.
\item Latin indices run over the three Euclidean components: $j=1,2,3$.
\end{enumerate}
In this notation, $A_{\mu}=g_{\mu\nu}A^{\nu}=\begin{cases}
A^{0} & \mu=0\\
-A^{j} & \mu=1,2,3
\end{cases}$. Furthermore, $\boldsymbol{A}:=\left(A^{1},A^{2},A^{3}\right)$ can
be considered a Euclidean vector in the subspace of $M_{4}$ spanned
by $\left\{ \boldsymbol{e}_{1},\boldsymbol{e}_{2},\boldsymbol{e}_{3}\right\} $.

Now consider a rank 2 tensor $F$. Analogous to the above conventions,
we can write $F$ in an array as

\begin{eqnarray*} F^{\mu\nu} & = & 
\left(\begin{array}{c;{2pt/2pt}ccc}
F^{00} & F^{01} & F^{02} & F^{03}\\\hdashline[2pt/2pt]
F^{10} & F^{11} & F^{12} & F^{13}\\
F^{20} & F^{21} & F^{22} & F^{23}\\
F^{30} & F^{31} & F^{32} & F^{33}
\end{array}\right)\\
& = & 
\left(\begin{array}{c;{2pt/2pt}c}
F^{00} & F^{0j} \\\hdashline[2pt/2pt]
F^{j0} & F^{jk} \\
\end{array}\right)\\
\end{eqnarray*}

where $F^{0j}$ and $F^{j0}$ can be considered vectors in the Euclidean
subspace, and $F^{jk}$ can be considered a Euclidean 2-tensor.\footnote{I use a different notation that Belitz; in his notation, $F^{0j}=\boldsymbol{F}_{h}$
and $F^{j0}=\boldsymbol{F}_{v}$.}
\begin{defn}
\textbf{\emph{Symmetric tensors.}} $F^{\mu\nu}$ is called a \emph{symmetric
tensor }iff
\[
\boxed{F^{\mu\nu}=F^{\nu\mu}},
\]
 from which it follows that $F^{0j}=F^{j0}.$ 
\end{defn}
~
\begin{defn}
\label{def:3_4-is-called}\textbf{\emph{Antisymmetric tensors.}} $F^{\mu\nu}$
is called an \emph{antisymmetric tensor} iff
\[
\boxed{F^{\mu\nu}=-F^{\nu\mu}},
\]
 from which it follows that $F^{jk}$ is antisymmetric, $F^{0j}=-F^{j0}$
and $F^{\mu\mu}=0$.\end{defn}
\begin{lem}
\label{lem:3_4_Antisymmetric-Euclidean-2-tensor}Antisymmetric Euclidean
2-tensors are isomorphic to Euclidean pseudovectors.\end{lem}
\begin{proof}
\begin{eqnarray*}
t^{jk}=-t^{kj} & \implies & t=\begin{pmatrix}0 & v^{3} & -v^{2}\\
-v^{3} & 0 & v^{1}\\
v^{2} & -v^{1} & 0
\end{pmatrix}\\
 & \implies & t^{jk}=\varepsilon^{jkl}v_{l}
\end{eqnarray*}


Since $t$ is a tensor and $\varepsilon$ is a pseudotensor, $v$
is a pseudovector.\footnote{For some reason, Belitz writes the components of $\boldsymbol{v}$
with subscripts instead of superscripts. I think they should be superscripts,
though.}\end{proof}
\begin{prop}
Any antisymmetric 2-tensor in Minkowski space can be written\begin{eqnarray*} F^{\mu\nu} & = & 
\left(\begin{array}{c;{2pt/2pt}c}
0      & a^{j}\\\hdashline[2pt/2pt]
-a^{j} & t^{jk}\\
\end{array}\right)\\
& := & 
\left(\begin{array}{c;{2pt/2pt}ccc}
0      & a^{1}  & a^{2}  & a^{3}  \\\hdashline[2pt/2pt]
-a^{1} & 0      & v^{3}  & -v^{2} \\
-a^{2} & -v^{3} & 0      & v^{1}  \\
-a^{3} & v^{2}  & -v^{1} & 0      \\
\end{array}\right)\\
\end{eqnarray*} where $\boldsymbol{a}$ is a Euclidean vector and $\boldsymbol{v}:=\left(v^{1},v^{2},v^{3}\right)$
is a Euclidean pseudovector.\end{prop}
\begin{proof}
See \cref{def:3_4-is-called} and \cref{lem:3_4_Antisymmetric-Euclidean-2-tensor}.\end{proof}
\begin{example}
\begin{eqnarray*} F{}^{\mu}{}_{\nu}=F^{\mu\kappa}g_{\kappa\nu} & = & 
\left(\begin{array}{c;{2pt/2pt}c}
0      & a^{j}\\\hdashline[2pt/2pt]
-a^{j} & t^{jk}\\
\end{array}\right)\left(\begin{array}{c;{2pt/2pt}c}
1 &                 \\\hdashline[2pt/2pt]
  & -\mathbbm{1}_{3}\\
\end{array}\right)\\
& = & 
\left(\begin{array}{c;{2pt/2pt}c}
0      & -a^{j}\\\hdashline[2pt/2pt]
-a^{j} & -t^{jk}\\
\end{array}\right)\\
\end{eqnarray*} Follow an analogous procedure to compute\begin{eqnarray*} F_{\mu\nu} & = & 
\left(\begin{array}{c;{2pt/2pt}c}
0     & -a^{j}\\\hdashline[2pt/2pt]
a^{j} & t^{jk}\\
\end{array}\right)\\
\end{eqnarray*} \begin{eqnarray*} F{}_{\mu}{}^{\nu} & = & 
\left(\begin{array}{c;{2pt/2pt}c}
0     & a^{j}  \\\hdashline[2pt/2pt]
a^{j} & -t^{jk}\\
\end{array}\right)\\
\end{eqnarray*}
\end{example}
~
\begin{example}
\label{example:action_scalar}$F_{\mu\nu}F^{\mu\nu}=2\left(\boldsymbol{v}^{2}-\boldsymbol{a}^{2}\right).$
This is just a Minkowski scalar!
\end{example}

\chapter{Maxwell's Equations\label{chap:Maxwell's-Equations}}


\section{The variational principle of classical electrodynamics}


\subsection{The Maxwellian action}

\doublebox{\begin{minipage}[t]{1\columnwidth}%
\textbf{Axiom} \textbf{1.} Space and time are described by a four-dimensional
Minkowski space with elements
\[
\boxed{x^{\mu}=\left(ct_{x},\boldsymbol{x}\right)},
\]
 where $t_{x}$ is called \emph{time, }\textbf{$\boldsymbol{x}$ }is
the position in space, and $c$ is a characteristic velocity.%
\end{minipage}}
\begin{rem}
We adopt the conventions outlined in \cref{chap:Mathematical-preliminaries},
\cref{sub:ch1sec3_4Minkowski-tensors}.
\end{rem}
\noindent %
\doublebox{\begin{minipage}[t]{1\columnwidth}%
\textbf{Axiom} \textbf{2.} Empty space (``vacuum'') supports a Minkowski
vector field
\[
A^{\mu}\left(x\right)
\]
 called the \emph{electromagnetic 4-vector potential}.%
\end{minipage}}
\begin{defn}
\textbf{\emph{Electromagnetic field tensor.}} The antisymmetric 2-tensor
field constructed from the 4-gradients of the electromagnetic 4-vector
potential via 
\[
\boxed{F^{\mu\nu}\left(x\right):=\partial^{\mu}A^{\nu}\left(x\right)-\partial^{\nu}A^{\mu}\left(x\right)}
\]
 is called the \emph{electromagnetic field tensor}. \end{defn}
\begin{rem}
By \cref{chap:Mathematical-preliminaries}, \cref{sub:ch1sec3_4Minkowski-tensors},
$F^{\mu\nu}\left(x\right)$ can be represented in terms of a Euclidean
vector field and a Euclidean pseudovector field.
\end{rem}
\doublebox{\begin{minipage}[t]{1\columnwidth}%
\textbf{Axiom} \textbf{3(a).} The physical field configurations in
vacuum are those that minimize the action 
\[
\boxed{S_{\textnormal{vac}}:=-\frac{1}{16\pi}\indefint{^{4}x}F_{\mu\nu}\left(x\right)F^{\mu\nu}\left(x\right)},
\]
 where $\infinitesimal{^{4}x}:=c\infinitesimal t\infinitesimal{\boldsymbol{x}}$.%
\end{minipage}}
\begin{rem}
The coefficient $\frac{1}{16\pi}$ is dependent on the unit convention
used. In this class we use \emph{CGS}.
\end{rem}
\hphantom{}
\begin{rem}
Classical electrodynamics is governed by a principle of least action,
as is classical mechanics. However, in electrodynamics we need to
find field configurations $A^{\mu}\left(x\right)$ that minimize the
action; in mechanics, we only had to find paths $\boldsymbol{x}\left(t\right)$. 
\end{rem}
\hphantom{}
\begin{rem}
As per \cref{example:action_scalar}, $F^{\mu\nu}F_{\mu\nu}$ is a
(Minkowski) scalar; therefore the theory is invariant under Lorentz
transformations (but \emph{not }Galilean).
\end{rem}
\doublebox{\begin{minipage}[t]{1\columnwidth}%
\textbf{Axiom} \textbf{3(b).} \label{axiom:action}Matter is characterized
(in part) by an $M_{4}$ vector $J^{\mu}\left(x\right)$ that couples
to $A^{\mu}\left(x\right)$ by the action 
\[
\boxed{S_{\textnormal{interaction}}:=-\frac{1}{c}\indefint{^{4}x}J^{\mu}\left(x\right)A_{\mu}\left(x\right)}.
\]
 The field plus its interaction with a \emph{given }$J^{\mu}\left(x\right)$
is described by the action
\[
\boxed{S=S_{\textnormal{vac}}+S_{\textnormal{interaction}}}.
\]
%
\end{minipage}}
\begin{rem}
$J^{\mu}\left(x\right)$ is called the \emph{4-current}.
\end{rem}
\hphantom{}
\begin{rem}
$J^{\mu}\left(x\right)$ is ``god-given''. We do not include the
feedback from the field on the matter. One needs another action term
to account for this.\end{rem}
\begin{defn}
\textbf{\emph{Dual field tensor.}} The \emph{dual}\footnote{Not ``dual'' as in the dual vector space; this is just the conventional
name for this tensor.}\emph{ field tensor}, denoted\footnote{This tilde is not implying any transformation; it is merely conventional.}
$\tilde{F}^{\mu\nu}$ is defined as 
\[
\boxed{\tilde{F}^{\mu\nu}:=\varepsilon^{\mu\nu\alpha\beta}F_{\alpha\beta}},
\]
 where $\varepsilon^{\mu\nu\alpha\beta}$ is the \emph{completely
antisymmetric 4-tensor}.\end{defn}
\begin{prop}
\label{prop:dual_tensor_gradient}~
\[
\partial_{\mu}\tilde{F}^{\mu\nu}\left(x\right)=0
\]
\end{prop}
\begin{proof}
Problem \#12.
\end{proof}

\subsection{Euler-Lagrange equations for fields}

Recall that in classical mechanics, for a system with $f$ degrees
of freedom, we had\footnote{In this section and elsewhere, the symbol $\overset{!}{=}$ represents
an equation we assert must be true as part of a proof (such as ``we
must set $\delta S=0$ to obtain extremals'' $\rightarrow$ ``$\delta S\overset{!}{=}0$'').}
\begin{description}
\item [{Lagrangian:}] 
\[
L=L\left(q_{1}\left(t\right),\dots,q_{f}\left(t\right),\dot{q}_{1}\left(t\right),\dots,\dot{q}_{f}\left(t\right)\right)
\]

\item [{action:}] We varied $\boldsymbol{q}\left(t\right)$ and examined
$\delta S$:
\[
S=\indefint tL\left(\boldsymbol{q}\left(t\right),\dot{\boldsymbol{q}}\left(t\right)\right)
\]

\item [{extremals:}] 
\begin{eqnarray*}
0\overset{!}{=}\delta S & = & \indefint t\underset{j}{\sum}\left[\frac{\partial L}{\partial q_{j}}\delta q_{j}+\frac{\partial L}{\partial\dot{q}_{j}}\delta\dot{q}_{j}\right]\\
 & = & \indefint t\underset{j}{\sum}\left[\frac{\partial L}{\partial q_{j}}-\frac{d}{dt}\frac{\partial L}{\partial\dot{q}_{j}}\right]\delta q_{j}\\
\delta q_{j}\textnormal{ arbitrary }\implies0 & = & \frac{\partial L}{\partial q_{j}}-\frac{d}{dt}\frac{\partial L}{\partial\dot{q}_{j}}
\end{eqnarray*}

\end{description}
In field theory, we follow an analogous procedure to obtain the Euler-Lagrange
equations. A scalar field $\phi\left(x\right)=\phi\left(\boldsymbol{x},t\right)$
can be considered a system with $f\rightarrow\text{\ensuremath{\infty}}$
degrees of freedom by discretizing $\phi$; to do so, we identify
$\phi\left(\boldsymbol{x}_{1},t\right):=q_{1}\left(t\right)$, $\phi\left(\boldsymbol{x}_{2},t\right):=q_{2}\left(t\right)$,
etc. Imagine dividing space into cubes with the position vector $\boldsymbol{x}_{j}$
pointing to the $j^{\textnormal{th}}$ such subdivision, then taking
the limit that the number of cube subdivisions goes to infinity.

We now need a ``Lagrangian density'' so that we can integrate over
the volume elements. That is, we now have
\begin{description}
\item [{Lagrangian~density:}] 
\[
\mathscr{L}=\mathscr{L}\left(\phi\left(\boldsymbol{x},t\right),\partial^{\mu}\phi\left(\boldsymbol{x},t\right)\right),
\]
 a function that depends on spatial gradients in addition to time
derivatives. 
\item [{Lagrangian:}] We obtain our Lagrangian by integrating over all
space, such that\footnote{We use the notation $d\boldsymbol{x}$ to represent the volume element
for $\mathbb{R}^{3}$ (in Cartesian coordinates, $d\boldsymbol{x}=dx\,dy\,dz$.} 
\[
L=\indefint{\boldsymbol{x}}\mathscr{L}\left(\phi\left(\boldsymbol{x},t\right),\partial^{\mu}\phi\left(\boldsymbol{x},t\right)\right).
\]

\item [{action:}] 
\begin{eqnarray*}
S=c\int dtL & = & \indefint{x^{0}}\indefint{\boldsymbol{x}}\mathscr{L}\left(\phi\left(\boldsymbol{x},t\right),\partial^{\mu}\phi\left(\boldsymbol{x},t\right)\right)\\
 & = & \indefint{^{4}x}\mathscr{L}\left(\phi\left(x\right),\partial^{\mu}\phi\left(x\right)\right)
\end{eqnarray*}

\item [{extremals:}] As before, we require
\begin{eqnarray*}
0\overset{!}{=}\delta S & = & \indefint{^{4}x}\left[\frac{\partial\mathscr{L}}{\partial\phi}\delta\phi+\frac{\partial\mathscr{L}}{\partial\left(\partial_{\mu}\phi\right)}\delta\left(\partial_{\mu}\phi\right)\right]\\
 & = & \indefint{^{4}x}\left[\frac{\partial\mathscr{L}}{\partial\phi}-\partial_{\mu}\frac{\partial\mathscr{L}}{\partial\left(\partial_{\mu}\phi\right)}\right]\delta\phi\\
\delta\phi\textnormal{ arbitrary }\implies0 & = & \frac{\partial\mathscr{L}}{\partial\phi}-\partial_{\mu}\frac{\partial\mathscr{L}}{\partial\left(\partial_{\mu}\phi\right)},
\end{eqnarray*}
where the second line follows from integration by parts and discarding
the boundary terms.
\end{description}
Thus, we obtain the Euler-Lagrange equations
\begin{equation}
\boxed{\partial_{\mu}\frac{\partial\mathscr{L}}{\partial\left(\partial_{\mu}\phi\right)}=\frac{\partial\mathscr{L}}{\partial\phi}}.\label{eq:E-L}
\end{equation}

\begin{rem}
These are the E-L equations for a scalar field, $\phi$. See Problem
\#13 for a more detailed derivation, in which the functional derivative
of the action is used. For a functional $S=S\left[\phi\left(x\right)\right]$,
the \emph{functional} \emph{derivative} is defined (for vectors $x,y$)
as 
\[
\boxed{\frac{\delta S}{\delta\phi\left(x\right)}:=\underset{\epsilon\rightarrow0}{\lim}\frac{1}{\epsilon}\left(S\left[\phi\left(y\right)+\epsilon\delta\left(y-x\right)\right]-S\left[\phi\left(y\right)\right]\right)}
\]

\end{rem}
~
\begin{rem}
This can be generalized to tensor fields; in fact, you just append
indices to $\phi$.
\end{rem}
~
\begin{rem}
In general, $\mathscr{L}$ will depend on higher order gradients.
Our action depends on gradients of $A^{\mu}\left(x\right)$ by Axiom
3(b). 
\end{rem}
~
\begin{rem}
Our E-L equations for fields are PDEs, in contrast to mechanics where
we only had coupled ODEs!
\end{rem}

\subsection{The field equations\label{sub:ch2sec1_3The-field-equations}}

From Axiom 3(b), the Maxwellian Lagrangian density is 
\[
\boxed{\mathscr{L}=-\frac{1}{16\pi}F_{\mu\nu}F^{\mu\nu}-\frac{1}{c}J^{\mu}A_{\mu}}
\]
\[
=\mathscr{L}\left(A^{\mu}\left(x\right),\partial^{\nu}A^{\mu}\left(x\right)\right).
\]
 Therefore, our Euler-Lagrange system of equations (Equation (\ref{eq:E-L}))
becomes
\begin{equation}
\partial_{\beta}\frac{\partial\mathscr{L}}{\partial\left(\partial_{\beta}A_{\alpha}\left(x\right)\right)}=\frac{\partial\mathscr{L}}{\partial\left(A_{\alpha}\left(x\right)\right)}.\label{eq:EL_for_field_A}
\end{equation}
 Now, $F^{\mu\nu}$ is defined in terms of gradients of $A^{\mu}$
only, so
\begin{eqnarray}
\frac{\partial\mathscr{L}}{\partial\left(A_{\alpha}\left(x\right)\right)} & = & -\frac{1}{c}\frac{\partial}{\partial\left(A_{\alpha}\left(x\right)\right)}\left[J^{\mu}\left(x\right)A_{\mu}\left(x\right)\right]\nonumber \\
 & = & -\frac{1}{c}J^{\alpha}\left(x\right).\label{eq:dL_dA_alpha}
\end{eqnarray}
 On the other side of the equation,
\begin{eqnarray}
\frac{\partial\mathscr{L}}{\partial\left(\partial_{\beta}A_{\alpha}\left(x\right)\right)} & = & -\frac{1}{16\pi}\frac{\partial}{\partial\left(\partial_{\beta}A_{\alpha}\right)}\left[F_{\mu\nu}g^{\kappa\mu}g^{\lambda\nu}F_{\kappa\lambda}\right]\nonumber \\
 & = & -\frac{1}{16\pi}g^{\kappa\mu}g^{\lambda\nu}\frac{\partial}{\partial\left(\partial_{\beta}A_{\alpha}\right)}\left[\left(\partial_{\mu}A_{\nu}-\partial_{\nu}A_{\mu}\right)\left(\partial_{\kappa}A_{\lambda}-\partial_{\lambda}A_{\kappa}\right)\right]\nonumber \\
 & = & -\frac{1}{16\pi}g^{\kappa\mu}g^{\lambda\nu}\left[\left(\delta_{\mu}^{\beta}\delta_{\nu}^{\alpha}-\delta_{\nu}^{\beta}\delta_{\mu}^{\alpha}\right)\left(\partial_{\kappa}A_{\lambda}-\partial_{\lambda}A_{\kappa}\right)+\left(\delta_{\kappa}^{\beta}\delta_{\lambda}^{\alpha}-\delta_{\lambda}^{\beta}\delta_{\kappa}^{\alpha}\right)\left(\partial_{\mu}A_{\nu}-\partial_{\nu}A_{\mu}\right)\right]\nonumber \\
 & = & -\frac{1}{16\pi}\left[\left(g^{\kappa\beta}g^{\lambda\alpha}-g^{\kappa\alpha}g^{\lambda\beta}\right)\left(\partial_{\kappa}A_{\lambda}-\partial_{\lambda}A_{\kappa}\right)+\left(g^{\beta\mu}g^{\alpha\nu}-g^{\alpha\mu}g^{\beta\nu}\right)\left(\partial_{\mu}A_{\nu}-\partial_{\nu}A_{\mu}\right)\right]\nonumber \\
 & = & -\frac{1}{16\pi}\left[\left(\partial^{\beta}A^{\alpha}-\partial^{\alpha}A^{\beta}-\partial^{\alpha}A^{\beta}+\partial^{\beta}A^{\alpha}\right)+\left(\partial^{\beta}A^{\alpha}-\partial^{\alpha}A^{\beta}-\partial^{\alpha}A^{\beta}+\partial^{\beta}A^{\alpha}\right)\right]\nonumber \\
 & = & -\frac{1}{16\pi}\left[4\left(\partial^{\beta}A^{\alpha}-\partial^{\alpha}A^{\beta}\right)\right]\nonumber \\
 & = & -\frac{1}{4\pi}F^{\beta\alpha}.\label{eq:EL_leftside}
\end{eqnarray}
 Inserting Equations (\ref{eq:dL_dA_alpha}) and (\ref{eq:EL_leftside})
into our Euler Lagrange Equation (\ref{eq:EL_for_field_A}), we obtain
\begin{equation}
\boxed{\boxed{\partial_{\mu}F^{\mu\nu}\left(x\right)=\frac{4\pi}{c}J^{\nu}\left(x\right)}}.\label{eq:The_Field_equations}
\end{equation}

\begin{rem}
All physical fields must obey these four equations.
\end{rem}
~
\begin{rem}
Since $F^{\mu\nu}$ is defined in terms of $A^{\mu}$, these equations
are differential equations for $A^{\mu}$, making $A^{\mu}$ the ``fundamental''
physical object. Alternatively, we can augment Equations (\ref{eq:The_Field_equations})
by \cref{prop:dual_tensor_gradient}:
\begin{equation}
\boxed{\boxed{\partial_{\mu}\varepsilon^{\mu\nu\kappa\lambda}F_{\kappa\lambda}\left(x\right)=0}},\label{eq:dual_zero}
\end{equation}
 which contains the structure of $F^{\mu\nu}$ in terms of gradients
of $A^{\mu}$. We can then consider Equations (\ref{eq:The_Field_equations})
and (\ref{eq:dual_zero}) to be field equations for $F^{\mu\nu}$,
regarding $F^{\mu\nu}$ as fundamental.
\end{rem}

\section{Conservation laws and gauge invariance}


\subsection{Continuity equation for the 4-current\label{sub:ch2sec2_1Continuity-equation-for}}
\begin{prop}
\label{prop:The-4-current-obeys}The 4-current obeys the \emph{continuity
equation}:
\[
\boxed{\partial_{\mu}J^{\mu}\left(x\right)=0}.
\]
\end{prop}
\begin{proof}
From \vref{sub:ch2sec1_3The-field-equations}, Equation (\ref{eq:The_Field_equations}),
\[
\partial_{\nu}J^{\nu}=\frac{c}{4\pi}\underset{\textnormal{sym.}}{\underbrace{\partial_{\nu}\partial_{\mu}}}\underset{\textnormal{a-sym.}}{\underbrace{F_{\,}^{\mu\nu}}}=-\frac{c}{4\pi}\underset{\textnormal{relabel }\mu\leftrightarrow\nu}{\underbrace{\partial_{\mu}\partial_{\nu}F^{\nu\mu}}}=-\frac{c}{4\pi}\partial_{\nu}\partial_{\mu}F^{\mu\nu}=-\partial_{\nu}J^{\nu}
\]
\[
\implies\quad\partial_{\nu}J^{\nu}=0
\]
\end{proof}
\begin{rem}
The 4-vector $J^{\mu}=\left(J^{0},\boldsymbol{J}\right)$ has a time-like
component defined as $J^{0}=:c\rho$ and space-like component defined
as $\boldsymbol{J}=:\boldsymbol{j}$. That is,
\[
\boxed{J^{\mu}=:\left(c\rho,\boldsymbol{j}\right)}.
\]
 $\rho$ is called \emph{electric charge density} and $\boldsymbol{j}$
is called \emph{electric current density}.
\end{rem}
~
\begin{rem}
In terms of $\rho$ and $\boldsymbol{j}$, \cref{prop:The-4-current-obeys}
takes the form $c\partial_{0}\rho+\partial_{i}j^{i}=0.$ But $\partial_{0}=\frac{\partial}{\partial\left(ct\right)}=\frac{1}{c}\partial_{t}$
and $\partial_{i}=\frac{\partial}{\partial x^{i}}=:\nabla_{i}$; thus,
the continuity equation is equivalent to
\begin{equation}
\boxed{\partial_{t}\rho\left(\boldsymbol{x},t\right)+\nabla\cdot\boldsymbol{j}\left(\boldsymbol{x},t\right)=0}.\label{eq:continuity_version_2}
\end{equation}

\end{rem}
~
\begin{rem}
Integrate Equation (\ref{eq:continuity_version_2}) over a spatial
volume $V$ with surface boundary $\left(V\right)$:
\[
\partial_{t}\defint{^{3}x}V{}\rho\left(\boldsymbol{x},t\right)=-\defint{^{3}x}V{}\nabla\cdot\boldsymbol{j}\left(\boldsymbol{x},t\right)=-\defint{\boldsymbol{S}}{\left(V\right)}{}\cdot\boldsymbol{j}\left(\boldsymbol{x},t\right).
\]
 Define 
\[
\boxed{Q\left(t\right):=\defint{^{3}x}V{}\rho\left(\boldsymbol{x},t\right)}
\]
 to be the \emph{total charge }within $V$. Then
\[
\frac{dQ}{dt}=-\defint{\boldsymbol{S}}{\left(V\right)}{}\cdot\boldsymbol{j}.
\]
 In words, the total charge within $V$ can only change if there is
a flux of charge current through the boundary surface $\left(V\right)$,
hence the name ``continuity equation''.\footnote{This ``conservation of charge'' is a result of our field equations.
The field equations are in turn a result of the actions we have postulated
through axioms.}
\end{rem}

\subsection{\label{sub:ch2sec2_2The-energy-momentum-tensor}The energy-momentum
tensor}
\begin{defn}
\label{def:energy-momentum-tensor-field}\textbf{\emph{Electromagnetic
energy-momentum tensor.}} The tensor field $T^{\mu\nu}\left(x\right)$,
defined as
\[
\boxed{T^{\mu\nu}:=-\frac{1}{4\pi}F^{\mu\alpha}F{}^{\nu}{}_{\alpha}+\frac{1}{16\pi}g^{\mu\nu}F_{\alpha\beta}F^{\alpha\beta}},
\]
 is called the \emph{electromagnetic energy-momentum tensor}. \end{defn}
\begin{rem}
It is not obvious what this tensor has to do with energy and momentum
for now; see Problem \#16 for some hints and LL for details.\end{rem}
\begin{prop}
~

\textbf{\emph{(1)}} $T^{\mu\nu}$ is symmetric; $\boxed{T^{\mu\nu}\left(x\right)=T^{\nu\mu}\left(x\right)}$.

\textbf{\emph{(2)}} $T^{\mu\nu}$ is traceless; $\boxed{T{}_{\mu}{}^{\mu}\left(x\right)=0}$.\end{prop}
\begin{proof}
~

\textbf{(1)} We know the second term in the definition of $T^{\mu\nu}$
is symmetric. For the first term,
\[
F^{\mu\alpha}F{}^{\nu}{}_{\alpha}=g^{\alpha\beta}F{}^{\mu}{}_{\beta}g_{\alpha\gamma}F^{\nu\gamma}=\delta_{\gamma}^{\beta}F{}^{\mu}{}_{\beta}F^{\nu\gamma}=F^{\nu\beta}F{}^{\mu}{}_{\beta}.
\]
 Thus the first term is symmetric and, in turn, $T^{\mu\nu}$ is symmetric.

~

\textbf{(2)} $-4\pi T{}^{\mu}{}_{\mu}=F^{\mu\alpha}F_{\mu\alpha}-\frac{1}{4}\underset{\delta_{\mu}^{\mu}=4}{\underbrace{g{}^{\mu}{}_{\mu}}}F_{\alpha\beta}F^{\alpha\beta}=0$\end{proof}
\begin{rem}
By \cref{chap:Mathematical-preliminaries}, \cref{sub:ch1sec3_4Minkowski-tensors},
$T^{\mu\nu}$ can be decomposed into $T^{00}$, Euclidean vector $T^{0j}$,
plus symmetric Euclidean tensor $T^{jk}$.
\end{rem}

\subsection{The continuity equation for the energy-momentum tensor\label{sub:ch2sec2_3The-continuity-equation}}
\begin{prop}
In the absence of matter ($J^{\mu}=0$), $T^{\mu\nu}$ obeys
\[
\partial_{\nu}T{}_{\mu}{}^{\nu}\left(x\right)=0.
\]
\end{prop}
\begin{proof}
From \cref{def:energy-momentum-tensor-field},\footnote{In the first line, we use the notational convention that $\partial_{\nu}F{}_{\mu}{}^{\alpha}F{}^{\nu}{}_{\alpha}$
implies that the partial acts on everything to the right; that is,
$\partial_{\nu}F{}_{\mu}{}^{\alpha}F{}^{\nu}{}_{\alpha}:=\partial_{\nu}\left(F{}_{\mu}{}^{\alpha}F{}^{\nu}{}_{\alpha}\right)$.
If we wanted the partial only acting on $F{}_{\mu}{}^{\alpha}$, we
would have to write $\left(\partial_{\nu}F{}_{\mu}{}^{\alpha}\right)F{}^{\nu}{}_{\alpha}$.} 
\begin{eqnarray*}
\partial_{\nu}T{}_{\mu}{}^{\nu} & = & \frac{1}{4\pi}\left[-\partial_{\nu}F{}_{\mu}{}^{\alpha}F{}^{\nu}{}_{\alpha}+\frac{1}{4}\partial_{\nu}\delta_{\mu}^{\nu}F_{\alpha\beta}F^{\alpha\beta}\right]\\
 & = & \frac{1}{4\pi}\left[-\left(\partial_{\nu}F{}_{\mu}{}^{\alpha}\right)F{}^{\nu}{}_{\alpha}-F{}_{\mu}{}^{\alpha}\partial_{\nu}F{}^{\nu}{}_{\alpha}+\frac{1}{4}\partial_{\mu}F_{\alpha\beta}F^{\alpha\beta}\right].
\end{eqnarray*}
 But by Equation (\ref{eq:The_Field_equations}), $\partial_{\nu}F{}^{\nu}{}_{\alpha}=\frac{4\pi}{c}J_{\alpha}=0$.
Furthermore, the last term can be rewritten as follows:
\begin{eqnarray*}
F_{\alpha\beta}\partial_{\mu}F^{\alpha\beta} & = & g_{\alpha\gamma}g_{\beta\kappa}F^{\gamma\kappa}\partial_{\mu}g^{\alpha\epsilon}g^{\beta\nu}F_{\epsilon\nu}=\delta_{\kappa}^{\nu}\delta_{\gamma}^{\epsilon}F^{\gamma\kappa}\partial_{\mu}F_{\epsilon\nu}=F^{\epsilon\kappa}\partial_{\mu}F_{\epsilon\kappa}\\
 & = & \left(\partial_{\mu}F_{\alpha\beta}\right)F^{\alpha\beta}
\end{eqnarray*}
\[
\implies\quad\partial_{\mu}F_{\alpha\beta}F^{\alpha\beta}=2\left(\partial_{\mu}F_{\alpha\beta}\right)F^{\alpha\beta}.
\]
\[
\implies\quad\partial_{\nu}T{}_{\mu}{}^{\nu}=\frac{1}{4\pi}\left[-\left(\partial_{\nu}F{}_{\mu}{}^{\alpha}\right)F{}^{\nu}{}_{\alpha}+\frac{1}{2}\left(\partial_{\mu}F_{\alpha\beta}\right)F^{\alpha\beta}\right].
\]
 By Problem \#12 (see Belitz's solution), 
\[
0=\partial_{\mu}F_{\alpha\beta}+\partial_{\alpha}F_{\beta\mu}+\partial_{\beta}F_{\mu\alpha}.
\]
\begin{eqnarray*}
\implies\quad\partial_{\nu}T{}_{\mu}{}^{\nu} & = & \frac{1}{4\pi}\left[-\left(\partial_{\nu}F{}_{\mu}{}^{\alpha}\right)F{}^{\nu}{}_{\alpha}-\frac{1}{2}\left(\partial_{\alpha}F_{\beta\mu}+\partial_{\beta}F_{\mu\alpha}\right)F^{\alpha\beta}\right]\\
 & = & \frac{1}{4\pi}\left[-\left(\partial_{\nu}F_{\mu\alpha}\right)F^{\nu\alpha}+\frac{1}{2}\underset{\alpha\rightarrow\nu,\,\beta\rightarrow\alpha}{\underbrace{\left(\partial_{\alpha}F_{\mu\beta}\right)F^{\alpha\beta}}}+\frac{1}{2}\underset{\beta\rightarrow\nu}{\underbrace{\left(\partial_{\beta}F_{\mu\alpha}\right)F^{\beta\alpha}}}\right].\\
 & = & \frac{1}{4\pi}\left[-\left(\partial_{\nu}F_{\mu\alpha}\right)F^{\nu\alpha}+\frac{1}{2}\left(\partial_{\nu}F_{\mu\alpha}\right)F^{\nu\alpha}+\frac{1}{2}\left(\partial_{\nu}F_{\mu\alpha}\right)F^{\nu\alpha}\right]\\
 & = & 0
\end{eqnarray*}
Note that in the third to last line we used the identity that, for
any tensor contraction,
\[
t^{\left(\cdots\right)\alpha\left(\cdots\right)}{}_{\alpha}{}^{\left(\cdots\right)}=t^{\left(\cdots\right)}{}_{\alpha}{}^{\left(\cdots\right)\alpha\left(\cdots\right)}.
\]
 That is, contracted indices can swap being upstairs or downstairs.\end{proof}
\begin{rem}
For any rank-$\left(n+1\right)$ tensor field $t^{\mu\alpha_{1}\dots\alpha_{n}}\left(x\right)$,
the continuity equation $\partial_{\mu}t^{\mu\alpha_{1}\dots\alpha_{n}}=0$
implies a conservation law for the rank-$n$ tensor $t^{0\alpha_{1}\dots\alpha_{n}}\left(x\right)$
by the arguments from \cref{sub:ch2sec2_1Continuity-equation-for}.
$\partial_{\mu}J^{\mu}=0$ is the case where $n=0$; the proposition
above is the case where $n=1$.\end{rem}
\begin{cor}
In the presence of matter, the continuity equation gets modified to
\[
\boxed{\partial_{\nu}T{}_{\mu}{}^{\nu}=-\frac{1}{c}F{}_{\mu}{}^{\nu}J_{\nu}}.
\]
\end{cor}
\begin{proof}
Problem \#17.
\end{proof}

\subsection{\label{sub:ch2sec2_4Gauge-invariance}Gauge invariance}

Let $\chi\left(x\right)$ be an arbitrary scalar function of spacetime.
\begin{defn}
\textbf{\emph{Gauge transformation.}} A transformation of the potential
$A^{\mu}\left(x\right)$ according to 
\[
\boxed{A^{\mu}\rightarrow A^{\mu}-\partial^{\mu}\chi}
\]
 is called a \emph{gauge transformation.}\end{defn}
\begin{prop}
The action from Axiom 3 is invariant under gauge transformations.\end{prop}
\begin{proof}
$F^{\mu\nu}=\partial^{\mu}A^{\nu}-\partial^{\nu}A^{\mu}\rightarrow\partial^{\mu}A^{\nu}-\partial^{\mu}\partial^{\nu}\chi-\partial^{\nu}A^{\mu}+\partial^{\nu}\partial^{\mu}\chi=\partial^{\mu}A^{\nu}-\partial^{\nu}A^{\mu}=F^{\mu\nu},$
so $S_{\textnormal{vac}}$ is invariant.

$S_{\textnormal{int}}=-\frac{1}{c}\int d^{4}xJ_{\mu}A^{\mu}\rightarrow S_{\textnormal{int}}-\underset{\textnormal{integ. by parts}}{\underbrace{\frac{1}{c}\int d^{4}xJ_{\mu}\partial^{\mu}\chi}}=S_{\textnormal{int}}+\frac{1}{c}\int d^{4}x\underset{=0}{\underbrace{\left(\partial^{\mu}J_{\mu}\right)}}\chi=S_{\textnormal{int}},$
so $S_{\textnormal{int}}$ is invariant. Therefore the total action
is invariant.\end{proof}
\begin{rem}
The potential is not unique. This is a result of the fact that $F^{\mu\nu}$
depends only on gradients of $A^{\mu}$.
\end{rem}
~
\begin{rem}
We may choose a gauge transformation to enforce a particular condition
on $A^{\mu}$.\end{rem}
\begin{cor}
$A^{\mu}\left(x\right)$ can always be chosen (gauge transformed)
such that 
\[
\partial_{\mu}A^{\mu}=0,
\]
 called the \emph{Lorenz gauge}.\end{cor}
\begin{proof}
Choose $\chi\left(x\right)$ such that it solves the PDE $\partial^{\mu}\partial_{\mu}\chi=\partial_{\mu}A^{\mu}$
(Laplace's equation). 
\[
\implies\quad\partial_{\mu}A^{\mu}\rightarrow\partial_{\mu}A^{\mu}-\underset{=\partial_{\mu}A^{\mu}}{\underbrace{\partial_{\mu}\partial^{\mu}\chi}}=0.
\]
\end{proof}
\begin{rem}
$\partial_{\mu}A^{\mu}$ is a Lore\textbf{ntz} scalar, and so the
Lore\textbf{nz} gauge is Lore\textbf{ntz} invariant.
\end{rem}

\section{Electric and magnetic fields}


\subsection{The field tensor in terms of Euclidean vector fields\label{sub:ch2sec3_1The-field-tensor}}

Since $F^{\mu\nu}$ is an antisymmetric Minkowski tensor, from \cref{chap:Mathematical-preliminaries},
\cref{sub:ch1sec3_4Minkowski-tensors} we can write it in the form\begin{eqnarray*} F_{\mu\nu} & = & 
\left(\begin{array}{c;{2pt/2pt}ccc}
0      & E_{x}  & E_{y}  & E_{z}  \\\hdashline[2pt/2pt]
-E_{x} & 0      & -B_{z} & B_{y}  \\
-E_{y} & B_{z}  & 0      & -B_{x} \\
-E_{x} & -B_{y} & B_{x}  & 0      \\
\end{array}\right)\\
& = & 
\left(\begin{array}{c;{2pt/2pt}c}
0      		 & \boldsymbol{E}\\\hdashline[2pt/2pt]
-\boldsymbol{E} & B_{jk}\\
\end{array}\right)\\
\end{eqnarray*}

...with $\boldsymbol{E}\left(x\right)=\left(E_{x}\left(x\right),E_{y}\left(x\right),E_{z}\left(x\right)\right)$
a Euclidean vector field, 

...and $\boldsymbol{B}\left(x\right)=\left(B_{x}\left(x\right),B_{y}\left(x\right),B_{z}\left(x\right)\right)$
a Euclidean pseudovector field.

Beware! There are some subtle notational details here. The above definition
uses Landau \& Lifshitz's notation. In terms of the numerical indices
used throughout the rest of this text, these vector fields are 
\begin{eqnarray*}
\boldsymbol{E} & = & \left(E_{x},E_{y},E_{z}\right):=\left(E^{1},E^{2},E^{3}\right)=-\left(E_{1},E_{2},E_{3}\right)\\
\boldsymbol{B} & = & \left(B_{x},B_{y},B_{z}\right):=\left(B^{1},B^{2},B^{3}\right)=-\left(B_{1},B_{2},B_{3}\right).
\end{eqnarray*}
One must be careful to not identify $E_{x}$ with $E_{1}$!
\begin{defn}
\textbf{\emph{Electric and magnetic field; magnetic field tensor.}}

\textbf{(a)} $\boldsymbol{E}\left(x\right)$ is called \emph{electric
field}; $\boldsymbol{B}\left(x\right)$ is called \emph{magnetic field.}

\textbf{(b)} The antisymmetric Euclidean tensor
\[
B_{jk}=\begin{pmatrix}0 & -B_{z} & B_{y}\\
B_{z} & 0 & -B_{x}\\
-B_{y} & B_{x} & 0
\end{pmatrix}=B^{jk}=-\varepsilon^{jkl}B_{l}
\]
 is called \emph{magnetic field tensor.}\end{defn}
\begin{rem}
What about the contravariant components $F^{\mu\nu}$?

\begin{minipage}[t]{1\columnwidth}%
\[
F^{\mu\nu}=g^{\mu\alpha}g^{\nu\beta}F_{\alpha\beta}=\begin{pmatrix}+\\
-\\
-\\
-
\end{pmatrix}_{\mu}\begin{pmatrix}+\\
-\\
-\\
-
\end{pmatrix}_{\nu}F_{\mu\nu}
\]
\begin{eqnarray*}  & = & 
\left(\begin{array}{c;{2pt/2pt}c}
0      		 & -\boldsymbol{E}\\\hdashline[2pt/2pt]
\boldsymbol{E} & B^{jk}=B_{jk}\\
\end{array}\right)\\
\end{eqnarray*}%
\end{minipage}
\end{rem}
\ From \cref{chap:Mathematical-preliminaries} \cref{sub:ch1sec3_4Minkowski-tensors},
we can write $A^{\mu}$ in the form
\[
A^{\mu}\left(x\right)=\left(\phi\left(x\right),\boldsymbol{A}\left(x\right)\right),
\]


...with $\phi\left(x\right):=A^{0}\left(x\right)=A_{0}\left(x\right)$
a Euclidean scalar field, 

...and $\boldsymbol{A}\left(x\right)=\left(A^{1}\left(x\right),A^{2}\left(x\right),A^{3}\left(x\right)\right)$
a Euclidean vector field.
\begin{defn}
\textbf{\emph{Scalar and vector potential.}} $\phi\left(x\right)$
is called \emph{scalar potential}, and $\boldsymbol{A}\left(x\right)$
is called \emph{vector potential}. \end{defn}
\begin{rem}
This is analogous to $J^{\mu}\left(x\right)=\left(c\rho\left(x\right),\boldsymbol{j}\left(x\right)\right)$,
with $\rho$ the charge density and $\boldsymbol{j}$ the current
density (see \cref{sub:ch2sec2_1Continuity-equation-for}).
\end{rem}

\subsection{Maxwell's equations}

From \cref{sub:ch2sec1_3The-field-equations} Equation (\ref{eq:dual_zero}),
we have 
\[
\partial_{\mu}\varepsilon^{\mu\nu\kappa\lambda}F_{\kappa\lambda}=0.
\]
 What are these in terms of $\boldsymbol{E}\left(x\right)$ and $\boldsymbol{B}\left(x\right)$? 
\begin{prop}
The field equation 
\[
\partial_{\mu}\varepsilon^{\mu\nu\kappa\lambda}F_{\kappa\lambda}=0
\]
 is equivalent to\footnote{Belitz refers to these as $\left(1\right)$ and $\left(2\right)$
instead of $\left(M1\right)$ and $\left(M2\right)$, respectively.}
\[
\boxed{\boxed{\nabla\cdot\boldsymbol{B}=0}}\tag{M1}
\]
\[
\boxed{\boxed{\frac{1}{c}\partial_{t}\boldsymbol{B}+\nabla\times\boldsymbol{E}=\boldsymbol{0}}}\tag{M2}
\]
\end{prop}
\begin{proof}
~

$\underline{\nu=0}$:$\quad$ Note that for, say, $\mu=1$, we obtain
$\partial_{1}\varepsilon^{10\kappa\lambda}F_{\kappa\lambda}=-\partial_{1}F_{23}+\partial_{1}F_{32}=-2\partial_{1}F_{23}$.
Thus, 
\[
0=2\left(-\partial_{1}F_{23}-\partial_{2}F_{31}-\partial_{3}F_{12}\right)=2\underset{\nabla\cdot\boldsymbol{B}}{\underbrace{\left(\partial_{1}B_{x}+\partial_{2}B_{y}+\partial_{3}B_{z}\right)}}\implies\nabla\cdot\boldsymbol{B}=0
\]


$\underline{\nu=1}$:$\quad$ Again, for a given choice of $\mu$,
the above simplification applies. Thus,
\[
0=2\left(\partial_{0}F_{23}-\partial_{2}F_{03}+\partial_{3}F_{02}\right)=2(-\frac{1}{c}\partial_{t}B_{x}\underset{-\left(\nabla\times\boldsymbol{E}\right)_{x}}{\underbrace{-\partial_{2}E_{z}+\partial_{3}E_{y}}})\implies\frac{1}{c}\partial_{t}B_{x}+\left(\nabla\times\boldsymbol{E}\right)_{x}=0
\]


$\underline{\nu=2,3}$:$\quad$ Cyclically permute the $\nu=1$ case.\end{proof}
\begin{rem}
These are the four homogeneous PDEs known as the first two Maxwell
Equations.

Now consider, from \cref{sub:ch2sec1_3The-field-equations}, the Euler-Lagrange
equation (\ref{eq:The_Field_equations}):
\[
\partial_{\mu}F^{\mu\nu}=\frac{4\pi}{c}J^{\nu}.
\]
What are these in terms of $\boldsymbol{E}\left(x\right)$ and $\boldsymbol{B}\left(x\right)$? \end{rem}
\begin{prop}
The field equation 
\[
\partial_{\mu}F^{\mu\nu}=\frac{4\pi}{c}J^{\nu}
\]
 is equivalent to 
\[
\boxed{\boxed{\nabla\cdot\boldsymbol{E}=4\pi\rho}}\tag{M3}
\]
\[
\boxed{\boxed{-\frac{1}{c}\partial_{t}\boldsymbol{E}+\nabla\times\boldsymbol{B}=\frac{4\pi}{c}\boldsymbol{j}}}\tag{M4}
\]
\end{prop}
\begin{proof}
~

$\underline{\nu=0}$:$\quad$ $\underset{0}{\underbrace{\partial_{0}F^{00}}}+\underset{\nabla\cdot\boldsymbol{E}}{\underbrace{\partial_{j}F^{j0}}}=\frac{4\pi}{c}\underset{c\rho}{\underbrace{J^{0}}}\implies\nabla\cdot\boldsymbol{E}=4\pi\rho$

$\underline{\nu=1}$:$\quad$ $\partial_{0}F^{01}+\partial_{i}F^{i1}=\frac{4\pi}{c}J^{1}\implies-\frac{1}{c}\partial_{t}E_{x}+\partial_{2}B_{z}-\partial_{3}B_{y}=-\frac{1}{c}\partial_{t}E_{x}+\left(\nabla\times\boldsymbol{B}\right)_{x}=\frac{4\pi}{c}j_{x}.$

$\underline{\nu=2,3}$:$\quad$ Cyclically permute the $\nu=1$ case.\end{proof}
\begin{rem}
Equations (M1)-(M4) are called \emph{Maxwell's Equations}. Their solutions
determine physical field equations for given charge and current densities.
\end{rem}
~
\begin{rem}
Equations (M1)-(M4) are equivalent to Equations (\ref{eq:dual_zero})
and (\ref{eq:The_Field_equations}).
\end{rem}
~
\begin{rem}
$\boldsymbol{E}$ and $\boldsymbol{B}$ are Euclidean vector fields,
so the Lorentz invariance thereof is obscured.
\end{rem}
~
\begin{rem}
\emph{Units}: We use CGS (centimeter-gram-second) units, not SI units
(see table below).  At some point when I have time I will write Maxwell's
equations in SI units here.
\end{rem}
\begin{center}
\begin{table}[H]
\begin{centering}
\begin{tabular}{ccc}
$\left[\textnormal{unit}\right]$ & CGS & SI\tabularnewline
\hline 
\hline 
\noalign{\vskip\doublerulesep}
$\left[\textnormal{charge}\right]$ & $\textnormal{esu}=\si{g^{1/2}.cm^{3/2}.s^{-1}}$ & $\si{\coulomb}$\tabularnewline[\doublerulesep]
\noalign{\vskip\doublerulesep}
\hline 
\noalign{\vskip\doublerulesep}
$\left[\rho\right]$ & $\si{g^{1/2}.cm^{-3/2}.s^{-1}}$ & $\si{\coulomb.m^{-3}}$\tabularnewline[\doublerulesep]
\noalign{\vskip\doublerulesep}
\hline 
\noalign{\vskip\doublerulesep}
$\left[\boldsymbol{j}\right]$ & $\si{g^{1/2}.cm^{-1/2}.s^{-2}}$ & $\si{\coulomb.m^{-2}.s^{-1}}$\tabularnewline[\doublerulesep]
\noalign{\vskip\doublerulesep}
\hline 
\noalign{\vskip\doublerulesep}
$\left[\boldsymbol{E}\right]$ & $\si{g^{1/2}.cm^{-1/2}.s^{-1}}$ & $\si{\newton.\coulomb^{-1}}$\tabularnewline[\doublerulesep]
\noalign{\vskip\doublerulesep}
\hline 
\noalign{\vskip\doublerulesep}
$\left[\boldsymbol{B}\right]$ & $\begin{array}{ccc}
\textnormal{gauss} & = & \si{g^{1/2}.cm^{1/2}.s^{-1}}\\
 & = & \textnormal{esu}\,\si{cm^{-2}}
\end{array}$ & $\si{\newton.\ampere^{-1}.m^{-1}}$\tabularnewline[\doublerulesep]
\noalign{\vskip\doublerulesep}
\end{tabular}
\par\end{centering}

\protect\caption{Comparison of CGS and SI units.}


\end{table}

\par\end{center}


\subsection{Discussion of Maxwell's equations}


\subsubsection*{Gauss' Law}

Consider a localized charge density $\rho$ in a larger volume $V$
with boundary surface $\left(V\right)$. Integrate (M3) over $V$:
\[
\underset{V}{\int}d^{3}x\:\nabla\cdot\boldsymbol{E}\left(\boldsymbol{x},t\right)=4\pi\underset{V}{\int}d^{3}x\:\rho\left(\boldsymbol{x},t\right).
\]
 If we define the \emph{total charge} within $V$ to be
\[
\boxed{Q\left(t\right):=\underset{V}{\int}d^{3}x\:\rho\left(\boldsymbol{x},t\right)}
\]
 then by using this and the Divergence Theorem above, we obtain
\[
\boxed{\Phi_{\boldsymbol{E}}:=\underset{\left(V\right)}{\int}d\boldsymbol{S}\cdot\boldsymbol{E}=4\pi Q}
\]
 In words, the flux of electric field through a closed surface is
equal to the total charge contained therein.
\begin{rem}
This is called \emph{Gauss' Law}.
\end{rem}
~
\begin{rem}
Electric charges are the sources of electric fields.
\end{rem}

\subsubsection*{Magnetic field divergence}

Integrate (M1) over $V$:
\[
0=\underset{V}{\int}d^{3}x\:\nabla\cdot\boldsymbol{B}\left(\boldsymbol{x},t\right).
\]
 Again, the Divergence Theorem yields
\[
\boxed{\Phi_{\boldsymbol{B}}:=\underset{\left(V\right)}{\int}d\boldsymbol{S}\cdot\boldsymbol{B}=0}.
\]
 In words, the flux of magnetic field through a closed surface is
always zero.
\begin{rem}
The magnetic field has no sources; there are no magnetic monopoles.
\end{rem}
~
\begin{rem}
In our Lorentz invariant formulation of 1, this comes from the asymmetry
of the two field equations:
\[
\partial_{\mu}F^{\mu\nu}=\frac{4\pi}{c}J^{\mu}\quad\textnormal{and}\quad\partial_{\mu}\tilde{F}^{\mu\nu}=0.
\]

\end{rem}

\subsubsection*{Faraday's Law}

Consider a surface $S$ with boundary $\left(S\right)$. 

Integrate (M2) over $S$:
\[
-\frac{1}{c}\underset{S}{\int}d\boldsymbol{S}\cdot\partial_{t}\boldsymbol{B}\left(\boldsymbol{x},t\right)=\underset{S}{\int}d\boldsymbol{S}\cdot\left(\nabla\times\boldsymbol{E}\right)\left(\boldsymbol{x},t\right).
\]
 By Stokes' Theorem,
\[
\boxed{-\dot{\Phi}_{\boldsymbol{B}}=c\underset{\left(S\right)}{\oint}d\boldsymbol{l}\cdot\boldsymbol{E}}
\]
 In words, the circulation of the electric field around a loop is
proportional to the time rate of change of the magnetic flux through
a surface bounded by that loop.
\begin{rem}
This is called \emph{Faraday's Law of induction.}
\end{rem}
~
\begin{rem}
Consider a closed $\boldsymbol{E}-$field line. $ $$\oint d\boldsymbol{l}\cdot\boldsymbol{E}>0\implies\dot{\Phi}_{\boldsymbol{B}}<0$.
So in a static $\boldsymbol{B}-$field, there can be \emph{no} closed
$\boldsymbol{E}-$field lines! 
\end{rem}

\subsubsection*{Ampre-Maxwell Law}

Integrate (M4) over $S$:
\[
\underset{S}{\int}d\boldsymbol{S}\cdot\left(\nabla\times\boldsymbol{B}\right)\left(\boldsymbol{x},t\right)=\frac{4\pi}{c}\underset{S}{\int}d\boldsymbol{S}\cdot\boldsymbol{J}\left(\boldsymbol{x},t\right)+\frac{1}{c}\underset{S}{\int}d\boldsymbol{S}\cdot\partial_{t}\boldsymbol{E}\left(\boldsymbol{x},t\right).
\]
 We define the \emph{total current} to be 
\[
\boxed{I\left(t\right):=\underset{S}{\int}d\boldsymbol{S}\cdot\boldsymbol{J}\left(\boldsymbol{x},t\right)}.
\]
 Using Stokes' Law once more yields
\[
\boxed{c\underset{\left(S\right)}{\oint}d\boldsymbol{l}\cdot\boldsymbol{B}=4\pi I+\dot{\Phi}_{\boldsymbol{E}}}
\]
 In words, the circulation of the magnetic field around a loop is
proportional to the sum of the total current and the \emph{displacement
current}. 
\begin{rem}
This is called the\emph{ Ampre-Maxwell Law}. 
\end{rem}
~
\begin{rem}
Currents induce $\boldsymbol{B}-$fields, and vice versa.
\end{rem}
~
\begin{rem}
For static fields, we have Ampre's Law:
\[
c\oint d\boldsymbol{l}\cdot\boldsymbol{B}=4\pi I.
\]
 The displacement current was later added by Maxwell.
\end{rem}

\subsection{Relations between fields and potentials\label{sub:ch2sec3_4Relations-between-fields}}
\begin{claim}
The electric and magnetic fields are related to the 4-potential by
\[
\boxed{\boldsymbol{E}=-\nabla\phi-\frac{1}{c}\partial_{t}\boldsymbol{A}}
\]
\[
\boxed{\boldsymbol{B}=\nabla\times\boldsymbol{A}}
\]
\end{claim}
\begin{proof}
From \cref{sub:ch2sec3_1The-field-tensor},
\[
E^{j}=-F^{0j}=-\partial^{0}A^{j}+\partial^{j}A^{0}=-\partial_{0}A^{j}-\partial_{j}A^{0}=-\frac{1}{c}\partial_{t}A^{j}-\partial_{j}\phi.
\]
 We also determined
\[
F_{12}=-B^{3}=\partial_{1}A_{2}-\partial_{2}A_{1}=\left(\nabla\times\boldsymbol{A}\right)_{3}=-\left(\nabla\times\boldsymbol{A}\right)^{3},
\]
 and cyclic for $B^{2}$, $B^{1}$.\end{proof}
\begin{rem}
In general, both the scalar and vector potentials determine $\boldsymbol{E}$.
\end{rem}
~
\begin{rem}
As a safety check, lets try gauge transforming the relation for $\boldsymbol{E}$:
\[
A^{\mu}\rightarrow A^{\mu}-\partial^{\mu}\chi\quad\implies\quad\begin{cases}
\phi\rightarrow & \phi-\frac{1}{c}\partial_{t}\chi\\
\boldsymbol{A}\rightarrow & \boldsymbol{A}+\nabla\chi
\end{cases}
\]
\[
\implies\boldsymbol{E}\rightarrow\boldsymbol{E}+\nabla\frac{1}{c}\partial_{t}\chi-\frac{1}{c}\partial_{t}\nabla\chi=\boldsymbol{E}.
\]
 Thus, $\boldsymbol{E}$ is invariant.
\end{rem}
~
\begin{rem}
$\boldsymbol{B}$ is also invariant under gauge transformations since
$\nabla\times\left(\nabla\chi\right)=0$.
\end{rem}

\subsection{\label{sub:ch2sec3_5Charges-in-electromagnetic}Charges in electromagnetic
fields}

So far, our attitude has been that the field equations determine $\boldsymbol{E}$
and $\boldsymbol{B}$ for given charges and currents. What about the
converse? For given fields, what is their influence on a point charge?

Let a point particle with charge $e$ by at point $\boldsymbol{y}\left(t\right)$
with velocity $\dot{\boldsymbol{y}}\left(t\right)=:\boldsymbol{v}\left(t\right).$
\begin{description}
\item [{charge~density}] $\cdots\quad\rho\left(\boldsymbol{x},t\right)=e\delta\left(\boldsymbol{x}-\boldsymbol{y}\left(t\right)\right)$
\item [{current~density}] $\cdots\quad\boldsymbol{j}\left(\boldsymbol{x},t\right)=\rho\left(\boldsymbol{x},t\right)\boldsymbol{v}\left(t\right)$
\item [{4-current}] $\cdots\quad J^{\mu}=\left(c\rho,\boldsymbol{j}\right),\,J_{\mu}=\left(c\rho,-\boldsymbol{j}\right)$
\item [{4-potential}] $\cdots\quad A^{\mu}=\left(\phi,\boldsymbol{A}\right)$
\end{description}
By Axiom 3,
\begin{eqnarray*}
S_{\textnormal{int}} & = & -\frac{1}{c}\indefint{^{4}x}J_{\mu}\left(x\right)A^{\mu}\left(x\right)\\
 & = & -\frac{1}{c}\indefint t\indefint{\boldsymbol{x}}c\rho\left(\boldsymbol{x},t\right)\phi\left(\boldsymbol{x},t\right)+\frac{1}{c}\indefint t\indefint{\boldsymbol{x}}\boldsymbol{j}\left(\boldsymbol{x},t\right)\cdot\boldsymbol{A}\left(\boldsymbol{x},t\right)\\
 & = & -e\indefint t\phi\left(\boldsymbol{y},t\right)+\frac{e}{c}\indefint t\boldsymbol{v}\left(t\right)\cdot\boldsymbol{A}\left(\boldsymbol{y},t\right).
\end{eqnarray*}
 Now consider the Lagrangian of the point particle, $\mathcal{L}_{\textnormal{int}}=\mathcal{L}_{\textnormal{int}}\left(\boldsymbol{y},\dot{\boldsymbol{y}},t\right)$,
which is related to $S_{\textnormal{int}}$ via $S_{\textnormal{int}}=\int dt\mathcal{L}_{\textnormal{int}}\left(\boldsymbol{y},\dot{\boldsymbol{y}},t\right)$.
Comparison with the above equation reveals
\[
\boxed{\mathcal{L}_{\textnormal{int}}\left(\boldsymbol{y},\dot{\boldsymbol{y}},t\right)=-e\phi\left(\boldsymbol{y},t\right)+\frac{e}{c}\boldsymbol{v}\left(t\right)\cdot\boldsymbol{A}\left(\boldsymbol{y},t\right)}.
\]

\begin{rem}
These are the scalar and vector potentials from PHYS611 Ch2 1.3 Example
1!
\end{rem}
~
\begin{rem}
Axiom 3 is consistent with our Mechanics axioms.
\end{rem}
~
\begin{rem}
$\mathcal{L}_{\textnormal{int}}$ must be augmented by the free particle
Lagrangian $\mathcal{L}_{0}$. Since the field equations are Lorentz
invariant, we must pick $\mathcal{L}_{0}$ such that it is as well;
we need the Einsteinian $\mathcal{L}_{0}^{E}$ for consistency. However,
the Galilean $\mathcal{L}_{0}^{G}$ works well enough if $\left|\boldsymbol{v}\right|\ll c$.
\end{rem}
~
\begin{rem}
The momentum of the particle is $\boldsymbol{p}=\frac{\partial\mathcal{L}_{0}}{\partial\boldsymbol{v}}$
(not $\frac{\partial\mathcal{L}}{\partial\boldsymbol{v}}$; see PHYS611
Ch2 1.4), and Newton's 2nd Law takes the form (from PHYS611):
\begin{eqnarray*}
\frac{d}{dt}\boldsymbol{p}=\boldsymbol{F} & = & \boldsymbol{F}^{\left(1\right)}+\boldsymbol{F}^{\left(2\right)}\\
 & = & -\nabla\underset{U}{\underbrace{e\phi}}-\left(\partial_{t}-\boldsymbol{v}\times\right)\underset{\boldsymbol{V}}{\underbrace{\left(\frac{e}{c}\boldsymbol{A}\right)}}\\
 & = & e(\underset{\boldsymbol{E}}{\underbrace{-\nabla\phi-\frac{1}{c}\partial_{t}\boldsymbol{A}}})+\frac{e}{c}\boldsymbol{v}\times(\underset{\boldsymbol{B}}{\underbrace{\nabla\times\boldsymbol{A}}})
\end{eqnarray*}
\[
\implies\boxed{\frac{d}{dt}\boldsymbol{p}=\boldsymbol{F}=e\boldsymbol{E}+\frac{e}{c}\boldsymbol{v}\times\boldsymbol{B}}
\]
which is the electric force plus the Lorentz force! In conclusion,
all of this is consistent with what we did in Mechanics.
\end{rem}

\subsection{\label{sub:ch2sec3_6Poynting's-theorem}Poynting's theorem}

Consider the continuity equation for the energy-momentum tensor
\[
\partial_{\nu}T^{\mu\nu}=-\frac{1}{c}F^{\mu\nu}J_{\nu}
\]
 from \cref{sub:ch2sec2_3The-continuity-equation} for $\mu=0$:\footnote{Recall that $T^{\mu\nu}:=-\frac{1}{4\pi}F^{\mu\alpha}F{}^{\nu}{}_{\alpha}+\frac{1}{16\pi}g^{\mu\nu}F_{\alpha\beta}F^{\alpha\beta}$
from \cref{sub:ch2sec2_2The-energy-momentum-tensor}, and $F_{\alpha\beta}F^{\alpha\beta}=2\left(\boldsymbol{E}^{2}-\boldsymbol{B}^{2}\right)$
from \cref{chap:Mathematical-preliminaries} \cref{sub:ch1sec3_4Minkowski-tensors}. }
\[
\underline{T^{00}}=-\frac{1}{8\pi}\left[2F^{0\alpha}F{}^{0}{}_{\alpha}-\frac{1}{2}F_{\alpha\beta}F^{\alpha\beta}\right]=-\frac{1}{8\pi}\left[2\boldsymbol{E}^{2}-\left(\boldsymbol{E}^{2}-\boldsymbol{B}^{2}\right)\right]=\underline{-\frac{1}{8\pi}\left(\boldsymbol{E}^{2}+\boldsymbol{B}^{2}\right)};
\]
\[
\underline{T^{01}}=-\frac{1}{4\pi}F^{0\alpha}F{}^{1}{}_{\alpha}+0=-\frac{1}{4\pi}F^{0j}F{}^{1}{}_{j}=-\frac{1}{4\pi}\left(E_{y}B_{z}-E_{z}B_{y}\right)=\underline{-\frac{1}{4\pi}\left(\boldsymbol{E}\times\boldsymbol{B}\right)^{1}},
\]
 and cyclic. 
\begin{eqnarray*}
\implies\partial_{\nu}T^{0\nu} & = & \frac{1}{c}\partial_{t}\left[-\frac{1}{8\pi}\left(\boldsymbol{E}^{2}+\boldsymbol{B}^{2}\right)\right]+\nabla\cdot\left(-\frac{1}{4\pi}\boldsymbol{E}\times\boldsymbol{B}\right)\\
 & =- & F^{0\nu}J_{\nu}=-F^{0j}J_{j}=\boldsymbol{E}\cdot\boldsymbol{j}.
\end{eqnarray*}
We summarize this with some new definitions as follows.
\begin{claim}
\textbf{\emph{Poynting's theorem.}} Define the \emph{energy density
of the fields} $u\left(\boldsymbol{x},t\right)$ as 
\[
\boxed{u:=\frac{1}{8\pi}\left(\boldsymbol{E}^{2}+\boldsymbol{B}^{2}\right)},
\]
 and define the \emph{energy current density }(or\emph{ Poynting vector})\emph{
}$\boldsymbol{P}\left(\boldsymbol{x},t\right)$ as
\[
\boxed{\boldsymbol{P}:=\frac{c}{4\pi}\boldsymbol{E}\times\boldsymbol{B}}.
\]
 Then
\[
\boxed{\partial_{t}u+\nabla\cdot\boldsymbol{P}=-\boldsymbol{E}\cdot\boldsymbol{j}}
\]
\end{claim}
\begin{proof}
Follows directly from the above discussion.\end{proof}
\begin{rem}
For $\boldsymbol{j}=\boldsymbol{0}$, this expresses local energy
conservation. It is analogous to \cref{sub:ch2sec2_1Continuity-equation-for}
with $\rho\rightarrow u$, $\boldsymbol{j}\rightarrow\boldsymbol{P}$.
\end{rem}
~
\begin{rem}
Recall from \cref{sub:ch2sec3_5Charges-in-electromagnetic} that since
$\boldsymbol{F}=e\boldsymbol{E}+\frac{e}{c}\boldsymbol{v}\times\boldsymbol{B}$,
the work per unit time (power) done by the fields on a charge $e$
is $\boldsymbol{v}\cdot\boldsymbol{F}=e\boldsymbol{v}\cdot\boldsymbol{E}$.
This implies 
\[
\boldsymbol{j}\cdot\boldsymbol{E}=\left(\frac{e\boldsymbol{v}}{V}\right)\cdot\boldsymbol{E}=\frac{\boldsymbol{v}\cdot\boldsymbol{F}}{V}
\]
 is the work per unit time and volume, or power density. So for $\boldsymbol{j}=\boldsymbol{0}$,
Poynting's theorem still expresses energy conservation. In words,
\begin{eqnarray*}
\left(\textnormal{energy change}\right) & = & -\left(\textnormal{energy transported by the energy current}\right)\\
 &  & -\left(\textnormal{work done by the field on the charges}\right)
\end{eqnarray*}

\end{rem}
~
\begin{rem}
We still need to show that $u\left(\boldsymbol{x},t\right)$ can be
sensibly interpreted as the energy density of the field.

Let $\boldsymbol{j}\left(\boldsymbol{x},t\right)$ be the current
density due to just one particle, as in \cref{sub:ch2sec3_5Charges-in-electromagnetic}
(for many particles, we just sum over them). Integrating,
\[
\indefint{\boldsymbol{x}}\boldsymbol{j}\cdot\boldsymbol{E}=\indefint{\boldsymbol{x}}\left[\boldsymbol{E}\left(\boldsymbol{x},t\right)\right]\cdot\left[e\boldsymbol{v}\delta\left(\boldsymbol{x}-\boldsymbol{y}\right)\right]=e\boldsymbol{v}\cdot\boldsymbol{E}\left(\boldsymbol{y}\right),
\]
 where $\boldsymbol{y}$ is the position of the particle.

Consider a non-relativistic particle:
\[
E_{kin}=\frac{m}{2}\boldsymbol{v}^{2}
\]
\[
\implies\frac{d}{dt}E_{kin}=m\boldsymbol{v}\cdot\frac{d\boldsymbol{v}}{dt}=\boldsymbol{v}\cdot\underset{=e\boldsymbol{E}}{\underbrace{\frac{d}{dt}\boldsymbol{p}}}=e\boldsymbol{v}\cdot\boldsymbol{E},
\]
 where the last step follows from \cref{sub:ch2sec3_5Charges-in-electromagnetic}.
Now, integrating Poynting's theorem over all space,
\[
\frac{d}{dt}\indefint{\boldsymbol{x}}u\left(\boldsymbol{x},t\right)+\underset{\underset{\left(\textnormal{since }\boldsymbol{P}\rightarrow\boldsymbol{0}\textnormal{ at }\infty\right)}{=\int d\boldsymbol{S}\cdot\boldsymbol{P}=0}}{\underbrace{\indefint{\boldsymbol{x}}\nabla\cdot\boldsymbol{P}\left(\boldsymbol{x},t\right)}}=-\indefint{\boldsymbol{x}}\boldsymbol{j}\cdot\boldsymbol{E}=-e\boldsymbol{v}\cdot\boldsymbol{E}=-\frac{d}{dt}E_{kin}.
\]
Defining the integral of $u$ as
\[
\boxed{U\left(t\right):=\indefint{\boldsymbol{x}}u\left(\boldsymbol{x},t\right)},
\]
 we see that
\[
\underline{\frac{d}{dt}\left(U+E_{kin}\right)=0}.
\]
 $U$ must be the \emph{field energy}, since the energy of the particle
plus the energy of the field must be conserved. Hence, $u$ is the
\emph{energy density of the field}. 
\end{rem}
~
\begin{rem}
If we integrated over a finite volume, the energy may change due to
an energy current across the volume boundary, and we see that, in
general,
\[
\indefint{\boldsymbol{x}}\nabla\cdot\boldsymbol{P}\left(\boldsymbol{x},t\right)\neq0.
\]
 Thus, $\boldsymbol{P}$ should be interpreted as the \emph{energy
current density of the field}.
\end{rem}
~
\begin{rem}
The remaining components of the continuity equation from \cref{sub:ch2sec2_3The-continuity-equation},
\[
\partial_{\nu}T^{j\nu}=-\frac{1}{c}F^{j\nu}J_{\nu}
\]
 express the fact that the energy current density is also conserved.
\end{rem}

\section{Lorentz transformations of the fields}


\subsection{\label{sub:ch2sec4_1Physical-interpretation-of}Physical interpretation
of a Lorentz boost}

Consider two inertial frames, $CS$ and $\widetilde{CS}$.

Let $\widetilde{CS}$ move with respect to $CS$ with a constant velocity
$\boldsymbol{V}=\left(V,0,0\right).$

From Problems \#8, 10, the transformation from $CS$ to $\widetilde{CS}$
is accomplished by a Lorentz boost:
\begin{eqnarray*}
c\tilde{t} & = & ct\cosh\phi+x\sinh\phi,\\
\tilde{x} & = & ct\sinh\phi+x\cosh\phi.
\end{eqnarray*}


Consider the origin of $\widetilde{CS}$ as viewed by $CS$. Then
$\tilde{x}=0$, and\footnote{I think there's a sign error here.}
\begin{eqnarray*}
x\cosh\phi & = & -ct\sinh\phi,\\
\implies V=\frac{x}{t} & = & -c\tanh\phi.
\end{eqnarray*}
\[
\implies\sinh\phi=\frac{\tanh\phi}{\sqrt{1-\tanh^{2}\phi}}=\frac{\nicefrac{V}{c}}{\sqrt{1-\left(\nicefrac{V}{c}\right)^{2}}},\quad\cosh\phi=\sqrt{1+\sinh^{2}\phi}=\frac{1}{\sqrt{1-\left(\nicefrac{V}{c}\right)^{2}}}.
\]

\begin{rem}
First, observe that when $c\rightarrow\infty$, we recover the Galileo
transformation
\[
\tilde{x}=x+Vt,\quad\tilde{t}=t.
\]
 Let us define the above quantities:
\[
\mbox{\boxed{\beta:=\frac{V}{c}}},\quad\boxed{\gamma:=\frac{1}{\sqrt{1-\beta^{2}}}}\implies\boxed{\cosh\phi=\gamma},\quad\boxed{\sinh\phi=\beta\gamma}.
\]
 With these results, the Lorentz boost can be written
\[
\boxed{D{}^{\mu}{}_{\nu}=\begin{pmatrix}\cosh\phi & \sinh\phi\\
\sinh\phi & \cosh\phi\\
 &  & 1\\
 &  &  & 1
\end{pmatrix}=\begin{pmatrix}\gamma & \beta\gamma\\
\beta\gamma & \gamma\\
 &  & 1\\
 &  &  & 1
\end{pmatrix}}
\]

\end{rem}

\subsection{\label{sub:ch2sec4_2Transformations-of-}Transformations of $\boldsymbol{E}$
and $\boldsymbol{B}$ under a Lorentz boost}

Consider the field tensor $F^{\mu\nu}$ in $CS$. The field transformed
field tensor\footnote{NOT the dual field tensor} $\tilde{F}^{\mu\nu}$
in $\widetilde{CS}$ is
\[
\tilde{F}^{\mu\nu}=D{}^{\mu}{}_{\alpha}D{}^{\nu}{}_{\beta}F^{\alpha\beta}\quad\textnormal{and}\quad\tilde{x}^{\mu}=D{}^{\mu}{}_{\nu}x^{\nu}.
\]
 Now let $D{}^{\mu}{}_{\nu}$ be a Lorentz boost. Then, from \cref{sub:ch2sec3_1The-field-tensor}
we have
\begin{eqnarray*}
\tilde{F}^{\mu\nu} & = & \left(DFD^{T}\right)^{\mu\nu}\\
 & = & \begin{pmatrix}\gamma & \beta\gamma\\
\beta\gamma & \gamma\\
 &  & 1\\
 &  &  & 1
\end{pmatrix}\begin{pmatrix}0 & -E_{x} & -E_{y} & -E_{z}\\
E_{x} & 0 & -B_{z} & B_{y}\\
E_{y} & B_{z} & 0 & -B_{x}\\
E_{z} & -B_{y} & B_{x} & 0
\end{pmatrix}\begin{pmatrix}\gamma & \beta\gamma\\
\beta\gamma & \gamma\\
 &  & 1\\
 &  &  & 1
\end{pmatrix}\\
 & = & \begin{pmatrix}E_{x}\beta\gamma & -E_{x}\gamma & \left(-E_{y}-B_{z}\beta\right)\gamma & \left(-E_{z}+B_{y}\beta\right)\gamma\\
E_{x}\gamma & -E_{x}\beta\gamma & \left(-E_{y}\beta-B_{z}\right)\gamma & \left(-E_{z}\beta+B_{y}\right)\gamma\\
E_{y} & B_{z} & 0 & -B_{x}\\
E_{z} & -B_{y} & B_{x} & 0
\end{pmatrix}\begin{pmatrix}\gamma & \beta\gamma\\
\beta\gamma & \gamma\\
 &  & 1\\
 &  &  & 1
\end{pmatrix}\\
 & = & \begin{pmatrix}0 & -E_{x} & -\left(E_{y}+B_{z}\beta\right)\gamma & -\left(E_{y}\beta+B_{z}\right)\gamma\\
E_{x} & 0 & -\left(E_{z}-B_{y}\beta\right)\gamma & -\left(E_{z}\beta-B_{y}\right)\gamma\\
\left(E_{y}+B_{z}\beta\right)\gamma & \left(E_{y}\beta+B_{z}\right)\gamma & 0 & -B_{x}\\
\left(E_{z}-B_{y}\beta\right)\gamma & \left(E_{z}\beta-B_{y}\right)\gamma & B_{x} & 0
\end{pmatrix}\\
 & =: & \begin{pmatrix}0 & -\tilde{E}_{x} & -\tilde{E}_{y} & -\tilde{E}_{z}\\
\tilde{E}_{x} & 0 & -\tilde{B}_{z} & \tilde{B}_{y}\\
\tilde{E}_{y} & \tilde{B}_{z} & 0 & -\tilde{B}_{x}\\
\tilde{E}_{z} & -\tilde{B}_{y} & \tilde{B}_{x} & 0
\end{pmatrix}
\end{eqnarray*}
Thus,
\begin{equation}
\begin{aligned}\boxed{\tilde{\boldsymbol{E}}\rightarrow\begin{cases}
\tilde{E}_{x}= & E_{x}\\
\tilde{E}_{y}= & E_{y}\cosh\phi+B_{z}\sinh\phi=\left(E_{y}+B_{z}\beta\right)\gamma\\
\tilde{E}_{z}= & E_{z}\cosh\phi-B_{y}\sinh\phi=\left(E_{z}-B_{y}\beta\right)\gamma
\end{cases}}\\
\boxed{\tilde{\boldsymbol{B}}\rightarrow\begin{cases}
\tilde{B}_{x}= & B_{x}\\
\tilde{B}_{y}= & B_{y}\cosh\phi-E_{z}\sinh\phi=\left(B_{y}-E_{z}\beta\right)\gamma\\
\tilde{B}_{z}= & B_{z}\cosh\phi+E_{y}\sinh\phi=\left(B_{z}+E_{y}\beta\right)\gamma
\end{cases}}
\end{aligned}
\label{eq:Boosted_fields}
\end{equation}

\begin{rem}
The field equations were formulated in terms of Minkowski tensors;
their Lorentz invariance is guaranteed. Equations (\ref{eq:Boosted_fields})
reflect this same Lorentz invariance of Maxwell's equations, which
are equivalent to the field equations.
\end{rem}
~
\begin{rem}
Let $V\ll c$, and keep terms to $O\left(\frac{V}{c}\right)$. 
\[
\implies\cosh\phi\approx1,\quad\sinh\phi\approx\frac{V}{c}
\]
\[
\implies\tilde{\boldsymbol{E}}\approx\boldsymbol{E}-\left(\frac{\boldsymbol{V}}{c}\right)\times\boldsymbol{B}+O\left(\left(\frac{V}{c}\right)^{2}\right),\quad\tilde{\boldsymbol{B}}\approx\boldsymbol{B}+\left(\frac{\boldsymbol{V}}{c}\right)\times\boldsymbol{E}+O\left(\left(\frac{V}{c}\right)^{2}\right).
\]

\end{rem}
~
\begin{rem}
Let $\boldsymbol{E}=0$, so there is no $\boldsymbol{E}-$field in
$CS$. 
\[
\implies\tilde{\boldsymbol{E}}\approx-\left(\frac{\boldsymbol{V}}{c}\right)\times\boldsymbol{B};
\]
 we see that in $\widetilde{CS}$ there \emph{is} an $\boldsymbol{E}-$field
so long as $\boldsymbol{B}\neq\boldsymbol{0}$!
\end{rem}

\subsection{\label{sub:ch2sec4_3Lorentz-invariants}Lorentz invariants}

From the field tensor $F^{\mu\nu}$ we can form the following Lorentz
scalar fields:\footnote{Belitz calls them scalars, but I think they are scalar \emph{fields}.}
\[
\boxed{I^{\left(1\right)}:=-\frac{1}{2}F^{\mu\nu}F_{\mu\nu}},\quad\boxed{I^{\left(2\right)}:=\frac{1}{8}\varepsilon^{\alpha\beta\mu\nu}F_{\alpha\beta}F_{\mu\nu}}.
\]

\begin{rem}
~

\begin{eqnarray*}
I^{\left(1\right)}\textnormal{ is a scalar field } & \implies & \tilde{I}^{\left(1\right)}=I^{\left(1\right)}\textnormal{ in all inertial frames.}\\
I^{\left(2\right)}\textnormal{ is a pseudoscalar field } & \implies & \left|\tilde{I}^{\left(2\right)}\right|=\left|I^{\left(2\right)}\right|\textnormal{ in all inertial frames.}
\end{eqnarray*}
 The absolute value signs are necessary since $\tilde{I}^{\left(2\right)}=\left(\det D\right)I^{\left(2\right)}$.\end{rem}
\begin{claim}
$\boxed{I^{\left(1\right)}=\boldsymbol{E}^{2}-\boldsymbol{B}^{2}}$\end{claim}
\begin{proof}
$I^{\left(1\right)}=-\frac{1}{2}\begin{pmatrix}0 & -\boldsymbol{E}\\
\boldsymbol{E} & B^{jk}
\end{pmatrix}\begin{pmatrix}0 & \boldsymbol{E}\\
-\boldsymbol{E} & B_{jk}
\end{pmatrix}=\boldsymbol{E}^{2}-\frac{1}{2}B^{jk}B_{jk}$. But\footnote{Note that when we write $\boldsymbol{E}^{2}$, we mean $E_{x}^{2}+E_{y}^{2}+E_{z}^{2}$.
Also, upper and lower indices don't matter in what follows (Euclidean).}
\[
\frac{1}{2}B^{jk}B_{jk}=\frac{1}{2}\varepsilon_{jkl}B_{l}\varepsilon_{jkm}B_{m}=\frac{1}{2}\left(\underset{3}{\underbrace{\delta_{kk}}}\delta_{lm}-\underset{\delta_{lm}}{\underbrace{\delta_{km}\delta_{lk}}}\right)B_{l}B_{m}=\boldsymbol{B}^{2}
\]
\[
\implies I^{\left(1\right)}=\boldsymbol{E}^{2}-\boldsymbol{B}^{2}
\]
\end{proof}
\begin{claim}
$\boxed{I^{\left(2\right)}=-\boldsymbol{E}\cdot\boldsymbol{B}}$\end{claim}
\begin{proof}
\begin{eqnarray*}
I^{\left(2\right)} & = & \frac{1}{8}\left[\varepsilon^{0123}F_{01}F_{23}+\varepsilon^{0132}F_{01}F_{32}\right.\\
 &  & +\varepsilon^{0213}F_{02}F_{13}+\varepsilon^{0231}F_{02}F_{31}\\
 &  & +\varepsilon^{0312}F_{03}F_{12}+\varepsilon^{0321}F_{03}F_{21}\\
 &  & \left.+\left(4\times6=24\textnormal{ other terms}\right)\right]\\
 & = & \frac{1}{4}\left[\varepsilon^{0123}F_{01}F_{23}+\varepsilon^{0213}F_{02}F_{13}+\varepsilon^{0312}F_{03}F_{12}\right.\\
 &  & \left.+\left(12\textnormal{ other terms}\right)\right]\\
 & = & \frac{1}{4}\left[-E_{x}B_{x}-E_{y}B_{y}-E_{z}B_{z}\right]\times4=-\boldsymbol{E}\cdot\boldsymbol{B}
\end{eqnarray*}
\end{proof}
\begin{prop}
The field combinations $I^{\left(1\right)}$ and $I^{\left(2\right)}$
are invariant under (proper) Lorentz transformations; i.e., their
absolute values have the same values in all inertial frames.\end{prop}
\begin{proof}
See above.\end{proof}
\begin{rem}
If $\boldsymbol{E}\perp\boldsymbol{B}$ in some inertial frame, then
$\boldsymbol{E}\perp\boldsymbol{B}$ in all other inertial frames
\end{rem}
~
\begin{rem}
Ditto if $\boldsymbol{E}^{2}=\boldsymbol{B}^{2}$ in some frame.
\end{rem}

\section{\label{sec:ch2sec5The-superposition-principle}The superposition
principle of Maxwell theory}


\subsection{Real solutions\label{sub:ch2sec5_1Real-solutions}}
\begin{prop}
\label{prop:LinearSolutions}Let $\rho^{\left(\alpha\right)}\left(x\right)$,
$\boldsymbol{j}^{\left(\alpha\right)}\left(x\right)$, with $\alpha=1,2$,
be two charge and current densities. Let $\boldsymbol{E}^{\left(\alpha\right)}\left(x\right)$,
$\boldsymbol{B}^{\left(\alpha\right)}\left(x\right)$ be solutions
of Maxwell's equations for $\rho^{\left(\alpha\right)}$, $\boldsymbol{j}^{\left(\alpha\right)}$,
and let $\lambda^{\left(\alpha\right)}\in\mathbb{R}$. Then
\begin{eqnarray*}
\boldsymbol{E} & = & \lambda^{\left(1\right)}\boldsymbol{E}^{\left(1\right)}+\lambda^{\left(2\right)}\boldsymbol{E}^{\left(2\right)},\\
\boldsymbol{B} & = & \lambda^{\left(1\right)}\boldsymbol{B}^{\left(1\right)}+\lambda^{\left(2\right)}\boldsymbol{B}^{\left(2\right)}
\end{eqnarray*}
 are solutions for 
\begin{eqnarray*}
\rho & = & \lambda^{\left(1\right)}\rho^{\left(1\right)}+\lambda^{\left(2\right)}\rho^{\left(2\right)},\\
\boldsymbol{j} & = & \lambda^{\left(1\right)}\boldsymbol{j}^{\left(1\right)}+\lambda^{\left(2\right)}\boldsymbol{j}^{\left(2\right)}.
\end{eqnarray*}
\end{prop}
\begin{proof}
$\nabla\cdot\boldsymbol{E}-4\pi\rho=\underset{=0}{\underbrace{\nabla\cdot\boldsymbol{E}^{\left(1\right)}-4\pi\rho^{\left(1\right)}}}+\underset{=0}{\underbrace{\nabla\cdot\boldsymbol{E}^{\left(2\right)}-4\pi\rho^{\left(2\right)}}}=0$,
etc.\end{proof}
\begin{rem}
This is obviously true since the theory is linear!
\end{rem}
~
\begin{rem}
If the action contained terms of higher than second order in $F_{\mu\nu}$,
this would not be true.
\end{rem}
~
\begin{rem}
A field theory that leads to linear field equations is called \emph{Gaussian}
or \emph{free}.\end{rem}
\begin{cor}
Let $\boldsymbol{E}^{\left(k\right)}\left(x\right)$, $\boldsymbol{B}^{\left(k\right)}\left(x\right)$
be solutions for $\rho^{\left(k\right)}\left(x\right)$, $\boldsymbol{j}^{\left(k\right)}\left(x\right)$,
where $k\in\mathbb{R}$, and let $\lambda\left(k\right):\mathbb{R}\rightarrow\mathbb{R}$
be sufficiently well behaved. Then
\begin{eqnarray*}
\boldsymbol{E}\left(x\right) & = & \indefint k\lambda\left(k\right)\boldsymbol{E}^{\left(k\right)}\left(x\right),\\
\boldsymbol{B}\left(x\right) & = & \indefint k\lambda\left(k\right)\boldsymbol{B}^{\left(k\right)}\left(x\right)
\end{eqnarray*}
 are solutions for
\begin{eqnarray*}
\rho\left(x\right) & = & \indefint k\lambda\left(k\right)\rho^{\left(k\right)}\left(x\right),\\
\boldsymbol{j}\left(x\right) & = & \indefint k\lambda\left(k\right)\boldsymbol{j}^{\left(k\right)}\left(x\right).
\end{eqnarray*}
\end{cor}
\begin{proof}
Generalize \cref{prop:LinearSolutions} to $\alpha=1,\dots,N$ and
let $N\rightarrow\infty$.\end{proof}
\begin{rem}
This can easily be generalized to $\boldsymbol{E}^{\left(\boldsymbol{k}\right)}\left(x\right)$,
where $\boldsymbol{k}\in\mathbb{R}^{3}$.\end{rem}
\begin{cor}
\label{cor:The-most-general}The most general solution of Maxwell's
equations is obtained as
\begin{eqnarray*}
\boldsymbol{E}\left(x\right) & = & \boldsymbol{E}^{\left(0\right)}\left(x\right)+\boldsymbol{E}^{\left(p\right)}\left(x\right),\\
\boldsymbol{B}\left(x\right) & = & \boldsymbol{B}^{\left(0\right)}\left(x\right)+\boldsymbol{B}^{\left(p\right)}\left(x\right)
\end{eqnarray*}
 where $\boldsymbol{E}^{\left(0\right)}$, $\boldsymbol{B}^{\left(0\right)}$
are the most general solutions of the homogeneous equations\footnote{That is, when $\rho=0$, $\boldsymbol{j}=\boldsymbol{0}$.}
and $\boldsymbol{E}^{\left(p\right)}$, $\boldsymbol{B}^{\left(p\right)}$
is a particular solution in the presence of $\rho$, $\boldsymbol{j}$.\end{cor}
\begin{proof}
Let $\boldsymbol{E}$, \textit{$\boldsymbol{B}$ be any solution for
$\rho$, $\boldsymbol{j}$, and let $\boldsymbol{E}^{\left(p\right)}$,
$\boldsymbol{B}^{\left(p\right)}$ be a particular solution.}

By \cref{prop:LinearSolutions},
\begin{eqnarray*}
\boldsymbol{E}^{\left(0\right)} & := & \boldsymbol{E}-\boldsymbol{E}^{\left(p\right)},\\
\boldsymbol{B}^{\left(0\right)} & := & \boldsymbol{B}-\boldsymbol{B}^{\left(p\right)}
\end{eqnarray*}
 are solutions for $\rho=0=\boldsymbol{j}$.

Conversely, if $\boldsymbol{E}^{\left(0\right)}$, $\boldsymbol{B}^{\left(0\right)}$
is a solution for $\rho=0=\boldsymbol{j}$, and $\boldsymbol{E}^{\left(p\right)}$,
$\boldsymbol{B}^{\left(p\right)}$ is some solution for $\rho$, $\boldsymbol{j}$,
then
\begin{eqnarray*}
\boldsymbol{E} & = & \boldsymbol{E}^{\left(0\right)}+\boldsymbol{E}^{\left(p\right)},\\
\boldsymbol{B} & = & \boldsymbol{B}^{\left(0\right)}+\boldsymbol{B}^{\left(p\right)}
\end{eqnarray*}
 is a solution for $\rho$, $\boldsymbol{j}$.
\end{proof}

\subsection{Complex solutions}

All physical solutions to Maxwell's equations must consist of real
fields $\boldsymbol{E}$, $\boldsymbol{B}$. However, it is sometimes
convenient to find complex solutions and take the real part afterwards.
\begin{prop}
Let $\boldsymbol{E}$, $\boldsymbol{B}$ be complex solutions for
complex sources $\rho$, $\boldsymbol{j}$. Then $\boldsymbol{E}^{*}$,
$\boldsymbol{B}^{*}$ are solutions for $\rho^{*}$, $\boldsymbol{j}^{*}$.\end{prop}
\begin{proof}
\[
\nabla\cdot\boldsymbol{E}=4\pi\rho\quad\implies\quad\nabla\cdot\left(\textnormal{Re }\boldsymbol{E}\right)+i\nabla\cdot\left(\textnormal{Im }\boldsymbol{E}\right)=4\pi\left(\textnormal{Re }\rho\right)+i4\pi\left(\textnormal{Im }\rho\right)
\]
\begin{eqnarray*}
\implies\nabla\cdot\left(\textnormal{Re }\boldsymbol{E}\right) & = & 4\pi\left(\textnormal{Re }\rho\right)\\
\implies\nabla\cdot\left(\textnormal{Im }\boldsymbol{E}\right) & = & 4\pi\left(\textnormal{Im }\rho\right)
\end{eqnarray*}
\[
\implies\nabla\cdot\left(\textnormal{Re }\boldsymbol{E}-i\textnormal{Im }\boldsymbol{E}\right)=4\pi\left(\textnormal{Re }\rho-i\textnormal{Im }\rho\right)\quad\implies\quad\nabla\cdot\boldsymbol{E}^{*}=4\pi\rho^{*},
\]
 etc. for the other Maxwell equations.\end{proof}
\begin{rem}
This, again, is because of linearity.\end{rem}
\begin{cor}
Let $\boldsymbol{E}$, $\boldsymbol{B}$ be complex solutions for
real (i.e. physical) sources $\rho$, $\boldsymbol{j}$. Then $\textnormal{Re }\boldsymbol{E}$,
$\textnormal{Re }\boldsymbol{B}$ are also solutions for $\rho$,
$\boldsymbol{j}$.\end{cor}
\begin{proof}
From Corollary (?), $\textnormal{Re }\boldsymbol{E}$, $\textnormal{Re }\boldsymbol{B}$
are solutions for $\textnormal{Re }\rho=\rho$, $\textnormal{Re }\boldsymbol{j}=\boldsymbol{j}$.\end{proof}
\begin{rem}
In this case, $\textnormal{Im }\boldsymbol{E}$, $\textnormal{Im }\boldsymbol{B}$
are solutions in the absence of sources (since $\textnormal{Im }\rho=0,$
$\textnormal{Im }\boldsymbol{j}=\boldsymbol{0}$). 
\end{rem}

\chapter{\label{chap:Static-solutions-of}Static solutions of Maxwell's equations}


\section{Poisson's equations}


\subsection{Electrostatics\label{sub:ch3sec1_1Electrostatics}}

Consider Maxwell's equations for static fields:
\begin{eqnarray*}
\left(M2\right) & \rightarrow & \nabla\times\boldsymbol{E}=0\\
\left(M3\right) & \rightarrow & \nabla\cdot\boldsymbol{E}=4\pi\rho.
\end{eqnarray*}
\begin{eqnarray*}
\left(M1\right) & \rightarrow & \nabla\cdot\boldsymbol{B}=0\\
\left(M4\right) & \rightarrow & \nabla\times\boldsymbol{B}=\frac{4\pi}{c}\boldsymbol{j}
\end{eqnarray*}
 
\begin{rem}
$\left(M2\right)$ and $\left(M3\right)$ now contain $\boldsymbol{E}$
only! $\left(M1\right)$ and $\left(M4\right)$ now contain $\boldsymbol{B}$
only! For static fields, $\boldsymbol{E}$ and $\boldsymbol{B}$ decouple!
\end{rem}
~
\begin{rem}
From \cref{chap:Maxwell's-Equations} \cref{sub:ch2sec3_4Relations-between-fields},
a static $\boldsymbol{E}-$field is determined by $\phi$ alone:
\[
\boxed{\boldsymbol{E}=-\nabla\phi},
\]
 and $\left(M2\right)$ is thus automatically satisfied, since $\left(\nabla\times\boldsymbol{E}\right)_{i}=-\left(\nabla\times\nabla\phi\right)_{i}=0$.\end{rem}
\begin{prop}
\label{prop:PoissonEquationsRho}The electrostatic potential $\phi\left(\boldsymbol{x}\right)$
obeys \emph{Poisson's equations} for $\rho\left(\boldsymbol{x}\right)$:
\[
\boxed{\nabla^{2}\phi=-4\pi\rho},
\]
 where $\nabla^{2}:=\partial_{j}\partial^{j}=:\Delta$ is the \emph{Laplace
operator}.\end{prop}
\begin{cor}
In vacuum, $\phi\left(\boldsymbol{x}\right)$ obeys the \emph{Laplace
equation}:
\[
\boxed{\nabla^{2}\phi=0}.
\]
\end{cor}
\begin{rem}
Solutions of Laplace's equation are called \emph{harmonic functions}. 
\end{rem}
~
\begin{rem}
$\phi\left(\boldsymbol{x}\right)=\textnormal{const.},\,x,\,y,\,z^{2}-\frac{1}{2}\left(x^{2}+y^{2}\right)$
are all harmonic functions.
\end{rem}
~
\begin{rem}
\label{rem:A-harmonic-functionextrema}A harmonic function can have
no extrema except at infinity; this is a theorem in analysis.
\end{rem}

\subsection{\label{sub:ch3sec1_2Magnetostatics}Magnetostatics}

From \cref{chap:Maxwell's-Equations}, \cref{sub:ch2sec3_4Relations-between-fields},
$\boldsymbol{B}=\nabla\times\boldsymbol{A}$.
\begin{rem}
This is always true!
\end{rem}
~
\begin{rem}
$\left(M1\right)$ is automatically fulfilled, since $\nabla\cdot\nabla\times\boldsymbol{A}=0$.\end{rem}
\begin{prop}
\label{prop:PoissonsEquationsj}The static Euclidean vector potential
$\boldsymbol{A}\left(\boldsymbol{x}\right)$ obeys
\[
\boxed{\nabla^{2}\boldsymbol{A}=-\frac{4\pi}{c}\boldsymbol{j}},
\]
where $\boldsymbol{j}=\boldsymbol{j}\left(\boldsymbol{x}\right)$.\end{prop}
\begin{proof}
\begin{eqnarray*}
\left(\nabla\times\nabla\times\boldsymbol{A}\right)_{i} & = & \varepsilon_{ijk}\partial_{j}\varepsilon_{klm}\partial_{l}A_{m}=\varepsilon_{kij}\varepsilon_{klm}\partial_{j}\partial_{l}A_{m}=\left(\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}\right)\partial_{j}\partial_{l}A_{m}\\
 & = & -\partial_{j}\partial_{j}A_{i}+\partial_{i}\partial_{j}A_{j}\\
 & = & -\nabla^{2}A_{i}+\partial_{i}\left(\nabla\cdot\boldsymbol{A}\right).
\end{eqnarray*}
 But by Problem \#19, we can always choose the Coulumb gauge to make
$\nabla\cdot\boldsymbol{A}=0$. 
\[
\implies\frac{4\pi}{c}\boldsymbol{j}=\nabla\times\boldsymbol{B}=\nabla\times\nabla\times\boldsymbol{A}=-\nabla^{2}\boldsymbol{A}.
\]
\end{proof}
\begin{rem}
Combining Propositions \ref{prop:PoissonEquationsRho} and \ref{prop:PoissonsEquationsj},
we see that the components of the static electromagnetic potential
obey Poisson's equation with $-\frac{4\pi}{c}$ times the components
$\left(c\rho,\boldsymbol{j}\right)$ of the 4-current as the inhomogeneity.
\end{rem}
~
\begin{rem}
\label{rem:Ch3sec1_2rem4_linear}Poisson's equation is linear; thus,
the most general solution is a particular solution plus the most general
solution of Laplace's equation (see \cref{chap:Maxwell's-Equations}
\cref{sub:ch2sec5_1Real-solutions} \cref{cor:The-most-general}). 
\end{rem}
~
\begin{rem}
\label{rem:Ch3sec1_2rem5_From--Remark}From \cref{sub:ch3sec1_1Electrostatics}
\cref{rem:A-harmonic-functionextrema}, the only solution of Laplace's
equation that vanishes at infinity is the zero solution; in an infinite
system, there is only one physical solution of Poisson's equation.
\end{rem}
~
\begin{rem}
Things get more complicated in a finite system with boundary conditions.
\end{rem}

\section{\label{sec:ch3sec2_Digression:-Fourier-transforms}Digression: Fourier
transforms and generalized functions}


\subsection{\label{sub:ch3sec2_1The-Fourier-transform}The Fourier transform
in classical analysis}

Let $f:\mathbb{R}^{n}\rightarrow\mathbb{C}$ be a complex-valued function
of $n$ real arguments that is \emph{absolutely integrable}:
\[
\indefint{\boldsymbol{x}}\left|f\left(\boldsymbol{x}\right)\right|<\infty.
\]

\begin{rem}
The space of these functions, denoted $\gamma^{\left(1\right)}$,
forms a vector space over $\mathbb{C}$ under addition of functions.
\end{rem}
\textbf{Notation: }

$\boldsymbol{x}=\left(x_{1},\dots,x_{n}\right)\in\mathbb{R}^{n},$

$\int d\boldsymbol{x}=\underset{\mathbb{R}^{n}}{\int}dx_{1}\cdots dx_{n}$,

$\boldsymbol{k}\cdot\boldsymbol{x}=k_{1}x_{1}+\cdots+k_{n}x_{n}\quad\left(\boldsymbol{k}\in\mathbb{R}^{n}\right).$
\begin{defn}
\textbf{\emph{Fourier transform.}} The \emph{Fourier transform} of
$f\left(\boldsymbol{x}\right)$ is defined as
\[
\boxed{\hat{f}\left(\boldsymbol{k}\right):=\indefint{\boldsymbol{x}}e^{-i\boldsymbol{k}\cdot\boldsymbol{x}}f\left(\boldsymbol{x}\right)=:\mathcal{F}\left[f\left(\boldsymbol{x}\right)\right]\left(\boldsymbol{k}\right)}
\]
\end{defn}
\begin{rem}
$\hat{f}:\mathbb{R}^{n}\rightarrow\mathbb{C}$ is another complex-valued
function of $\mathbb{R}^{n}$.
\end{rem}
~
\begin{rem}
The Fourier transform is a \emph{linear integral transform.}
\end{rem}
~
\begin{rem}
$\mathcal{F}\left[\lambda_{1}f_{1}+\lambda_{2}f_{2}\right]=\lambda_{1}\mathcal{F}\left[f_{1}\right]+\lambda_{2}\mathcal{F}\left[f_{2}\right]\quad\forall\lambda_{1,2}\in\mathbb{C}$
due to this linearity.\end{rem}
\begin{prop}
$\hat{f}\left(\boldsymbol{k}\right)$ is \emph{bounded} and \emph{continuous}.\end{prop}
\begin{proof}
To show that $\hat{f}$ is bounded, 
\[
\left|\hat{f}\left(\boldsymbol{k}\right)\right|=\left|\indefint{\boldsymbol{x}}e^{-i\boldsymbol{k}\cdot\boldsymbol{x}}f\left(\boldsymbol{x}\right)\right|\leq\indefint{\boldsymbol{x}}\left|e^{-i\boldsymbol{k}\cdot\boldsymbol{x}}f\left(\boldsymbol{x}\right)\right|=\indefint{\boldsymbol{x}}\left|f\left(\boldsymbol{x}\right)\right|<\infty,
\]
 where we have used the triangle inequality.

To show that $\hat{f}$ is continuous, 
\begin{eqnarray*}
\left|\hat{f}\left(\boldsymbol{k}_{1}\right)-\hat{f}\left(\boldsymbol{k}_{2}\right)\right| & = & \left|\indefint{\boldsymbol{x}}\left(e^{-i\boldsymbol{k}_{1}\cdot\boldsymbol{x}}-e^{-i\boldsymbol{k}_{2}\cdot\boldsymbol{x}}\right)f\left(\boldsymbol{x}\right)\right|\leq\indefint{\boldsymbol{x}}\left|e^{-i\boldsymbol{k}_{1}\cdot\boldsymbol{x}}-e^{-i\boldsymbol{k}_{2}\cdot\boldsymbol{x}}\right|\left|f\left(\boldsymbol{x}\right)\right|\\
 &  & \rightarrow0\quad\textnormal{for}\quad\boldsymbol{k}_{1}\rightarrow\boldsymbol{k}_{2},
\end{eqnarray*}
 where, again, we have used the triangle inequality.\end{proof}
\begin{prop}
Let $x_{l}f\left(\boldsymbol{x}\right)$ be absolutely integrable.
Then $\hat{f}\left(\boldsymbol{k}\right)$ is differentiable with
respect to $k_{l}$ and
\[
\boxed{\frac{\partial}{\partial k_{l}}\hat{f}\left(\boldsymbol{k}\right)=\mathcal{F}\left[-ix_{l}f\right]\left(\boldsymbol{k}\right)}.
\]
\end{prop}
\begin{proof}
$\frac{\partial}{\partial k_{l}}\hat{f}\left(\boldsymbol{k}\right)=\frac{\partial}{\partial k_{l}}\int d\boldsymbol{x}e^{-i\boldsymbol{k}\cdot\boldsymbol{x}}f\left(\boldsymbol{x}\right)=-i\int d\boldsymbol{x}e^{-i\boldsymbol{k}\cdot\boldsymbol{x}}x_{l}f\left(\boldsymbol{x}\right)=\mathcal{F}\left[-ix_{l}f\right]\left(\boldsymbol{k}\right)$.
Note that we needed to stipulate that $x_{l}f\left(\boldsymbol{x}\right)$
was absolutely integrable to proceed with the last step.\end{proof}
\begin{prop}
\label{prop:fourier_of_derivativeLet--be}Let $f\left(\boldsymbol{x}\right)$
be differentiable with respect to $x_{l}$, and let $\frac{\partial}{\partial x_{l}}f$
be absolutely integrable. Then
\[
\boxed{\mathcal{F}\left[\partial_{l}f\right]\left(\boldsymbol{k}\right)=ik_{l}\hat{f}\left(\boldsymbol{k}\right)}.
\]
\end{prop}
\begin{proof}
For $n=1$, 
\[
\indefint xe^{-ikx}\frac{d}{dx}f\left(x\right)=\underset{=0}{\underbrace{\left.e^{-ikx}f\left(x\right)\right|_{-\infty}^{\infty}}}-\indefint x\left(-ik\right)e^{-ikx}f\left(x\right)=ik\hat{f}\left(k\right).
\]
\end{proof}
\begin{rem}
The Fourier transform turns derivatives into products! Prospect: turn
differential equations into algebraic ones!
\end{rem}
~
\begin{rem}
This also works for $n>1$ and higher derivatives. For instance, for
$n=3$,
\[
\boxed{\mathcal{F}\left[\nabla^{2}f\right]\left(\boldsymbol{k}\right)=-\boldsymbol{k}^{2}\hat{f}\left(\boldsymbol{k}\right)}.
\]
\end{rem}
\begin{prop}
\[
\boxed{\mathcal{F}\left[f^{*}\right]\left(\boldsymbol{k}\right)=\left(\mathcal{F}\left[f\right]\left(-\boldsymbol{k}\right)\right)^{*}}.
\]
\end{prop}
\begin{proof}
$\mathcal{F}\left[f^{*}\right]\left(\boldsymbol{k}\right)=\int d\boldsymbol{x}e^{-i\boldsymbol{k}\cdot\boldsymbol{x}}f^{*}\left(\boldsymbol{x}\right)=\left(\int d\boldsymbol{x}e^{i\boldsymbol{k}\cdot\boldsymbol{x}}f\left(\boldsymbol{x}\right)\right)^{*}=\left(\hat{f}\left(-\boldsymbol{k}\right)\right)^{*}.$\end{proof}
\begin{thm}
\textbf{\emph{Convolution theorem.}} Let $f_{1}$, $f_{2}$ be absolutely
integrable, and let their \emph{convolution} $f_{1}\star f_{2}$,
defined as 
\[
\boxed{\left(f_{1}\star f_{2}\right)\left(\boldsymbol{y}\right):=\indefint{\boldsymbol{x}}f_{1}\left(\boldsymbol{y}-\boldsymbol{x}\right)f_{2}\left(\boldsymbol{x}\right)},
\]
 exist and be absolutely integrable. Then
\[
\boxed{\mathcal{F}\left[f_{1}\star f_{2}\right]\left(\boldsymbol{k}\right)=\hat{f}_{1}\left(\boldsymbol{k}\right)\hat{f}_{2}\left(\boldsymbol{k}\right)}.
\]
\end{thm}
\begin{proof}
\begin{eqnarray*}
\mathcal{F}\left[f_{1}\star f_{2}\right]\left(\boldsymbol{k}\right) & = & \indefint{\boldsymbol{y}}e^{-i\boldsymbol{k}\cdot\boldsymbol{y}}\indefint{\boldsymbol{x}}f_{1}\left(\boldsymbol{y}-\boldsymbol{x}\right)f_{2}\left(\boldsymbol{x}\right)\\
 & = & \indefint{\boldsymbol{x}}e^{-i\boldsymbol{k}\cdot\boldsymbol{x}}\indefint{\boldsymbol{y}}e^{-i\boldsymbol{k}\cdot\left(\boldsymbol{y}-\boldsymbol{x}\right)}f_{1}\left(\boldsymbol{y}-\boldsymbol{x}\right)f_{2}\left(\boldsymbol{x}\right)\\
 & = & \indefint{\boldsymbol{x}}e^{-i\boldsymbol{k}\cdot\boldsymbol{x}}f_{2}\left(\boldsymbol{x}\right)\indefint{\boldsymbol{z}e^{-i\boldsymbol{k}\cdot\boldsymbol{z}}f_{1}\left(\boldsymbol{z}\right)}
\end{eqnarray*}
\end{proof}
\begin{rem}
Convolutions in real space turn into products in Fourier space.
\end{rem}

\subsection{\label{sub:ch3sec2_2Inverse-Fourier-transforms}Inverse Fourier transforms}

Let $f_{1}$, $f_{2}$ be absolutely integrable.
\begin{lem}
\label{lem:inverse_fourier}$\indefint{\boldsymbol{x}}f_{1}\left(\boldsymbol{x}\right)\left(\hat{f}_{2}\left(\boldsymbol{x}\right)\right)^{*}=\indefint{\boldsymbol{y}}\hat{f}_{1}\left(-\boldsymbol{y}\right)\left(f_{2}\left(\boldsymbol{y}\right)\right)^{*}.$\end{lem}
\begin{proof}
\begin{eqnarray*}
\indefint{\boldsymbol{x}}f_{1}\left(\boldsymbol{x}\right)\left(\hat{f}_{2}\left(\boldsymbol{x}\right)\right)^{*} & = & \indefint{\boldsymbol{x}}f_{1}\left(\boldsymbol{x}\right)\left(\indefint{\boldsymbol{y}}e^{-i\boldsymbol{x}\cdot\boldsymbol{y}}f_{2}\left(\boldsymbol{y}\right)\right)^{*}\\
 & = & \indefint{\boldsymbol{x}}f_{1}\left(\boldsymbol{x}\right)\indefint{\boldsymbol{y}}e^{i\boldsymbol{x}\cdot\boldsymbol{y}}\left(f_{2}\left(\boldsymbol{y}\right)\right)^{*}\\
 & = & \indefint{\boldsymbol{y}}\left(f_{2}\left(\boldsymbol{y}\right)\right)^{*}\underset{\hat{f}_{1}\left(-\boldsymbol{y}\right)}{\underbrace{\indefint{\boldsymbol{x}}f_{1}\left(\boldsymbol{x}\right)e^{i\boldsymbol{x}\cdot\boldsymbol{y}}}}\\
 & = & \indefint{\boldsymbol{y}}\hat{f}_{1}\left(-\boldsymbol{y}\right)\left(f_{2}\left(\boldsymbol{y}\right)\right)^{*}
\end{eqnarray*}
\end{proof}
\begin{thm}
\textbf{\emph{Inverse Fourier transform.}} Let $f\left(\boldsymbol{x}\right)$
and $\hat{f}\left(\boldsymbol{k}\right)$ exist and be absolutely
integrable. Then the inverse Fourier transform is 
\[
\boxed{f\left(\boldsymbol{x}\right)=\frac{1}{\left(2\pi\right)^{n}}\int d\boldsymbol{k}\:e^{i\boldsymbol{k}\cdot\boldsymbol{x}}\hat{f}\left(\boldsymbol{k}\right)=:\mathcal{F}^{-1}\left[\hat{f}\right]\left(\boldsymbol{x}\right)}.
\]
\end{thm}
\begin{rem}
This means $\mathcal{F}\left[\mathcal{F}\left[f\right]\right]=\left(2\pi\right)^{n}f$;
i.e., the Fourier transform is its own inverse apart from a factor
of $\left(2\pi\right)^{n}$.\end{rem}
\begin{proof}
Consider \cref{lem:inverse_fourier} with $f_{1}=f$, $f_{2}\left(\boldsymbol{y}\right)=e^{-\alpha\boldsymbol{y}^{2}}e^{i\boldsymbol{y}\cdot\boldsymbol{x}}$,
where $\alpha>0$. 
\begin{eqnarray*}
\implies\hat{f}_{2}\left(\boldsymbol{k}\right) & = & \indefint{\boldsymbol{y}}e^{-i\boldsymbol{k}\cdot\boldsymbol{y}}e^{-\alpha\boldsymbol{y}^{2}}e^{i\boldsymbol{x}\cdot\boldsymbol{y}}\\
 & = & \indefint{\boldsymbol{y}}e^{-i\boldsymbol{y}\cdot\left(\boldsymbol{k}-\boldsymbol{x}\right)}e^{-\alpha\boldsymbol{y}^{2}}\\
 & = & \left(\frac{\pi}{\alpha}\right)^{\frac{n}{2}}e^{-\frac{1}{4\alpha}\left(\boldsymbol{k}-\boldsymbol{x}\right)^{2}},
\end{eqnarray*}
where the last step, that Fourier transform of a Gaussian is a Gaussian,
is the result of Problem \#27. By the Lemma, 
\[
\indefint{\boldsymbol{y}}\hat{f}\left(-\boldsymbol{y}\right)\underset{\left(f_{2}\left(\boldsymbol{y}\right)\right)^{*}}{\underbrace{e^{-\alpha\boldsymbol{y}^{2}}e^{-i\boldsymbol{y}\cdot\boldsymbol{x}}}}=\int d\boldsymbol{k}\:f\left(\boldsymbol{k}\right)\underset{\left(\hat{f}_{2}\left(\boldsymbol{k}\right)\right)^{*}}{\underbrace{\left(\frac{\pi}{\alpha}\right)^{\frac{n}{2}}e^{-\frac{1}{4\alpha}\left(\boldsymbol{x}-\boldsymbol{k}\right)^{2}}}}.
\]
 Consider the limit as $\alpha\rightarrow0$. 

On the left hand side,
\[
\lim_{\alpha\rightarrow0}\indefint{\boldsymbol{y}}\hat{f}\left(-\boldsymbol{y}\right)e^{-\alpha\boldsymbol{y}^{2}}e^{-i\boldsymbol{y}\cdot\boldsymbol{x}}=\underline{\int d\boldsymbol{k}\:\hat{f}\left(\boldsymbol{k}\right)e^{i\boldsymbol{k}\cdot\boldsymbol{x}}}
\]


On the right hand side, by the Intermediate Value Theorem,
\begin{eqnarray*}
\lim_{\alpha\rightarrow0}\int d\boldsymbol{k}\:f\left(\boldsymbol{k}\right)\left(\frac{\pi}{\alpha}\right)^{\frac{n}{2}}e^{-\frac{1}{4\alpha}\left(\boldsymbol{x}-\boldsymbol{k}\right)^{2}} & = & f\left(\boldsymbol{x}\right)\lim_{\alpha\rightarrow0}\left(\frac{\pi}{\alpha}\right)^{\frac{n}{2}}\int d\boldsymbol{k}\:e^{-\frac{1}{4\alpha}\left(\boldsymbol{x}-\boldsymbol{k}\right)^{2}}\\
 & = & f\left(\boldsymbol{x}\right)\lim_{\alpha\rightarrow0}\left(\frac{\pi}{\alpha}\right)^{\frac{n}{2}}\left(\int d\boldsymbol{k}\:e^{-\frac{1}{4\alpha}\boldsymbol{k}^{2}}\right)^{n}\\
 & = & f\left(\boldsymbol{x}\right)\lim_{\alpha\rightarrow0}\left(\frac{\pi}{\alpha}\right)^{\frac{n}{2}}\left(2\sqrt{a}\right)^{n}\left(\sqrt{\pi}\right)^{n}\\
 & = & \underline{\left(2\pi\right)^{n}f\left(\boldsymbol{x}\right)}
\end{eqnarray*}

\end{proof}

\subsection{\label{sub:ch3sec2_3Test-functions}Test functions}
\begin{description}
\item [{problem:}] In classical analysis, very few functions are Fourier
transformable, and even simple functions are not Fourier transformable
\item [{solution:}] ``Generalized functions'' (sometimes called ``distributions'')
\end{description}
In order to define generalized functions, we first consider function
spaces in addition to $\gamma^{\left(1\right)}$.
\begin{defn}
\textbf{\emph{Test functions.}} A function $F:\mathbb{R}\rightarrow\mathbb{C}$
is called a \emph{test function} iff 

$\left(\textnormal{i}\right)\quad F$ is differentiable arbitrarily
many times, and 

$\left(\textnormal{ii}\right)\quad F$ and all of its derivatives
go to zero faster than any power\footnote{That is, $\lim_{x\rightarrow\infty}x^{N}F^{\left(n\right)}\left(x\right)=0$
for all $N,n\in\mathbb{N}$.} $\left|x\right|\rightarrow\infty$.\end{defn}
\begin{example}
$F\left(x\right)=e^{-x^{2}}$ is a test function, So is $x^{n}e^{-mx^{2}}$
for all $m,n\in\mathbb{N}$.\end{example}
\begin{defn}
\textbf{\emph{\label{def:Weakly-increasing-functions.}Weakly increasing
functions.}} A function $\phi:\mathbb{R}\rightarrow\mathbb{C}$ is
called a \emph{weakly increasing function} iff

$\left(\textnormal{i}\right)\quad\phi$ is differentiable arbitrarily
many times, and

$\left(\textnormal{i}\right)\quad\phi$ and all its derivatives do
not grow faster than $\left|x\right|^{N}$ for $\left|x\right|\rightarrow\infty$,
where $N\in\mathbb{N}$ may depend on the order of the derivative.\end{defn}
\begin{example}
$ $Any polynomial is a weakly increasing function, but $e^{x}$ is
not.\end{example}
\begin{rem}
The derivative of a test function is a test function; so is the sum
of two test functions, as well as scalar multiples of test functions.
\textbf{Thus, the set of test functions forms a vector space; we call
it $\gamma$.}
\end{rem}
~
\begin{rem}
Let $F$ be a test function and let $\phi$ be a weakly increasing
function. Then
\[
G\left(x\right):=F\left(x\right)\phi\left(x\right)
\]
 is a test function.
\end{rem}
Now, test functions are all Fourier transformable since they are absolute-integrable
(they die off at infinitely very fast). But is the Fourier transform
of a test function also a test function?
\begin{prop}
If $F\left(x\right)$ is a test function, then so is its Fourier transform,
$\hat{F}\left(k\right):=\int dxe^{-ikx}F\left(x\right)$.\end{prop}
\begin{proof}
Consider the $p^{\textnormal{th}}$ derivative of $\hat{F}\left(k\right)$:
\[
\hat{F}^{\left(p\right)}\left(k\right):=\frac{d^{p}}{dk^{p}}\hat{F}\left(k\right)=\left(-i\right)^{p}\indefint xx^{p}F\left(x\right)e^{-ikx}=\left(-i\right)^{p}\mathcal{F}\left[x^{p}F\left(x\right)\right]\left(k\right)
\]
 By the two remarks above, $x^{p}F\left(x\right)$ is a test function,
and so $\mathcal{F}\left[x^{p}F\left(x\right)\right]\left(k\right)$
exists.
\begin{eqnarray*}
\left|\hat{F}^{\left(p\right)}\left(k\right)\right| & = & \left|\indefint xx^{p}F\left(x\right)e^{-ikx}\right|\\
 & = & \left|\indefint xx^{p}F\left(x\right)\frac{1}{-ik}\frac{d}{dx}e^{-ikx}\right|
\end{eqnarray*}
Integrating by parts (the boundary term vanishes since $F$ is a test
function):
\[
\implies\left|\hat{F}^{\left(p\right)}\left(k\right)\right|=\left|\frac{1}{-ik}\indefint xe^{-ikx}\frac{d}{dx}\left(x^{p}F\left(x\right)\right)\right|.
\]
We can do this again to pile on more derivatives onto $x^{p}F\left(x\right)$
at the cost of a term $\frac{1}{-ik}$. Doing this $N-1$ more times
(where $N\in\mathbb{N}$ is arbitrary), we get
\[
\left|\hat{F}^{\left(p\right)}\left(k\right)\right|=\left|\left(\frac{1}{-ik}\right)^{N}\indefint xe^{-ikx}\frac{d^{N}}{dx^{N}}\left(x^{p}F\left(x\right)\right)\right|.
\]
 By the triangle inequality, this becomes
\[
\left|\hat{F}^{\left(p\right)}\left(k\right)\right|\leq\frac{1}{\left|k\right|^{N}}\underset{<\infty\textnormal{ since }F\in\gamma}{\underbrace{\indefint x\left|\frac{d^{N}}{dx^{N}}\left(x^{p}F\left(x\right)\right)\right|}}=O\left(\frac{1}{\left|k\right|^{N}}\right).
\]
 Since $N$ can be made arbitrarily large, $\left|\hat{F}^{\left(p\right)}\left(k\right)\right|$
falls off faster than any power. Thus, $\hat{F}^{\left(p\right)}\left(k\right)$
is a test function.\footnote{Specifically, $\hat{F}^{\left(0\right)}\left(k\right)=\hat{F}\left(k\right)$
is a test function, so the proposition is true.}\end{proof}
\begin{rem}
The inverse Fourier transform is given by the theorem in \cref{sub:ch3sec2_2Inverse-Fourier-transforms}.\end{rem}
\begin{prop}
\textbf{\emph{Parseval's equation.}} Let $F_{1}\left(x\right)$ and
$F_{2}\left(x\right)$ be test functions, and let $\hat{F}_{1}\left(k\right)$
and $\hat{F}_{2}\left(k\right)$ be their Fourier transforms. Then
\[
\int\frac{dk}{2\pi}\:\hat{F}_{1}\left(k\right)\hat{F}_{2}\left(k\right)=\indefint xF_{1}\left(x\right)F_{2}\left(-x\right).
\]
\end{prop}
\begin{proof}
$\int\frac{dk}{2\pi}\:\hat{F}_{1}\left(k\right)\hat{F}_{2}\left(k\right)=\int\frac{dk}{2\pi}\:\indefint xe^{-ikx}F_{1}\left(x\right)\hat{F}_{2}\left(k\right)=\indefint xF_{1}\left(x\right)\int\frac{dk}{2\pi}\:\hat{F}_{2}\left(k\right)e^{-ikx}=\indefint xF_{1}\left(x\right)F_{2}\left(-x\right).$

\end{proof}

\subsection{\label{sub:ch3sec2_4Generalized-functions}Generalized functions}
\begin{defn}
\textbf{\emph{\label{def:Regular-sequences.-Let}Regular sequences.
}}Let $n\in\mathbb{N}$, and let $\left\{ f_{n}\left(x\right)\right\} $
be a sequence of test functions. The sequence is called \emph{regular}
iff
\begin{equation}
\lim_{n\rightarrow\infty}\indefint xf_{n}\left(x\right)F\left(x\right)\label{eq:regular_limit}
\end{equation}
 exists for all test functions $F\left(x\right)$. \end{defn}
\begin{rem}
The integral exists for all $n$, so the only issue is whether the
limit exists.\end{rem}
\begin{example}
Consider the sequence $\left\{ e^{-\frac{x^{2}}{n^{2}}}\right\} ,$
where $n\in\mathbb{N}$. This sequence is regular since $\lim_{n\rightarrow\infty}\int dxe^{-\frac{x^{2}}{n^{2}}}F\left(x\right)=\int dxF\left(x\right)$
for all $F\in\gamma$. For proof, see Problem \#30.\end{example}
\begin{defn}
\textbf{\emph{Equivalence of regular sequences.}} Two regular sequences
of test functions are called \emph{equivalent} iff their limits from
Equation (\ref{eq:regular_limit}) are equal.\end{defn}
\begin{example}
$\left\{ e^{-\frac{x^{2}}{n^{4}}}\right\} $ is equivalent to $\left\{ e^{-\frac{x^{2}}{n^{2}}}\right\} $;
so is $\left\{ e^{-\frac{x^{2}}{n}}\right\} $.\end{example}
\begin{defn}
\textbf{\emph{\label{def:Generalized-functions,-regulariz}Generalized
functions, regularizations.}}\textbf{ }The set of all equivalent regular
sequences $\left\{ f_{n}\left(x\right)\right\} $ defines a \emph{generalized
function }(or \emph{distribution})\footnote{In other words, if we let $L\in\mathbb{C}$, then $f\left(x\right)=\left\{ \left\{ f_{n}\left(x\right)\right\} :\lim_{n\rightarrow\infty}\indefint xf_{n}\left(x\right)F\left(x\right)=L\right\} $.
(Note to reader: I am not sure if this is correct, gotta double check)} $f\left(x\right)$, and we define the integral 
\[
\indefint xf\left(x\right)F\left(x\right):=\lim_{n\rightarrow\infty}\indefint xf_{n}\left(x\right)F\left(x\right)
\]
 by the limit on the right hand side, which exists for all $F\in\gamma$
and is the same for all of the equivalent sequences.

Any of the equivalent sequences is called a \emph{regularization}
of the generalized function $f\left(x\right)$.\end{defn}
\begin{example}
\label{example:constant_gen_function}$\left\{ e^{-\frac{x^{2}}{n^{2}}}\right\} $
and its equivalent sequences define the generalized function $f\left(x\right)=1$.
$\left\{ e^{-\frac{x^{2}}{n^{2}}}\right\} $ is a regularization of
$f\left(x\right)=1$.\end{example}
\begin{rem}
The properties of the generalized function $f\left(x\right)=1$ coincide
with those of the ordinary function.
\end{rem}
~
\begin{rem}
Differentiation, addition, multiplication with weakly increasing functions,
and Fourier transforms of generalized functions can all be defined
in terms of their regularizations; doing so yields generalized functions.
However, multiplication between two generalized functions can \emph{not}
be consistently defined. \end{rem}
\begin{prop}
\label{prop:ordinary_generalized_functions}Let $f\left(x\right)$
be a function (in the ordinary sense) such that there exists an $N\in\mathbb{N}$
such that $\frac{f\left(x\right)}{\left(1+x^{2}\right)^{N}}$ is absolutely
integrable.

Then one can construct sequences of test functions $\left\{ f_{n}\left(x\right)\right\} $
such that $\lim_{n\rightarrow\infty}\indefint xf_{n}\left(x\right)F\left(x\right)=\indefint xf\left(x\right)F\left(x\right)$
for all test functions $F\left(x\right)$.\end{prop}
\begin{proof}
See books (e.g., Lighthill Chapter 2.3).\end{proof}
\begin{rem}
This result says that a large class of ordinary functions can be considered
generalized functions.\end{rem}
\begin{example}
Consider the ordinary function $\textnormal{sgn }x:=\frac{\left|x\right|}{x}$.
This function fulfills the premise of \cref{prop:ordinary_generalized_functions}
for $N=1$. Thus, $\textnormal{sgn }x$ is a generalized function.
One regularization is $\left\{ \tanh\left(nx\right)\right\} $ (for
proof see Problem \#31).\end{example}
\begin{rem}
Such constructed generalized functions are called \emph{regular generalized
functions}. The derivative of any regular generalized function exists,
but in general it is not regular.\end{rem}
\begin{example}
$\frac{d}{dx}\textnormal{sgn }x$ exists as a generalized function,
but it is not regular (see Problem \#32).\end{example}
\begin{defn}
\label{def:distribution_limit}\textbf{\emph{Distribution limit.}}
Let $f_{t}\left(x\right)$ be a generalized function for any value
of the parameter $t$, and let $f\left(x\right)$ be another generalized
function such that
\[
\lim_{t\rightarrow c}\indefint xf_{t}\left(x\right)F\left(x\right)=\indefint xf\left(x\right)F\left(x\right)
\]
 for all test functions $F\left(x\right)$, where $c$ may be finite
or infinite, and the set of parameters $t$ may be continuous or discrete.
Then we say
\[
\lim_{t\rightarrow c}f_{t}\left(x\right)=f\left(x\right).
\]
\end{defn}
\begin{rem}
This is sometimes called a \emph{distribution limit}. \end{rem}
\begin{example}
$\lim_{\epsilon\rightarrow0}\left|x\right|^{\epsilon}\textnormal{sgn }x=\textnormal{sgn }x$.
See Problem \#31(c) for more.
\end{example}
~
\begin{example}
Consider the test functions $f_{n}\left(x\right)$ that make up a
regular sequence (in the sense of \cref{def:Regular-sequences.-Let})
to be generalized functions (math books say we can), and let $f\left(x\right)$
be the generalized function that is defined by this sequence and its
equivalence class. Then
\[
\lim_{n\rightarrow\infty}f_{n}\left(x\right)=f\left(x\right).
\]
\end{example}
\begin{prop}
Under the conditions of \cref{def:distribution_limit}, we have

$\left(\textnormal{i}\right)\quad\lim_{t\rightarrow c}f_{t}'\left(x\right)=f'\left(x\right)$

$\left(\textnormal{ii}\right)\quad\lim_{t\rightarrow c}f_{t}\left(ax+b\right)=f\left(ax+b\right)$

$\left(\textnormal{iii}\right)\quad\lim_{t\rightarrow c}\phi\left(x\right)f_{t}\left(x\right)=\phi\left(x\right)f\left(x\right)$
for any weakly increasing function $\phi\left(x\right)$.\end{prop}
\begin{proof}
Math books.
\end{proof}

\subsection{The $\delta-$function}
\begin{defn}
\textbf{\emph{Dirac delta function.}} The generalized function $\delta\left(x\right)$
is defined as the set of equivalent regular sequences (of test functions)
for which 
\[
\boxed{\indefint x\delta\left(x\right)F\left(x\right)=\lim_{n\rightarrow\infty}\indefint xf_{n}\left(x\right)F\left(x\right)=F\left(0\right)\quad\forall F\in\gamma}.
\]
\end{defn}
\begin{rem}
There is no ordinary function that has this property.\end{rem}
\begin{prop}
One regularization of $\delta\left(x\right)$ is the sequence defined
by 
\[
\boxed{f_{n}\left(x\right)=\sqrt{\frac{n}{\pi}}e^{-nx^{2}}\quad\left(n\in\mathbb{N}\right).}
\]
 \end{prop}
\begin{proof}
First, note that $f_{n}$ are test functions, as required by \cref{def:Generalized-functions,-regulariz}.
Also note that
\[
\indefint xf_{n}\left(x\right)=\sqrt{\frac{n}{\pi}}\indefint xe^{-nx^{2}}=\frac{1}{\sqrt{\pi}}\indefint xe^{-x^{2}}=1.
\]
\begin{multline*}
\implies\left|\indefint xf_{n}\left(x\right)F\left(x\right)-F\left(0\right)\right|=\left|\indefint xf_{n}\left(x\right)\left(F\left(x\right)-F\left(0\right)\right)\right|\\
\leq\indefint xf_{n}\left(x\right)\left|F\left(x\right)-F\left(0\right)\right|=\indefint xf_{n}\left(x\right)\left|x\right|\left|\frac{F\left(x\right)-F\left(0\right)}{x}\right|\quad\\
\begin{aligned}\leq\left(\sup F'\right)\int dx\,\left|x\right|f_{n}\left(x\right) & = & 2\left(\sup F'\right)\int_{0}^{\infty}dx\:x\sqrt{\frac{n}{\pi}}e^{-nx^{2}}\\
 & = & \frac{2}{\sqrt{n\pi}}\underset{\textnormal{const.}}{\underbrace{\left(\sup F'\right)}}\underset{\textnormal{indep. of }n}{\underbrace{\int_{0}^{\infty}dx\:xe^{-x^{2}}}}
\end{aligned}
\\
\rightarrow0\quad\textnormal{for}\quad n\rightarrow\infty,
\end{multline*}
 where the first inequality is the triangle inequality, and the second
inequality comes from the fact that $F'$ is bounded, allowing us
to pull out the (finite) supremum of $F'$.\end{proof}
\begin{prop}
The Fourier transform of $\delta\left(x\right)$ is 
\[
\boxed{\hat{\delta}\left(k\right)=1}.
\]
\end{prop}
\begin{proof}
Consider the regularization $f_{n}\left(x\right)=\sqrt{\frac{n}{\pi}}e^{-nx^{2}}$.
From Problem \#27, $\hat{f}_{n}\left(k\right)=e^{-\frac{k^{2}}{4n}}$.
But from \cref{sub:ch3sec2_4Generalized-functions} \cref{example:constant_gen_function},
this is a regularization of the generalized function that is identically
equal to $1$.\end{proof}
\begin{cor}
The $\delta-$function can be written
\[
\boxed{\delta\left(x\right)=\int\frac{dk}{2\pi}\:e^{ikx}}.
\]
\end{cor}
\begin{proof}
From the theorem from \cref{sub:ch3sec2_2Inverse-Fourier-transforms},
\[
\delta\left(x\right)=\int\frac{dk}{2\pi}\:e^{ikx}\hat{\delta}\left(k\right)=\int\frac{dk}{2\pi}\:e^{ikx}.
\]
\end{proof}
\begin{rem}
This integral does not exist in classical analysis!\end{rem}
\begin{prop}
Let $\phi\left(x\right)$ be a weakly increasing function.\footnote{As per \cref{sub:ch3sec2_3Test-functions} \cref{def:Weakly-increasing-functions.}.}
Then
\[
\phi\left(x\right)\delta\left(x\right)=\phi\left(0\right)\delta\left(x\right).
\]
\end{prop}
\begin{proof}
$\indefint x\delta\left(x\right)\underset{\textnormal{test fct.}}{\underbrace{\phi\left(x\right)F\left(x\right)}}=\phi\left(0\right)F\left(0\right)=\phi\left(0\right)\indefint x\delta\left(x\right)F\left(x\right)\quad\forall F\in\gamma.$.\end{proof}
\begin{cor}
Let $\phi\left(x\right)$ be a weakly increasing function. Then\footnote{This result says we can now use the $\delta-$function with weakly
increasing functions!}
\[
\boxed{\indefint x\delta\left(x\right)\phi\left(x\right)=\phi\left(0\right)}.
\]
\end{cor}
\begin{proof}
$\indefint x\delta\left(x\right)\phi\left(x\right)=\phi\left(0\right)\indefint x\delta\left(x\right)=\phi\left(0\right)\hat{\delta}\left(k=0\right)=\phi\left(0\right).$\end{proof}
\begin{rem}
This is consistent with $\hat{\delta}\left(k\right)=\indefint xe^{-ikx}\delta\left(x\right)=1$.
\end{rem}
~
\begin{rem}
We can define \emph{even} and \emph{odd }generalized functions in
analogy to the definitions for ordinary functions:\end{rem}
\begin{example}
$\delta\left(x\right)=\delta\left(-x\right)$ is even, since $\delta\left(-x\right)=\int\frac{dk}{2\pi}\:e^{-ikx}=\int\frac{dk}{2\pi}\:e^{ikx}=\delta\left(x\right).$
Accordingly, $\delta'\left(x\right):=\frac{d}{dx}\delta\left(x\right)$
is odd.\end{example}
\begin{rem}
The $\delta-$function makes Fourier back transforms easy:
\[
\int\frac{dk}{2\pi}\:\hat{f}\left(k\right)e^{ikx}=\int\frac{dk}{2\pi}\:e^{ikx}\indefint ye^{-iky}f\left(y\right)=\indefint y\delta\left(y-x\right)f\left(y\right)=f\left(x\right).
\]
 We can now Fourier transform weakly increasing functions, not just
absolutely integrable ones!\end{rem}
\begin{prop}
The $\delta-$function has the properties

\[
\left(\textnormal{i}\right)\quad\delta\left(ax\right)=\frac{1}{\left|a\right|}\delta\left(x\right)\quad\forall a\in\mathbb{R}-\left\{ 0\right\} ,
\]


\[
\left(\textnormal{ii}\right)\quad f\left(x\right)\delta\left(a-x\right)=f\left(a\right)\delta\left(a-x\right),
\]


\[
\left(\textnormal{iii}\right)\quad\delta\left(f\left(x\right)\right)=\underset{j}{\sum}\frac{1}{\left|f'\left(x_{j}\right)\right|}\delta\left(x-x_{j}\right),
\]
 where the $x_{j}$ are all real zeros of $f\left(x\right)$ and we
assume they are simple and isolated.\end{prop}
\begin{proof}
~

\begin{eqnarray*}
\left(\textnormal{i}\right)\quad\indefint xF\left(x\right)\delta\left(ax\right)=\textnormal{sgn }a\int\frac{dx}{a}\:F\left(\frac{x}{a}\right)\delta\left(x\right)=\frac{\textnormal{sgn }a}{a}F\left(0\right) & = & \frac{1}{\left|a\right|}F\left(0\right)\\
 & = & \frac{1}{\left|a\right|}\indefint xF\left(x\right)\delta\left(x\right)\quad\forall F\in\gamma.
\end{eqnarray*}
\begin{eqnarray*}
\left(\textnormal{ii}\right)\quad\indefint xF\left(x\right)f\left(x\right)\delta\left(x-a\right)=\indefint xF\left(x+a\right)f\left(x+a\right)\delta\left(x\right) & = & F\left(a\right)f\left(a\right)\\
 & = & \indefint xF\left(x\right)f\left(a\right)\delta\left(a-x\right)\quad\forall F\in\gamma.
\end{eqnarray*}
$\left(\textnormal{iii}\right)\quad$ Let $f\left(x\right)=:y$, $\implies x=f^{-1}\left(y\right),$
$dy=f'\left(x\right)dx$. Then
\begin{eqnarray*}
\indefint xF\left(x\right)\delta\left(f\left(x\right)\right)=\underset{j}{\sum}\underset{x_{j}-\varepsilon}{\overset{x_{j}+\varepsilon}{\int}}dx\:F\left(x\right)\delta\left(f\left(x\right)\right) & = & \underset{f\left(x_{j}-\varepsilon\right)}{\overset{f\left(x_{j}+\varepsilon\right)}{\int}}dy\:\frac{F\left(x=f^{-1}\left(y\right)\right)}{\left|f'\left(x=f^{-1}\left(y\right)\right)\right|}\delta\left(y\right)\\
 & = & \frac{F\left(x_{j}\right)}{\left|f'\left(x_{j}\right)\right|}\\
 & = & \underset{j}{\sum}\int dx\,F\left(x\right)\frac{\delta\left(x-x_{j}\right)}{\left|f'\left(x_{j}\right)\right|}\quad\forall F\in\gamma
\end{eqnarray*}
\end{proof}
\begin{example}
$\delta\left(x^{2}-a^{2}\right)=\frac{1}{2\left|a\right|}\left[\delta\left(x+a\right)+\delta\left(x-a\right)\right].$
\end{example}



\section{Solutions of Poisson's Equation}


\subsection{\label{sub:ch3sec3_1The-general-solution}The general solution of
Poisson's equation}
\begin{prop}
Every Fourier transformable solution of Poisson's equation is uniquely
determined by the inhomogeneity $\rho$ via 
\[
\phi\left(\boldsymbol{x}\right)=\int\frac{d\boldsymbol{k}}{\left(2\pi\right)^{3}}\:e^{i\boldsymbol{k}\cdot\boldsymbol{x}}\frac{4\pi}{\boldsymbol{k}^{2}}\hat{\rho}\left(\boldsymbol{k}\right)
\]
\end{prop}
\begin{proof}
From \cref{sub:ch3sec1_1Electrostatics},\footnote{Here and elsewhere, the symbol $\overset{\mathcal{F}}{\longrightarrow}$
is used to indicate a Fourier transform is taken.} 
\begin{eqnarray*}
\nabla^{2}\phi=-4\pi\rho\quad\overset{\mathcal{F}}{\longrightarrow}\quad-\boldsymbol{k}^{2}\hat{\phi}\left(\boldsymbol{k}\right) & = & -4\pi\hat{\rho}\left(\boldsymbol{k}\right)\\
\implies\hat{\phi}\left(\boldsymbol{k}\right) & = & \frac{4\pi}{\boldsymbol{k}^{2}}\hat{\rho}\left(\boldsymbol{k}\right)\quad\overset{\mathcal{F}^{-1}}{\longrightarrow}\quad\phi\left(\boldsymbol{x}\right)=\mathcal{F}^{-1}\left[\frac{4\pi}{\boldsymbol{k}^{2}}\hat{\rho}\left(\boldsymbol{k}\right)\right]\left(\boldsymbol{x}\right).
\end{eqnarray*}
\end{proof}
\begin{rem}
Thanks to the theory in \ref{sec:ch3sec2_Digression:-Fourier-transforms},
the class of solutions that can be constructed in this way is much
larger than before, since weakly increasing functions are allowed.
\end{rem}
~
\begin{rem}
\label{rem:ch3sec3_1_rem2-Remark-}\cref{sub:ch3sec1_2Magnetostatics}
\cref{rem:Ch3sec1_2rem5_From--Remark} follows immediately:
\begin{eqnarray*}
\nabla^{2}\phi=0 & \iff & \boldsymbol{k}^{2}\hat{\phi}\left(\boldsymbol{k}\right)=0\\
 & \iff & \hat{\phi}\left(\boldsymbol{k}\right)=0\quad\forall\boldsymbol{k}\neq\boldsymbol{0}\\
 & \iff & \phi\left(\boldsymbol{x}\right)=\textnormal{const.}
\end{eqnarray*}

\end{rem}
~
\begin{rem}
All of this is consistent with \cref{sub:ch3sec1_2Magnetostatics}
\cref{rem:Ch3sec1_2rem4_linear}.
\end{rem}

\subsection{\label{sub:ch3sec3_2The-Coulomb-potential}The Coulomb potential}

What is the potential from one charge?

Consider a point charge: $\rho\left(\boldsymbol{x}\right)=e\delta\left(\boldsymbol{x}\right)$,
where $\delta\left(\boldsymbol{x}\right):=\delta\left(x\right)\delta\left(y\right)\delta\left(z\right)$.
\begin{thm}
The electrostatic potential resulting from a point charge is the \emph{Coulomb
potential}:\footnote{$r:=\left|\boldsymbol{x}\right|$}
\[
\boxed{\phi\left(\boldsymbol{x}\right)=\frac{e}{r}}.
\]
\end{thm}
\begin{proof}
\[
\hat{\rho}\left(\boldsymbol{k}\right)=\mathcal{F}\left[e\delta\left(\boldsymbol{x}\right)\right]\left(\boldsymbol{k}\right)=e
\]
\[
\implies\hat{\phi}\left(\boldsymbol{k}\right)=\frac{4\pi}{\boldsymbol{k}^{2}}e\quad\overset{\mathcal{F}^{-1}}{\longrightarrow}\quad\phi\left(\boldsymbol{x}\right)=\frac{e}{r}
\]
 (For derivation of this inverse Fourier transform, see Problem \#28).\end{proof}
\begin{rem}
We have now \emph{derived }the Coulomb potential from a least action
principle, whereas it was postulated in PHYS 611.\end{rem}
\begin{cor}
The electric field of a point charge is
\[
\boxed{\boldsymbol{E}\left(\boldsymbol{x}\right)=e\frac{\boldsymbol{x}}{r^{3}}}.
\]
\end{cor}
\begin{proof}
$\boldsymbol{E}=-\nabla\phi=-e\nabla\frac{1}{r}=-e\left(-\frac{1}{2}\frac{2\boldsymbol{x}}{r^{3}}\right)=e\frac{\boldsymbol{x}}{r^{3}}.$\end{proof}
\begin{rem}
The electric field of a point charge is purely radial and isotropic.
\end{rem}

\subsection{\label{sub:ch3sec3_3Poisson's-formula}Poisson's formula}
\begin{prop}
Let $\rho\left(\boldsymbol{x}\right)$ be a charge distribution whose
Fourier transform exists. Then 
\[
\boxed{\phi\left(\boldsymbol{x}\right)=\indefint{\boldsymbol{y}}\frac{\rho\left(\boldsymbol{y}\right)}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}}.
\]
 This is known as \emph{Poisson's formula}.\end{prop}
\begin{proof}
From \cref{sub:ch3sec3_1The-general-solution},
\[
\phi\left(\boldsymbol{x}\right)=\mathcal{F}^{-1}\underset{\hat{\phi}\left(\boldsymbol{k}\right)}{\left[\underbrace{\frac{4\pi}{\boldsymbol{k}^{2}}\hat{\rho}\left(\boldsymbol{k}\right)}\right]}\left(\boldsymbol{x}\right).
\]
 We know that
\begin{eqnarray*}
\mathcal{F}^{-1}\left[\frac{4\pi}{\boldsymbol{k}^{2}}\right]\left(\boldsymbol{x}\right) & = & \frac{1}{\left|\boldsymbol{x}\right|},\\
\mathcal{F}^{-1}\left[\hat{\rho}\left(\boldsymbol{k}\right)\right]\left(\boldsymbol{x}\right) & = & \rho\left(\boldsymbol{x}\right).
\end{eqnarray*}
 From the convolution theorem from \cref{sub:ch3sec2_1The-Fourier-transform},
\begin{eqnarray*}
\phi\left(\boldsymbol{x}\right) & = & \left(\mathcal{F}^{-1}\left[\frac{4\pi}{\boldsymbol{k}^{2}}\right]\star\mathcal{F}^{-1}\left[\hat{\rho}\left(\boldsymbol{k}\right)\right]\right)\left(\boldsymbol{x}\right)\\
 & = & \indefint{\boldsymbol{y}}\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\rho\left(\boldsymbol{y}\right).
\end{eqnarray*}
\end{proof}
\begin{rem}
For $\rho\left(\boldsymbol{y}\right)=e\delta\left(\boldsymbol{y}\right),$
we get
\[
\phi\left(\boldsymbol{x}\right)=\indefint{\boldsymbol{y}}\underset{\begin{array}{c}
\textnormal{weakly}\\
\textnormal{inc.}
\end{array}}{\underbrace{\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}}}e\delta\left(\boldsymbol{y}\right)=\frac{e}{\left|\boldsymbol{x}\right|}.
\]

\end{rem}
~
\begin{rem}
$\nabla_{\boldsymbol{x}}\left(\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\right)=-\frac{\left(\boldsymbol{x}-\boldsymbol{y}\right)}{\left|\boldsymbol{x}-\boldsymbol{y}\right|^{3}}$
\[
\implies\boxed{\boldsymbol{E}\left(\boldsymbol{x}\right)=\indefint{\boldsymbol{y}}\frac{\boldsymbol{x}-\boldsymbol{y}}{\left|\boldsymbol{x}-\boldsymbol{y}\right|^{3}}\rho\left(\boldsymbol{y}\right)}.
\]

\end{rem}

\subsection{The field of a uniformly moving charge}

Consider a charge $e$ that moves with constant velocity with respect
to an observer. Finding the fields are potentials is much easier if
one starts in the frame of the charge!

Let $CS'$ be the inertial frame in which the charge is at rest. From
\cref{sub:ch3sec3_2The-Coulomb-potential},\footnote{The primes are not derivatives!}
\[
\phi'\left(\boldsymbol{x}'\right)=\frac{e}{r'},
\]
 and 
\[
A'^{\mu}=\left(\phi'\left(\boldsymbol{x}'\right),0\right).
\]


Let $CS$ be the inertial frame of the observer, and let $\boldsymbol{v}=\left(v,0,0\right)$.
Then $CS$ and $CS'$ are related by a Lorentz boost; from \cref{chap:Maxwell's-Equations}
\cref{sub:ch2sec4_1Physical-interpretation-of},\footnote{Recall that $\gamma:=\frac{1}{\sqrt{1-\frac{v^{2}}{c^{2}}}}$.}
\begin{eqnarray*}
x' & = & \gamma\left(x-vt\right)\\
y' & = & y\\
z' & = & z
\end{eqnarray*}
 and, by boosting $A'^{\mu}$, 
\begin{eqnarray*}
\underset{A^{0}}{\underbrace{\phi}}=\gamma\underset{A'^{0}}{\underbrace{\phi'}}=\gamma\frac{e}{r'} & = & \gamma\frac{e}{\sqrt{x'^{2}+y'^{2}+z'^{2}}}\\
 & = & \gamma\frac{e}{\left(\gamma^{2}\left(x-vt\right)^{2}+y^{2}+z^{2}\right)^{\frac{1}{2}}}\\
 & = & \frac{e}{\left(\left(x-vt\right)^{2}+\left(1-\frac{v^{2}}{c^{2}}\right)\left(y^{2}+z^{2}\right)\right)^{\frac{1}{2}}}.
\end{eqnarray*}
 This is the scalar potential due to the moving charge, which we can
rewrite as
\[
\boxed{\phi\left(\boldsymbol{x},t\right)=\frac{e}{R^{*}}},\quad\textnormal{where }\boxed{R^{*}:=\sqrt{\left(x-vt\right)^{2}+\left(1-\frac{v^{2}}{c^{2}}\right)\left(y^{2}+z^{2}\right)}=\frac{r'}{\gamma}}.
\]


What about $\boldsymbol{A}$?
\[
\boldsymbol{A}\left(\boldsymbol{x},t\right)=\gamma\frac{\boldsymbol{v}}{c}\phi'=\frac{\boldsymbol{v}}{c}\phi\left(\boldsymbol{x},t\right)
\]
\[
\implies\boxed{\boldsymbol{A}=\frac{\boldsymbol{v}}{c}\frac{e}{R^{*}}}.
\]


We calculate the fields using the same procedure. In $CS'$, we have
\[
\boldsymbol{E}'\left(\boldsymbol{x}'\right)=e\frac{\boldsymbol{x}'}{r'^{3}},\quad\boldsymbol{B}'\left(\boldsymbol{x}'\right)=0.
\]
 We boost these, using the results from \cref{chap:Maxwell's-Equations}
\cref{sub:ch2sec4_2Transformations-of-}:
\begin{eqnarray*}
E_{x} & = & E'_{x}=\frac{ex'}{\left(r'\right)^{3}}=\frac{e}{\gamma^{2}}\frac{x-vt}{\left(R^{*}\right)^{3}}\\
E_{y} & = & \gamma E'_{y}=\gamma\frac{ey'}{\left(r'\right)^{3}}=\frac{e}{\gamma^{2}}\frac{y}{\left(R^{*}\right)^{3}}\\
E_{z} & = & \frac{e}{\gamma^{2}}\frac{z}{\left(R^{*}\right)^{3}}.
\end{eqnarray*}
 Thus,
\[
\boxed{\boldsymbol{E}=\frac{e}{\gamma^{2}}\frac{\boldsymbol{R}}{\left(R^{*}\right)^{3}}},\textnormal{ where }\boxed{\boldsymbol{R}\left(\boldsymbol{x},t\right):=\left(x-vt,y,z\right)}.
\]
 Note that $\boldsymbol{R}$ is the Galilean transformed $\boldsymbol{x}$.

What about $\boldsymbol{B}$? Again, from \cref{chap:Maxwell's-Equations}
\cref{sub:ch2sec4_2Transformations-of-},
\begin{eqnarray*}
B_{x} & = & B'_{x}=0\\
B_{y} & = & -\gamma\frac{v}{c}E'_{z}=-\frac{v}{c}E_{z}\\
B_{z} & = & \gamma\frac{v}{c}E'_{y}=\frac{v}{c}E_{y}
\end{eqnarray*}
\[
\implies\boxed{\boldsymbol{B}\left(\boldsymbol{x},t\right)=\frac{\boldsymbol{v}}{c}\times\boldsymbol{E}\left(\boldsymbol{x},t\right)}.
\]



\subsubsection*{Discussion of $\boldsymbol{E}\left(\boldsymbol{x},t\right)$:}

Let $\theta$ be the angle between $\boldsymbol{v}$ and $\boldsymbol{R}$.\footnote{Reminder: in this section and elsewhere, a bold letter represents
a vector, and the unbolded letter represents the magnitude of that
vector: $\boldsymbol{R}$ vs. $R:=\left|\boldsymbol{R}\right|$.}
\[
\implies\frac{\sqrt{y^{2}+z^{2}}}{R}=\sin\theta\quad\implies y^{2}+z^{2}=R^{2}\sin^{2}\theta
\]
\[
\implies\left(R^{*}\right)^{2}=R^{2}-\frac{v^{2}}{c^{2}}\left(y^{2}+z^{2}\right)=R^{2}\left(1-\frac{v^{2}}{c^{2}}\sin^{2}\theta\right)
\]
\[
\implies\boxed{\boldsymbol{E}\left(\boldsymbol{x},t\right)=\frac{e}{\gamma^{2}}\frac{\boldsymbol{R}\left(\boldsymbol{x},t\right)}{R^{3}\left(\boldsymbol{x},t\right)}\frac{1}{\left[1-\frac{v^{2}}{c^{2}}\sin^{2}\theta\left(t\right)\right]^{\frac{3}{2}}}}.
\]
 Thus, for a fixed distance $R$ from the charge, $\boldsymbol{E}$
is minimized for $\theta=0,\pi$; i.e., in the direction of the motion.
The minimal value is
\[
\underline{E_{\Vert}=\frac{e}{R^{2}}\left(1-\frac{v^{2}}{c^{2}}\right)}.
\]
 We can maximize $\boldsymbol{E}$ by taking $\theta=\pm\frac{\pi}{2}$;
i.e., in the direction perpendicular to the motion. The maximal value
is
\[
\underline{E_{\bot}=\frac{e}{R^{2}}\frac{1}{\sqrt{1-\frac{v^{2}}{c^{2}}}}}.
\]
 The field is no longer isotropic, but squeezed in the direction of
the motion.. This is a manifestation of the Lorentz contraction.
\begin{rem}
We \emph{could} have solved for the fields in this way: The 4-current
in $CS'$ is
\[
J'^{\mu}=\left(\rho'\left(x'\right),\boldsymbol{j}'\left(x'\right)\right),\quad\textnormal{with }\rho'\left(x'\right)=e\delta\left(\boldsymbol{x}'\right),\,\boldsymbol{j}'=\boldsymbol{0}.
\]
 Thus, the observer in $CS$ sees\end{rem}
\begin{description}
\item [{charge~density:}] $\underline{\rho\left(\boldsymbol{x},t\right)}=\gamma\rho'\left(\boldsymbol{x}',t'\right)=\gamma e\delta\left(\gamma\left(x-vt\right)\right)\delta\left(y\right)\delta\left(z\right)=\underline{e\delta\left(\boldsymbol{R}\right)}.$
\item [{current~density:}] $\underline{\boldsymbol{j}\left(\boldsymbol{x},t\right)}=\gamma\frac{\boldsymbol{v}}{c}c\rho'=\boldsymbol{v}\rho=\underline{e\boldsymbol{v}\delta\left(\boldsymbol{R}\right)}.$
\end{description}
Then we solve Maxwell's equations for this time-dependent 4-current.
This is equivalent, but much harder to do!


\subsection{\label{sub:ch3sec3_5Electrostatic-interaction}Electrostatic interaction}

Consider a time-independent charge density.
\begin{prop}
The energy of the electric field produced by $\rho=\rho\left(\boldsymbol{x}\right)$
is
\[
\boxed{U=\frac{1}{2}\indefint{\boldsymbol{x}}d\boldsymbol{y}\:\rho\left(\boldsymbol{x}\right)\rho\left(\boldsymbol{y}\right)\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}}.
\]
\end{prop}
\begin{proof}
From \cref{chap:Maxwell's-Equations} \cref{sub:ch2sec3_6Poynting's-theorem},
\begin{eqnarray*}
U & = & \frac{1}{8\pi}\underset{V}{\int}d\boldsymbol{x}\:\boldsymbol{E}^{2}\left(\boldsymbol{x}\right)\\
 & = & -\frac{1}{8\pi}\indefint{\boldsymbol{x}}\boldsymbol{E}\cdot\nabla\phi\\
 & = & -\frac{1}{8\pi}\underset{\underset{\rightarrow0\textnormal{ as }V\rightarrow\infty}{\int d\boldsymbol{s}\cdot\boldsymbol{E}\phi}}{\underbrace{\indefint{\boldsymbol{x}}\nabla\left(\boldsymbol{E}\phi\right)}}+\frac{1}{8\pi}\indefint{\boldsymbol{x}}\underset{4\pi\rho}{\underbrace{\left(\nabla\cdot\boldsymbol{E}\right)}}\phi\\
 & = & \frac{1}{2}\indefint{\boldsymbol{x}}\rho\left(\boldsymbol{x}\right)\phi\left(\boldsymbol{x}\right)\\
 & = & \frac{1}{2}\indefint{\boldsymbol{x}}d\boldsymbol{y}\:\rho\left(\boldsymbol{x}\right)\rho\left(\boldsymbol{y}\right)\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|},
\end{eqnarray*}
where the last line follows from Poisson's formula (\cref{sub:ch3sec3_3Poisson's-formula})\end{proof}
\begin{rem}
Let $\rho\left(\boldsymbol{x}\right)$ be composed of $N$ localized
charge distributions:$\rho\left(\boldsymbol{x}\right)=\summation{\alpha=1}N\rho^{\left(\alpha\right)}\left(\boldsymbol{x}\right).$
\begin{eqnarray*}
\implies U & = & \frac{1}{2}\summation{\alpha,\beta}{}\indefint{\boldsymbol{x}}d\boldsymbol{y}\:\rho^{\left(\alpha\right)}\left(\boldsymbol{x}\right)\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\rho^{\left(\beta\right)}\left(\boldsymbol{y}\right)\\
 & = & \summation{\alpha}{}U^{\left(\alpha\right)}+\summation{\alpha\neq\beta}{}U^{\left(\alpha,\beta\right)},
\end{eqnarray*}
 where 
\[
\underline{U^{\left(\alpha\right)}:=\frac{1}{2}\indefint{\boldsymbol{x}}d\boldsymbol{y}\:\rho^{\left(\alpha\right)}\left(\boldsymbol{x}\right)\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\rho^{\left(\alpha\right)}\left(\boldsymbol{y}\right)}
\]
 is called the \emph{electrostatic self-energy} of charge distribution
$\alpha$, and
\[
\underline{U^{\left(\alpha,\beta\right)}:=\left(1-\delta_{\alpha\beta}\right)\frac{1}{2}\indefint{\boldsymbol{x}}d\boldsymbol{y}\:\rho^{\left(\alpha\right)}\left(\boldsymbol{x}\right)\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\rho^{\left(\beta\right)}\left(\boldsymbol{y}\right)}
\]
 is called the \emph{electrostatic interaction energy} of localized
charge distributions $\alpha$ and $\beta$ via the Coulomb interaction.
As we shall see, the $\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}$
term in the self-energy is an issue. 
\end{rem}
~
\begin{rem}
Consider charged point particles: $\rho^{\left(\alpha\right)}\left(\boldsymbol{x}\right)=e_{\alpha}\delta\left(\boldsymbol{x}-\boldsymbol{x}^{\left(\alpha\right)}\right).$
\[
\implies U^{\left(\alpha,\beta\right)}=\left(1-\delta_{\alpha\beta}\right)\frac{1}{2}\frac{e_{\alpha}e_{\beta}}{\left|\boldsymbol{x}^{\left(\alpha\right)}-\boldsymbol{x}^{\left(\beta\right)}\right|}.\tag{Coulomb interaction}
\]
 But $U^{\left(\alpha\right)}$ does not exist since we get $\frac{1}{0}$
once the $\delta$ functions are applied to the integrals.
\end{rem}
~
\begin{rem}
Thus, the concept of a point charge leads to an infinite self-energy
and makes no sense in classical electrodynamics. Only the interaction
energy of point charges is physically meaningful.
\end{rem}
~
\begin{rem}
One solution is to propose that maybe there aren't point charges;
particles have some spacial extension. Let's estimate the smallest
extension $r_{0}$ of a charge $e$ that still makes physical sense.

Let $\frac{e^{2}}{r_{0}}\cong mc^{2}$. Then $r_{0}\cong\frac{e^{2}}{mc^{2}}$.
For electrons, $r_{0}^{e}:=\frac{e^{2}}{m_{e}c^{2}}\cong\SI{2.8E-13}{cm}$.
This is called the \emph{classical electron radius}. But experimental
results place an upper limit on the radius of the electron to be $r_{e}<\SI{E-20}{cm}$. 

We see that something is wrong with classical electrodynamics. Quantum
mechanics is needed to resolve this issue. Ultimately, perfectly point-like
things are not likely to be physical though. The Planck length may
be the limit.
\end{rem}

\subsection{\label{sub:ch3sec3_6The-law-of}The law of Biot \& Savart}
\begin{prop}
\label{prop:A-stationary-current}A stationary\footnote{That is, a ``macroscopically stationary'' or ``steady'' current.}
current density distribution $\boldsymbol{j}=\boldsymbol{j}\left(\boldsymbol{x}\right)$
leads to a vector potential
\[
\boxed{\boldsymbol{A}=\frac{1}{c}\indefint{\boldsymbol{y}}\frac{\boldsymbol{j}\left(\boldsymbol{y}\right)}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}}.
\]
\end{prop}
\begin{proof}
From \cref{sub:ch3sec1_2Magnetostatics}, each component of $\boldsymbol{A}$
obeys Poisson's equation. The solution for each component is given
by Poisson's formula. \end{proof}
\begin{rem}
The proof in \cref{sub:ch3sec1_2Magnetostatics} \cref{prop:PoissonsEquationsj}
required the use of the Coulomb gauge ($\nabla\cdot\boldsymbol{A}=0$)!\end{rem}
\begin{prop}
\textbf{\emph{Law of Biot \& Savart}}

The magnetic field generated by a static current density is
\[
\boxed{\boldsymbol{B}\left(\boldsymbol{x}\right)=-\frac{1}{c}\indefint{\boldsymbol{y}}\frac{\left(\boldsymbol{x}-\boldsymbol{y}\right)\times\boldsymbol{j}\left(\boldsymbol{y}\right)}{\left|\boldsymbol{x}-\boldsymbol{y}\right|^{3}}}.
\]
\end{prop}
\begin{proof}
$\boldsymbol{B}=\nabla\times\boldsymbol{A}$, and 
\begin{eqnarray*}
\left(\nabla_{\boldsymbol{x}}\times\frac{\boldsymbol{j}\left(\boldsymbol{y}\right)}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\right)_{i} & = & \varepsilon_{ijk}\partial_{j}\frac{j_{k}\left(\boldsymbol{y}\right)}{\left(\summation{l=1}3\left(x_{l}-y_{l}\right)^{2}\right)^{\frac{1}{2}}}\\
 & = & \varepsilon_{ijk}j_{k}\left(\boldsymbol{y}\right)\left(-\frac{1}{2}\right)\frac{2\left(x_{j}-y_{j}\right)}{\left|\boldsymbol{x}-\boldsymbol{y}\right|^{3}}\\
 & = & -\varepsilon_{ijk}\left(\boldsymbol{x}-\boldsymbol{y}\right)_{j}j_{k}\left(\boldsymbol{y}\right)\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|^{3}}\\
 & = & -\left[\left(\boldsymbol{x}-\boldsymbol{y}\right)\times\boldsymbol{j}\left(\boldsymbol{y}\right)\right]_{i}\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|^{3}}.
\end{eqnarray*}
\end{proof}
\begin{rem}
Notice the analogy between electrostatics and magnetostatics.
\end{rem}
~
\begin{rem}
See 4.6 below for a discussion of the concept of a stationary current
density
\end{rem}

\subsection{Magnetostatic interaction}

Consider a time-independent current density.
\begin{prop}
\label{prop:magnetostatic_interaction}The energy of the magnetic
field produced by $\boldsymbol{j}=\boldsymbol{j}\left(\boldsymbol{x}\right)$
is
\[
\boxed{U=\frac{1}{2c^{2}}\indefint{\boldsymbol{x}}d\boldsymbol{y}\:\boldsymbol{j}\left(\boldsymbol{x}\right)\cdot\boldsymbol{j}\left(\boldsymbol{y}\right)\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}}.
\]
\end{prop}
\begin{lem}
$\nabla\cdot\left(\boldsymbol{A}\times\boldsymbol{B}\right)=\boldsymbol{B}\cdot\left(\nabla\times\boldsymbol{A}\right)-\boldsymbol{A}\cdot\left(\nabla\times\boldsymbol{B}\right)$\end{lem}
\begin{proof}
\begin{eqnarray*}
\nabla\cdot\left(\boldsymbol{A}\times\boldsymbol{B}\right) & = & \partial_{i}\varepsilon_{ijk}A_{j}B_{k}\\
 & = & \varepsilon_{ijk}\left(\partial_{i}A_{j}\right)B_{k}+\varepsilon_{ijk}A_{j}\left(\partial_{i}B_{k}\right)\\
 & = & B_{k}\varepsilon_{kij}\partial_{i}A_{j}-A_{j}\varepsilon_{jik}\partial_{i}B_{k}\\
 & = & \boldsymbol{B}\cdot\left(\nabla\times\boldsymbol{A}\right)-\boldsymbol{A}\cdot\left(\nabla\times\boldsymbol{B}\right)
\end{eqnarray*}

\end{proof}
~
\begin{proof}
\emph{(Of \cref{prop:magnetostatic_interaction})}

From \cref{chap:Maxwell's-Equations} \cref{sub:ch2sec3_6Poynting's-theorem},
\begin{eqnarray*}
U & = & \frac{1}{8\pi}\underset{V}{\int}d\boldsymbol{x}\:\boldsymbol{B}^{2}\left(\boldsymbol{x}\right)\\
 & = & \frac{1}{8\pi}\indefint{\boldsymbol{x}}\boldsymbol{B}\cdot\left(\nabla\times\boldsymbol{A}\right)\\
 & \overset{1.}{=} & \frac{1}{8\pi}\underset{\underset{\rightarrow0\textnormal{ as }V\rightarrow\infty}{\int d\boldsymbol{s}\cdot\left(\boldsymbol{A}\times\boldsymbol{B}\right)}}{\underbrace{\indefint{\boldsymbol{x}}\nabla\cdot\left(\boldsymbol{A}\times\boldsymbol{B}\right)}}+\frac{1}{8\pi}\indefint{\boldsymbol{x}}\underset{\frac{4\pi}{c}\boldsymbol{j}}{\boldsymbol{A}\cdot\underbrace{\left(\nabla\times\boldsymbol{B}\right)}}\\
 & = & \frac{1}{2c}\indefint{\boldsymbol{x}}\boldsymbol{A}\left(\boldsymbol{x}\right)\cdot\boldsymbol{j}\left(\boldsymbol{x}\right)\\
 & \overset{2.}{=} & \frac{1}{2c}\indefint{\boldsymbol{x}}\boldsymbol{j}\left(\boldsymbol{x}\right)\cdot\frac{1}{c}\indefint{\boldsymbol{y}}\frac{\boldsymbol{j}\left(\boldsymbol{y}\right)}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\\
 & = & \frac{1}{2c^{2}}\indefint{\boldsymbol{x}}d\boldsymbol{y}\:\frac{\boldsymbol{j}\left(\boldsymbol{x}\right)\cdot\boldsymbol{j}\left(\boldsymbol{y}\right)}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}
\end{eqnarray*}

\begin{enumerate}
\item From the lemma above.
\item From \cref{sub:ch3sec3_6The-law-of} \cref{prop:A-stationary-current}.
\end{enumerate}
\end{proof}
\begin{rem}
Let $\boldsymbol{j}\left(\boldsymbol{x}\right)$ be composed of $N$
localized current distributions: $\boldsymbol{j}\left(\boldsymbol{x}\right)=\summation{\alpha=1}N\boldsymbol{j}^{\left(\alpha\right)}\left(\boldsymbol{x}\right).$
\begin{eqnarray*}
\implies U & = & \frac{1}{2c^{2}}\summation{\alpha,\beta}{}\indefint{\boldsymbol{x}}d\boldsymbol{y}\:\boldsymbol{j}^{\left(\alpha\right)}\left(\boldsymbol{x}\right)\cdot\boldsymbol{j}^{\left(\beta\right)}\left(\boldsymbol{y}\right)\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\\
 & = & \summation{\alpha}{}U^{\left(\alpha\right)}+\summation{\alpha\neq\beta}{}U^{\left(\alpha,\beta\right)},
\end{eqnarray*}
 where 
\[
\underline{U^{\left(\alpha\right)}:=\frac{1}{2c^{2}}\indefint{\boldsymbol{x}}d\boldsymbol{y}\:\boldsymbol{j}^{\left(\alpha\right)}\left(\boldsymbol{x}\right)\cdot\boldsymbol{j}^{\left(\alpha\right)}\left(\boldsymbol{y}\right)\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}}
\]
 is called the \emph{magnetostatic self-energy} of charge distribution
$\alpha$, and
\[
\underline{U^{\left(\alpha,\beta\right)}:=\left(1-\delta_{\alpha\beta}\right)\frac{1}{2}\indefint{\boldsymbol{x}}d\boldsymbol{y}\:\boldsymbol{j}^{\left(\alpha\right)}\left(\boldsymbol{x}\right)\cdot\boldsymbol{j}^{\left(\beta\right)}\left(\boldsymbol{y}\right)\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}}
\]
 is called the \emph{magnetostatic interaction energy} of localized
current distributions $\alpha$ and $\beta$ via the magnetostatic
interaction.
\end{rem}

\section{Multipole expansion for static fields}


\subsection{\label{sub:ch3sec4_1The-electric-dipole}The electric dipole moment}

Consider a localized charge distribution $\rho=\rho\left(\boldsymbol{y}\right)$.
\begin{description}
\item [{question:}] What are the potential $\phi\left(\boldsymbol{x}\right)$
and the field $\boldsymbol{E}\left(\boldsymbol{x}\right)$ at a point
$\boldsymbol{x}$ far from the charges?
\end{description}
Let $\rho\left(\boldsymbol{y}\right)=0$ for $\left|\boldsymbol{y}\right|>r_{0}$;
let $\left|\boldsymbol{x}\right|=:r\gg r_{0}$.
\begin{eqnarray*}
\implies\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}=\frac{1}{\sqrt{r^{2}-2\boldsymbol{x}\cdot\boldsymbol{y}+\boldsymbol{y}^{2}}} & = & \frac{1}{r}\biggl(1-2\understuff{\frac{\boldsymbol{x}\cdot\boldsymbol{y}}{r^{2}}}{O\left(\frac{r_{0}}{r}\right)}+\understuff{\frac{\boldsymbol{y}^{2}}{r^{2}}}{O\left(\frac{r_{0}^{2}}{r^{2}}\right)}\biggr)^{-\frac{1}{2}}\\
 & = & \frac{1}{r}\left[1+\frac{\boldsymbol{x}\cdot\boldsymbol{y}}{r^{2}}+O\left(\frac{1}{r^{2}}\right)\right],
\end{eqnarray*}
 where the last step follows from the binomial approximation. Poisson's
formula (\cref{sub:ch3sec3_3Poisson's-formula}) gives
\begin{eqnarray*}
\phi\left(\boldsymbol{x}\right) & = & \indefint{\boldsymbol{y}}\frac{\rho\left(\boldsymbol{y}\right)}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}=\indefint{\boldsymbol{y}}\rho\left(\boldsymbol{y}\right)\frac{1}{r}\left[1+\frac{\boldsymbol{x}\cdot\boldsymbol{y}}{r^{2}}+O\left(\frac{1}{r^{2}}\right)\right]\\
 & = & \frac{1}{r}\indefint{\boldsymbol{y}}\rho\left(\boldsymbol{y}\right)+\frac{\boldsymbol{x}}{r^{3}}\cdot\indefint{\boldsymbol{y}}\boldsymbol{y}\rho\left(\boldsymbol{y}\right)+O\left(\frac{1}{r^{3}}\right)
\end{eqnarray*}

\begin{prop}
For large distances $r$ from the localized charge distribution, the
scalar potential has the form
\[
\boxed{\phi\left(\boldsymbol{x}\right)=\frac{Q}{r}+\frac{\boldsymbol{d}\cdot\boldsymbol{x}}{r^{3}}+O\left(\frac{1}{r^{3}}\right)},\textnormal{ }\mathit{where}
\]


$\underline{Q:=\indefint{\boldsymbol{y}}\rho\left(\boldsymbol{y}\right)}$
is the \emph{total charge}, and

$\underline{\boldsymbol{d}:=\indefint{\boldsymbol{y}}\boldsymbol{y}\rho\left(\boldsymbol{y}\right)}$
is the \emph{electric dipole moment.}
\end{prop}
~
\begin{rem}
Analogous results hold for the gravitational potential of a localized
mass distribution (PHYS 611). 
\end{rem}
~
\begin{rem}
If $Q=0$, then $\boldsymbol{d}$ is independent of the origin of
the coordinate system:

Let $\boldsymbol{x}'=\boldsymbol{x}+\boldsymbol{a}$ with $\boldsymbol{a}=\textnormal{const.}$
Then in the new coordinate system we have $\rho'\left(\boldsymbol{y}\right)=\rho\left(\boldsymbol{y}-\boldsymbol{a}\right)$
\begin{eqnarray*}
\implies\underline{\boldsymbol{d}'} & = & \indefint{\boldsymbol{y}}\boldsymbol{y}\rho'\left(\boldsymbol{y}\right)\\
 & = & \indefint{\boldsymbol{y}}\boldsymbol{y}\rho\left(\boldsymbol{y}-\boldsymbol{a}\right)=\indefint{\boldsymbol{y}}\left(\boldsymbol{y}+\boldsymbol{a}\right)\rho\left(\boldsymbol{y}\right)=\underline{\boldsymbol{d}+Q\boldsymbol{a}}
\end{eqnarray*}
 $\therefore Q=0\implies\boldsymbol{d}'=\boldsymbol{d}.$

If you ever get confused about this, consider a collection of point
charges $e_{\alpha}$ at locations $\boldsymbol{x}_{\alpha}$:
\[
\rho\left(\boldsymbol{y}\right)=\summation{\alpha}{}e_{\alpha}\delta\left(\boldsymbol{y}-\boldsymbol{x}_{\alpha}\right)
\]
 Transform $CS\rightarrow CS'$ such that $\boldsymbol{x}'_{\alpha}=\boldsymbol{x}_{\alpha}+\boldsymbol{a}$.
Then
\begin{eqnarray*}
\rho'\left(\boldsymbol{y}\right) & = & \summation{\alpha}{}e_{\alpha}\delta\left(\boldsymbol{y}-a-\boldsymbol{x}_{\alpha}\right)\\
 & = & \rho\left(\boldsymbol{y}-\boldsymbol{a}\right).
\end{eqnarray*}
\end{rem}
\begin{cor}
The field at large distances is 
\[
\boxed{\boldsymbol{E}\left(\boldsymbol{x}\right)=Q\frac{\boldsymbol{x}}{r^{3}}+\frac{3\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{d}\right)\hat{\boldsymbol{x}}-\boldsymbol{d}}{r^{3}}+O\left(\frac{1}{r^{4}}\right)},
\]
 where $\hat{\boldsymbol{x}}:=\frac{\boldsymbol{x}}{\left|\boldsymbol{x}\right|}$.\end{cor}
\begin{proof}
$\boldsymbol{E}=-\nabla\phi$ and $-\nabla\frac{Q}{r}=Q\frac{\boldsymbol{x}}{r^{3}}$
(see \cref{sub:ch3sec3_2The-Coulomb-potential}),
\begin{eqnarray*}
\nabla\frac{\boldsymbol{d}\cdot\boldsymbol{x}}{r^{3}} & = & \frac{1}{r^{3}}\nabla\left(\boldsymbol{d}\cdot\boldsymbol{x}\right)+\boldsymbol{d}\cdot\boldsymbol{x}\nabla\frac{1}{r^{3}}\\
 & = & \frac{\boldsymbol{d}}{r^{3}}+\boldsymbol{d}\cdot\boldsymbol{x}\left(-\frac{3}{2}\right)\left(\frac{1}{r^{5}}\right)\left(2\boldsymbol{x}\right)\\
 & = & \frac{\boldsymbol{d}}{r^{3}}-\frac{3\left(\boldsymbol{d}\cdot\hat{\boldsymbol{x}}\right)\hat{\boldsymbol{x}}}{r^{3}}.
\end{eqnarray*}
\end{proof}
\begin{rem}
For $Q=0$, the leading contribution to the field falls off as $\frac{1}{r^{3}}$.
\end{rem}
~
\begin{rem}
We can continue the expansion, with the next term being the quadrupole
moment (a rank-2 tensor; see PHYS 611 and Problem \#35). However,
it is advantageous to introduce a more general concept.
\end{rem}

\subsection{\label{sub:ch3sec4_2Legendre-functions-and}Legendre functions and
spherical harmonics}

Note: the proofs in this section are omitted; see math books for proofs.
\begin{defn}
\textbf{\emph{Legendre polynomials.}} The polynomials of degree $l$
defined by
\[
\boxed{P_{l}\left(x\right):=\frac{1}{2^{l}l!}\left(\frac{d}{dx}\right)^{l}\left(x^{2}-1\right)^{l}},\textnormal{ where }l=0,1,2,\dots
\]
 are called \emph{Legendre polynomials}. \end{defn}
\begin{rem}
The first few Legendre polynomials are

$P_{0}\left(x\right)=1$

$P_{1}\left(x\right)=x$

$P_{2}\left(x\right)=\frac{1}{2}\left(3x^{2}-1\right)$
\end{rem}
~
\begin{rem}
The $P_{l}\left(x\right)$ have the following properties $\forall l$:
\end{rem}
\begin{tabular}{ll}
$\left(0\right)\quad P_{l}\left(1\right)=1$ & \tabularnewline
\noalign{\vskip0.2cm}
$\left(\textnormal{i}\right)\quad P_{l}\left(-x\right)=\left(-\right)^{l}P_{l}\left(x\right)$ & \quad{}\emph{parity}\tabularnewline
\noalign{\vskip0.2cm}
$\left(\textnormal{ii}\right)\quad\left(1-x^{2}\right)P_{l}''\left(x\right)-2xP_{l}'\left(x\right)+l\left(l+1\right)P_{l}\left(x\right)=0$ & \quad{}\emph{differential} \emph{equation}\tabularnewline
\noalign{\vskip0.2cm}
$\left(\textnormal{iii}\right)\quad P_{l+1}\left(x\right)=\left(2l+1\right)xP_{l}\left(x\right)-lP_{l-1}\left(x\right)$ & \quad{}\emph{recursion} \emph{relation}\tabularnewline
\noalign{\vskip0.2cm}
$\left(\textnormal{iv}\right)\quad\int_{-1}^{1}dx\:P_{l}\left(x\right)P_{l'}\left(x\right)=\delta_{ll'}\frac{2}{2l+1}$ & \quad{}\emph{orthogonality}\tabularnewline
\noalign{\vskip0.2cm}
\end{tabular}
\begin{rem}
$P_{l}\left(x\right)$ are members of a larger family of orthogonal
polynomials known as the \emph{classical orthogonal polynomials}.\end{rem}
\begin{thm}
\textbf{\emph{\label{thm:Completeness-of-theLegendre}Completeness
of the Legendre polynomials}}

Any piecewise continuous and continuously differentiable function
$f:\left[-1,1\right]\rightarrow\mathbb{R}$ can be expanded in Legendre
polynomials as
\[
\boxed{f\left(x\right)=\summation{l=0}{\infty}f_{l}P_{l}\left(x\right)},
\]
 where 
\[
\underline{f_{l}=\left(\frac{2l+1}{2}\right)\indefint xf\left(x\right)P_{l}\left(x\right)}
\]
 (from orthogonality).\end{thm}
\begin{defn}
\textbf{\emph{Associated Legendre functions.}} The functions (which
are not polynomials now)
\[
\boxed{P_{l}^{m}\left(x\right):=\frac{\left(-\right)^{m}}{2^{l}l!}\left(1-x^{2}\right)^{\frac{m}{2}}\left(\frac{d}{dx}\right)^{l+m}\left(x^{2}-1\right)^{l}}\quad\begin{array}{c}
l=0,1,2,\dots\\
m=-l,-l+1,\dots,l-1,l
\end{array}
\]
 are called \emph{associated Legendre functions.}\end{defn}
\begin{rem}
$P_{l}^{0}\left(x\right)=P_{l}\left(x\right)$ are the Legendre polynomials.
\end{rem}
~
\begin{rem}
For fixed $l$, there are $2l+1$ functions $P_{l}^{m}$.
\end{rem}
~
\begin{rem}
The first few $P_{l}^{m}\left(x\right)$ are 

$P_{0}^{0}\left(x\right)=P_{0}\left(x\right)=1$

$P_{1}^{0}\left(x\right)=P_{1}\left(x\right)=x$

$P_{1}^{1}\left(x\right)=-\sqrt{1-x^{2}}$

$P_{1}^{-1}\left(x\right)=\frac{1}{2}\sqrt{1-x^{2}}$
\end{rem}
~
\begin{rem}
The $P_{l}^{m}$ have the properties:
\end{rem}
\begin{tabular}{ll}
$\left(\textnormal{i}\right)\quad P_{l}^{m}\left(\pm1\right)=0$ & \quad{}\emph{zeroes}\tabularnewline
\noalign{\vskip0.2cm}
$\left(\textnormal{ii}\right)\quad P_{l}^{-m}\left(x\right)=\left(-\right)^{m}\frac{\left(l-m\right)!}{\left(l+m\right)!}P_{l}^{m}\left(x\right)$ & \quad{}\emph{$m-$symmetry}\tabularnewline
\noalign{\vskip0.2cm}
$\left(\textnormal{iii}\right)\quad\frac{d}{dx}\left[\left(1-x^{2}\right)\frac{d}{dx}P_{l}^{m}\left(x\right)\right]+\left[l\left(l+1\right)-\frac{m^{2}}{1-x^{2}}\right]P_{l}^{m}\left(x\right)=0$ & \quad{}\emph{differential equation}\tabularnewline
\noalign{\vskip0.2cm}
$\left(\textnormal{iv}\right)\quad\int_{-1}^{1}dx\:\understuff{P_{l}^{m}\left(x\right)P_{l'}^{m}\left(x\right)}{\textnormal{same }m}=\delta_{ll'}\frac{2}{2l+1}\frac{\left(l+m\right)!}{\left(l-m\right)!}$ & \quad{}\emph{orthogonality}\tabularnewline
\noalign{\vskip0.2cm}
\end{tabular}
\begin{defn}
\textbf{\emph{Spherical harmonics.}} Consider a unit sphere. Let $\Omega=\left(\theta,\varphi\right)$
be a point on the sphere, and let $\eta=\cos\theta$ $\left(-1\leq\eta\leq1\right)$.
The $\mathbb{C}-$valued functions defined on the sphere by
\[
\boxed{Y_{lm}\left(\Omega\right)=\left[\frac{\left(2l+1\right)\left(l-m\right)!}{4\pi\left(l+m\right)!}\right]^{\frac{1}{2}}e^{im\varphi}P_{l}^{m}\left(\eta\right)}
\]
 are called \emph{spherical harmonics}. \end{defn}
\begin{rem}
Different books define the normalization differently!
\end{rem}
~
\begin{rem}
The first few spherical harmonics are

$Y_{00}\left(\Omega\right)=\frac{1}{\sqrt{4\pi}}$

$Y_{10}\left(\Omega\right)=\sqrt{\frac{3}{4\pi}}\cos\theta$

$Y_{1,\pm1}\left(\Omega\right)=\mp\sqrt{\frac{3}{8\pi}}e^{\pm i\varphi}\sin\theta$
\end{rem}
~
\begin{rem}
\label{rem:9_spherical_harmonic_propertiesThe--have}The $Y_{lm}$
have the properties:\footnote{The symbol $\Lambda$ represents the angular part of the Laplacian
$\nabla^{2}$ in spherical coordinates:

$\Lambda:=\frac{1}{\sin\theta}\frac{\partial}{\partial\theta}\sin\theta\frac{\partial}{\partial\theta}+\frac{1}{\sin^{2}\theta}\frac{\partial^{2}}{\partial\varphi^{2}}=\frac{\partial}{\partial\eta}\left(1-\eta^{2}\right)\frac{\partial}{\partial\eta}+\frac{1}{1-\eta^{2}}\frac{\partial^{2}}{\partial\varphi^{2}}$, 

where we have defined $\eta:=\cos\theta$.}
\end{rem}
\begin{tabular}{ll}
$\left(\textnormal{i}\right)\quad Y_{lm}^{*}\left(\Omega\right)=\left(-\right)^{m}Y_{l,-m}\left(\Omega\right)$ & \quad{}\emph{complex conjugate}\tabularnewline
\noalign{\vskip0.2cm}
$\left(\textnormal{ii}\right)\quad-i\frac{\partial}{\partial\varphi}Y_{lm}\left(\Omega\right)=mY_{lm}\left(\Omega\right)$ & \multirow{2}{*}{$\Biggr\}$\hspace{0.25em}\emph{differential equations}}\tabularnewline
\noalign{\vskip0.2cm}
$\hspace{2.5em}\Lambda Y_{lm}\left(\Omega\right)=-l\left(l+1\right)Y_{lm}\left(\Omega\right)$ & \tabularnewline
\noalign{\vskip0.2cm}
$\left(\textnormal{iii}\right)\quad\indefint{\Omega}Y_{lm}^{*}\left(\Omega\right)Y_{l'm'}\left(\Omega\right)=\delta_{ll'}\delta_{mm'}$ & \quad{}\emph{orthogonality}\tabularnewline
\noalign{\vskip0.2cm}
\end{tabular}
\begin{thm}
\textbf{\emph{\label{thm:Completeness-of-spherical}Completeness of
spherical harmonics}}

Any piecewise-continuous and continuously differentiable function
on the sphere, $f\left(\Omega\right)$, can be expanded in spherical
harmonics:
\[
\boxed{f\left(\Omega\right)=\summation{l,m}{}f_{lm}Y_{lm}\left(\Omega\right)},
\]
 where the coefficients are given by
\[
\underline{f_{lm}=\indefint{\Omega}f\left(\Omega\right)Y_{lm}^{*}\left(\Omega\right)}
\]
\end{thm}
\begin{rem}
This is often referred to by saying ``the $Y_{lm}$ form a complete
set on the sphere.''\end{rem}
\begin{prop}
\textbf{\emph{\label{prop:Addition-theorem}Addition theorem}}

Let $\Omega=\left(\theta,\varphi\right)$, $\Omega'=\left(\theta',\varphi'\right)$,
and let $\gamma$ be the angle between the two points:
\[
\cos\gamma=\cos\theta\cos\theta'+\sin\theta\sin\theta'\cos\left(\varphi-\varphi'\right).
\]
 Then
\[
\boxed{P_{l}\left(\cos\gamma\right)=\frac{4\pi}{2l+1}\summation{m=-l}lY_{lm}^{*}\left(\Omega'\right)Y_{lm}\left(\Omega\right)}.
\]
\end{prop}
\begin{cor}
\textbf{\emph{Sum rule}}

For $\gamma=0$, we have $\Omega=\Omega'$ and $P_{l}\left(1\right)=1=\frac{4\pi}{2l+1}\summation{m=-l}lY_{lm}^{*}\left(\Omega\right)Y_{lm}\left(\Omega\right)$.
\[
\implies\boxed{\summation{m=-l}l\left|Y_{lm}\left(\Omega\right)\right|^{2}=\frac{2l+1}{4\pi}}.
\]

\end{cor}

\subsection{\label{sub:ch3sec4_3Separation-of-the}Separation of the Laplace
operator in spherical coordinates}

Consider the Laplace operator:
\[
\nabla^{2}=:\Delta=\frac{1}{r}\frac{\partial^{2}}{\partial r^{2}}r+\frac{1}{r^{2}}\Lambda,
\]
 with
\[
\Lambda:=\frac{1}{\sin\theta}\frac{\partial}{\partial\theta}\sin\theta\frac{\partial}{\partial\theta}+\frac{1}{\sin^{2}\theta}\frac{\partial^{2}}{\partial\varphi^{2}}
\]
 from \cref{sub:ch3sec4_2Legendre-functions-and} \cref{rem:9_spherical_harmonic_propertiesThe--have}.
\[
\implies\nabla^{2}f\left(r,\theta,\varphi\right)=\left(\frac{1}{r}\frac{\partial^{2}}{\partial r^{2}}r+\frac{1}{r^{2}}\Lambda\right)f\left(r,\theta\varphi\right)\underset{\begin{array}{c}
\textnormal{acts on }\\
r\textnormal{ only}
\end{array}}{=\underbrace{\frac{1}{r}\partial_{r}^{2}r}f}+\underset{\begin{array}{c}
\textnormal{acts on }\\
\theta,\varphi\\
\textnormal{only}
\end{array}}{\underbrace{\frac{1}{r^{2}}\Lambda}f}
\]

\begin{thm}
\label{thm:Spherical-differential-equation}The differential equation
for the function $\psi\left(\boldsymbol{x}\right)$
\[
\boxed{\left[-\nabla^{2}+V\left(r\right)\right]\psi\left(r,\theta,\varphi\right)=a\left(r,\theta,\varphi\right)}\tag{*}
\]
 is solved by
\[
\boxed{\psi\left(r,\theta,\varphi\right)=\frac{1}{r}\summation{l,m}{}u_{lm}\left(r\right)Y_{lm}\left(\Omega\right)},
\]
 where $u_{lm}\left(r\right)$ is the solution of the ODE
\[
\boxed{\left(-\frac{d^{2}}{dr^{2}}+V_{l}\left(r\right)\right)u_{lm}\left(r\right)=ra_{lm}\left(r\right)}\tag{**}
\]
 with 
\[
\underline{V_{l}\left(r\right):=V\left(r\right)+\frac{l\left(l+1\right)}{r^{2}}}
\]
 and
\[
\underline{a_{lm}\left(r\right)=\indefint{\Omega}a\left(r,\theta,\varphi\right)Y_{lm}^{*}\left(\Omega\right)}\tag{\ensuremath{\dagger}}
\]
\end{thm}
\begin{rem}
The Poisson Equation has the form $\left(*\right)$.
\end{rem}
~
\begin{rem}
This theorem is also very useful in Quantum Mechanics.\end{rem}
\begin{proof}
\emph{(Of \cref{thm:Spherical-differential-equation})}

\textbf{ansatz:} $\psi\left(r,\theta,\varphi\right)=\frac{1}{r}\summation{l,m}{}u_{lm}\left(r\right)Y_{lm}\left(\Omega\right)$
\begin{eqnarray*}
\left(*\right)\implies a\left(r,\Omega\right) & = & -\frac{1}{r}\partial_{r}^{2}r\frac{1}{r}\summation{l,m}{}u_{lm}\left(r\right)Y_{lm}\left(\Omega\right)-\frac{1}{r^{2}}\frac{1}{r}\summation{l,m}{}u_{lm}\left(r\right)\understuff{\Lambda Y_{lm}\left(\Omega\right)}{-l\left(l+1\right)Y_{lm}}+V\left(r\right)\frac{1}{r}\summation{l,m}{}u_{lm}\left(r\right)Y_{lm}\left(\Omega\right)\\
 & = & \summation{l,m}{}\left[-\frac{1}{r}\partial_{r}^{2}+\frac{l\left(l+1\right)}{r^{3}}+\frac{V\left(r\right)}{r}\right]u_{lm}\left(r\right)Y_{lm}\left(\Omega\right)
\end{eqnarray*}
 (see \cref{sub:ch3sec4_2Legendre-functions-and}). By \cref{sub:ch3sec4_2Legendre-functions-and}
\cref{thm:Completeness-of-spherical}, any reasonably well behaved
$a\left(r,\Omega\right)$ can be expanded in spherical harmonics:
\[
a\left(r,\Omega\right)=\summation{l,m}{}a_{lm}\left(r\right)Y_{lm}\left(\Omega\right),
\]
 with $a_{lm}\left(r\right)$ given by $\left(\dagger\right)$. Inserting
this into the above equation:
\begin{eqnarray*}
\summation{l,m}{}a_{lm}\left(r\right)Y_{lm}\left(\Omega\right) & = & \summation{l,m}{}\left[-\frac{1}{r}\partial_{r}^{2}+\frac{l\left(l+1\right)}{r^{3}}+\frac{V\left(r\right)}{r}\right]u_{lm}\left(r\right)Y_{lm}\left(\Omega\right),\\
\implies\underline{ra_{lm}\left(r\right)} & = & \left[-\partial_{r}^{2}+\frac{l\left(l+1\right)}{r^{2}}+V\left(r\right)\right]u_{lm}\left(r\right)=\underline{\left(-\frac{d^{2}}{dr^{2}}+V_{l}\left(r\right)\right)u_{lm}\left(r\right)}
\end{eqnarray*}
 which follows from the orthonormality of $Y_{lm}$.
\end{proof}

\subsection{\label{sub:ch3sec4_4Expansion-of-harmonic}Expansion of harmonic
functions in spherical harmonics}

Consider harmonic functions, i.e., solutions of
\[
\boxed{\nabla^{2}\phi\left(\boldsymbol{x}\right)=0},\tag{*}
\]
 and assume that $\phi$ is twice continuously differentiable.
\begin{prop}
The most general solution of $\left(*\right)$ has the form
\[
\boxed{\phi\left(\boldsymbol{x}\right)=\summation{l,m}{}\left[\phi_{lm}^{+}\left(\boldsymbol{x}\right)+\phi_{lm}^{-}\left(\boldsymbol{x}\right)\right]},
\]
 where
\begin{eqnarray*}
\phi_{lm}^{+}\left(\boldsymbol{x}\right) & := & q_{lm}^{+}Y_{lm}\left(\Omega\right)\frac{1}{r^{l+1}}\\
\phi_{lm}^{-}\left(\boldsymbol{x}\right) & := & q_{lm}^{-}Y_{lm}\left(\Omega\right)r^{l}
\end{eqnarray*}
 with constant coefficients $q_{lm}^{\pm}$. \end{prop}
\begin{proof}
Since $\nabla^{2}\phi=0$, we can expand $\phi$ using the theorem
in \cref{sub:ch3sec4_3Separation-of-the} with $V\left(r\right)=0$,
$a\left(\boldsymbol{x}\right)=0$.
\[
\implies\partial_{r}^{2}u_{lm}\left(r\right)=\frac{l\left(l+1\right)}{r^{2}}u_{lm}\left(r\right).
\]


\textbf{ansatz:} $u_{lm}\left(r\right)=r^{n}$.
\[
\implies n\left(n-1\right)=l\left(l+1\right)
\]
\[
\implies n=\begin{cases}
l+1\\
-l
\end{cases}
\]
 The two linearly independent solutions are therefore 
\[
u_{lm}\left(r\right)=\begin{cases}
r^{-l}\\
r^{l+1}
\end{cases}
\]
\[
\implies\phi\left(\boldsymbol{x}\right)=\summation{l,m}{}\left(A\frac{1}{r^{l+1}}+Br^{l}\right)Y_{lm},
\]
 with $A$, $B$ arbitrary constants.\end{proof}
\begin{rem}
$\phi_{lm}^{+}\left(\boldsymbol{x}\rightarrow\boldsymbol{0}\right)\rightarrow\infty\quad\forall l,\qquad\phi_{lm}^{-}\left(\boldsymbol{x}\rightarrow\infty\right)\rightarrow\infty\quad\forall l>0$.

Thus, the only harmonic function that is finite at $r=0$ and $r\rightarrow\infty$
is the constant $l=0$ contribution (see \cref{sub:ch3sec1_2Magnetostatics}
\cref{rem:Ch3sec1_2rem5_From--Remark}, \cref{sub:ch3sec3_1The-general-solution}
\cref{rem:ch3sec3_1_rem2-Remark-}).
\end{rem}

\subsection{\label{sub:ch3sec4_5Multipole-expansion-of}Multipole expansion of
the electrostatic potential}
\begin{lem}
\[
\boxed{\frac{1}{\left|\boldsymbol{x}-\boldsymbol{x}'\right|}=\frac{1}{r_{+}}\summation{l=0}{\infty}\left(\frac{r_{-}}{r_{+}}\right)^{l}\frac{4\pi}{2l+1}\summation{m=-l}lY_{lm}\left(\Omega\right)Y_{lm}^{*}\left(\Omega'\right)},
\]
 where
\begin{eqnarray*}
\boldsymbol{x}=\left(r,\Omega\right), &  & r_{+}=\max\left(r,r'\right),\\
\boldsymbol{x}'=\left(r',\Omega'\right), &  & r_{-}=\min\left(r,r'\right).
\end{eqnarray*}
\end{lem}
\begin{proof}
Let $\cos\gamma=\frac{\boldsymbol{x}\cdot\boldsymbol{x}'}{rr'}$ (that
is, $\gamma$ is the angle between $\boldsymbol{x},\boldsymbol{x}'$).
\[
\implies\left|\boldsymbol{x}-\boldsymbol{x}'\right|=\sqrt{r^{2}-2rr'\cos\gamma+r'^{2}}.
\]


\emph{Case 1:} $\underline{r>r'}$
\begin{eqnarray*}
\implies\frac{1}{\left|\boldsymbol{x}-\boldsymbol{x}'\right|} & \overset{1.}{=} & \frac{1}{r}\left[1-2\frac{r'}{r}\cos\gamma+\left(\frac{r'}{r}\right)^{2}\right]^{-\frac{1}{2}}\\
 & = & \frac{1}{r_{+}}\left[1-2\frac{r_{-}}{r_{+}}\cos\gamma+\left(\frac{r_{-}}{r_{+}}\right)^{2}\right]^{-\frac{1}{2}}\\
 & \overset{2.}{=} & \frac{1}{r_{+}}\summation{l=0}{\infty}f_{l}\left(\frac{r_{-}}{r_{+}}\right)P_{l}\left(\cos\gamma\right)\\
 & \overset{3.}{=} & \frac{1}{r_{+}}\summation{l=0}{\infty}\frac{4\pi}{2l+1}f_{l}\left(\frac{r_{-}}{r_{+}}\right)\summation{m=-l}lY_{lm}^{*}\left(\Omega'\right)Y_{lm}\left(\Omega\right)
\end{eqnarray*}

\begin{enumerate}
\item Note that it is only possible to factor $1/r$ if $r>r'$.
\item From \cref{thm:Completeness-of-theLegendre} in \cref{sub:ch3sec4_2Legendre-functions-and},
we can expand the square root in terms of Legendre polynomials since
$\cos\gamma\in\left[-1,1\right]$.
\item From the \emph{Addition Theorem} in \cref{sub:ch3sec4_2Legendre-functions-and}.
\end{enumerate}
\textbf{Remaining question:} What is $f_{l}\left(\frac{r_{-}}{r_{+}}\right)$? 

From \cref{sub:ch3sec4_4Expansion-of-harmonic}, $\frac{1}{\left|\boldsymbol{x}-\boldsymbol{x}'\right|}$
is harmonic for $r>r'$ since
\[
\nabla_{\boldsymbol{x}}^{2}\frac{1}{\left|\boldsymbol{x}-\boldsymbol{x}'\right|}=\nabla_{\boldsymbol{x}}^{2}\frac{1}{r}=\frac{1}{r}\partial_{r}^{2}r\frac{1}{r}=0.
\]
 Furthermore, $ $$\frac{1}{\left|\boldsymbol{x}-\boldsymbol{x}'\right|}=O\left(\frac{1}{r}\right)$
for $r\rightarrow\infty$. By \cref{sub:ch3sec4_4Expansion-of-harmonic},
we know $\frac{1}{\left|\boldsymbol{x}-\boldsymbol{x}'\right|}$ has
the form $\phi_{lm}^{+}$ since it falls off as $\frac{1}{r}$. Thus,
\[
\frac{1}{r}f_{l}\left(\frac{r'}{r}\right)=\frac{1}{r}\left(\frac{r'}{r}\right)^{l}c_{l}
\]
 for some constant $c_{l}$. For $\gamma=0$,
\[
\frac{1}{\left|\boldsymbol{x}-\boldsymbol{x}'\right|}=\frac{1}{r}\left[1-2\frac{r'}{r}+\left(\frac{r'}{r}\right)^{2}\right]^{-\frac{1}{2}}=\frac{1}{r}\frac{1}{1-\frac{r'}{r}}=\frac{1}{r}\summation{l=0}{\infty}\left(\frac{r'}{r}\right)^{l}=\frac{1}{r}\summation{l=0}{\infty}f_{l}\left(\frac{r'}{r}\right)\understuff{P_{l}\left(1\right)}1,
\]
 where the second to last step is the geometric series. Comparing
the two equations above, $\implies c_{l}=1$. 
\[
\implies\underline{f_{l}\left(\frac{r_{-}}{r_{+}}\right)=\left(\frac{r_{-}}{r_{+}}\right)^{l}}.
\]


\emph{Case 2:} $\underline{r'>r}$ analogous.\end{proof}
\begin{prop}
The electrostatic potential of a localized charge distribution $\rho=\rho\left(\boldsymbol{x}\right)$
(that is, $\rho\left(\boldsymbol{x}\right)=0$ for $\left|\boldsymbol{x}\right|>r_{0}$)
can be written, for $\left|\boldsymbol{x}\right|>r_{0}$,
\[
\boxed{\phi\left(\boldsymbol{x}\right)=\summation{l,m}{}\frac{Q_{lm}}{r^{l+1}}\sqrt{\frac{4\pi}{2l+1}}Y_{lm}\left(\Omega\right)},
\]
 where
\[
\underline{Q_{lm}:=\sqrt{\frac{4\pi}{2l+1}}\int_{0}^{\infty}dr\:r^{2}r^{l}\indefint{\Omega}\rho\left(r,\Omega\right)Y_{lm}^{*}\left(\Omega\right)}
\]
 are the \emph{multipole moments} of the charge distribution.\end{prop}
\begin{proof}
From \cref{sub:ch3sec3_3Poisson's-formula} and inserting the lemma,\footnote{Note that $\left|\boldsymbol{x}\right|:=r$, $\left|\boldsymbol{y}\right|:=y$.}
\begin{eqnarray*}
\phi\left(\boldsymbol{x}\right) & = & \indefint{\boldsymbol{y}}\frac{\rho\left(\boldsymbol{y}\right)}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\\
 & = & \indefint{\boldsymbol{y}}\rho\left(\boldsymbol{y}\right)\frac{1}{r}\summation{l=0}{\infty}\left(\frac{y}{r}\right)^{l}\frac{4\pi}{2l+1}\summation{m=-l}lY_{lm}\left(\Omega_{\boldsymbol{x}}\right)Y_{lm}^{*}\left(\Omega_{\boldsymbol{y}}\right)\\
 & = & \summation{l,m}{}\frac{1}{r^{l+1}}\sqrt{\frac{4\pi}{2l+1}}Y_{lm}\left(\Omega_{\boldsymbol{x}}\right)\understuff{\sqrt{\frac{4\pi}{2l+1}}\indefint yy^{2}y^{l}\indefint{\Omega}\rho\left(y,\Omega_{\boldsymbol{y}}\right)Y_{lm}^{*}\left(\Omega_{\boldsymbol{y}}\right)}{=:Q_{lm}}
\end{eqnarray*}
\end{proof}
\begin{rem}
~

For $l=0$, the moment is
\[
\underline{Q_{00}}=\sqrt{4\pi}\int_{0}^{\infty}dr\:r^{2}\indefint{\Omega}\rho\left(r,\Omega\right)\understuff{\frac{1}{\sqrt{4\pi}}}{Y_{00}^{*}}=\underline{Q},
\]
 the total charge.

For $l=1,$ we have $Q_{1,-1}$, $Q_{10}$, $Q_{11}$:
\[
Q_{1m}=\sqrt{\frac{4\pi}{3}}\int_{0}^{\infty}dr\:r^{3}\indefint{\Omega}\rho\left(r,\Omega\right)\left[\delta_{m0}\cos\theta-\delta_{m1}\frac{1}{\sqrt{2}}e^{-i\varphi}\sin\theta+\delta_{m,-1}\frac{1}{\sqrt{2}}e^{i\varphi}\sin\theta\right]\sqrt{\frac{3}{4\pi}}
\]
\begin{eqnarray*}
\implies\underline{Q_{10}} & = & \int_{0}^{\infty}dr\:r^{2}\indefint{\Omega}\rho\left(r,\Omega\right)r\cos\theta=\int d\boldsymbol{x}x_{3}\rho\left(\boldsymbol{x}\right)=\underline{d_{3}}\\
\implies\underline{Q_{11}} & = & -\frac{1}{\sqrt{2}}\int_{0}^{\infty}dr\:r^{2}\indefint{\Omega}\rho\left(r,\Omega\right)e^{-i\varphi}r\sin\theta\\
 & = & -\frac{1}{\sqrt{2}}\indefint{\boldsymbol{x}}\rho\left(\boldsymbol{x}\right)\left[r\sin\theta\cos\varphi-ir\sin\theta\sin\varphi\right]\\
 & = & -\frac{1}{\sqrt{2}}\indefint{\boldsymbol{x}}\rho\left(\boldsymbol{x}\right)\left[x_{1}-ix_{2}\right]=\underline{-\frac{1}{\sqrt{2}}\left(d_{1}-id_{2}\right)}\\
\implies\underline{Q_{1,-1}} & = & \underline{\frac{1}{\sqrt{2}}\left(d_{1}+id_{2}\right)}
\end{eqnarray*}
\begin{eqnarray*}
\implies\underline{d_{1}} & = & \underline{\frac{1}{\sqrt{2}}\left(Q_{1,-1}-Q_{11}\right)},\\
\underline{d_{2}} & = & \underline{\frac{1}{\sqrt{2}}\left(Q_{1,-1}+Q_{11}\right)}.
\end{eqnarray*}

\end{rem}

\subsection{Multipole expansion of the electrostatic interaction}

Consider a charge density $\rho_{<}\left(\boldsymbol{x}\right)$
confined to a region $R_{<}$ inside a sphere of radius $r_{0}$.
Let $\rho_{<}\left(\boldsymbol{x}\right)$ be subject to a charge
density $\rho_{>}\left(\boldsymbol{y}\right)$ confined to a region
$R_{>}$ outside a sphere radius $R_{0}>r_{0}$.  What is the electrostatic
interaction energy $U$ between these charge distributions?

From \cref{sub:ch3sec3_5Electrostatic-interaction}, 
\begin{eqnarray*}
\underline{U} & = & \frac{1}{2}\indefint{\boldsymbol{x}}d\boldsymbol{y}\:\rho\left(\boldsymbol{x}\right)\rho\left(\boldsymbol{y}\right)\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\\
 & = & \frac{1}{2}\underset{R_{<}}{\int}d\boldsymbol{x}\:\rho_{<}\left(\boldsymbol{x}\right)\underset{R_{>}}{\int}d\boldsymbol{y}\:\rho_{>}\left(\boldsymbol{y}\right)\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}+\frac{1}{2}\underset{R_{>}}{\int}d\boldsymbol{x}\:\rho_{>}\left(\boldsymbol{x}\right)\underset{R_{<}}{\int}d\boldsymbol{y}\:\rho_{<}\left(\boldsymbol{y}\right)\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\\
 & = & \underset{R_{<}}{\int}d\boldsymbol{x}\:\rho_{<}\left(\boldsymbol{x}\right)\underset{R_{>}}{\int}d\boldsymbol{y}\:\rho_{>}\left(\boldsymbol{y}\right)\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\\
 & = & \underline{\underset{R_{<}}{\int}d\boldsymbol{x}\:\rho_{<}\left(\boldsymbol{x}\right)\phi_{>}\left(\boldsymbol{x}\right)},
\end{eqnarray*}
 where 
\[
\phi_{>}\left(\boldsymbol{x}\right)=\underset{R_{>}}{\int}d\boldsymbol{y}\:\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\rho_{>}\left(\boldsymbol{y}\right)
\]
 is the potential generated by the charges in $R_{>}$ at $\boldsymbol{x}$.\footnote{Note that the choice to use $\phi_{>}$ as the source and $\rho_{<}$
as the test charge is arbitrary; we could have written $U$ in terms
of $\rho_{>}$ and $\phi_{<}$.}

If $R_{0}\gg r_{0}$, $\phi_{>}\left(\boldsymbol{x}\right)$ will
vary slowly within $R_{<}$, so we can Taylor expand:
\[
\phi_{>}\left(\boldsymbol{x}\right)=\phi_{>}\left(\boldsymbol{x}=\boldsymbol{0}\right)+\boldsymbol{x}\cdot\left.\nabla\phi_{>}\right|_{\boldsymbol{x}=\boldsymbol{0}}+\frac{1}{2}x_{i}x_{j}\left.\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\phi_{>}\right|_{\boldsymbol{x}=\boldsymbol{0}}+\dots
\]
 From \cref{sub:ch3sec1_1Electrostatics}, $\phi_{>}\left(\boldsymbol{x}\right)$
obeys Laplace's equation $\forall\boldsymbol{x}\in R_{<}$.
\[
\implies\delta_{ij}\left.\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\phi_{>}\right|_{\boldsymbol{x}=\boldsymbol{0}}=0
\]
\[
\implies\phi_{>}\left(\boldsymbol{x}\right)=\phi_{>}\left(\boldsymbol{x}=\boldsymbol{0}\right)+\boldsymbol{x}\cdot\left.\nabla\phi_{>}\right|_{\boldsymbol{x}=\boldsymbol{0}}+\frac{1}{2}\left(x_{i}x_{j}-\frac{\boldsymbol{x}^{2}}{3}\delta_{ij}\right)\left.\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\phi_{>}\right|_{\boldsymbol{x}=\boldsymbol{0}}+\dots
\]
 
\begin{defn}
Define the following:

~

\begin{tabular}{ll}
$\phi_{0}:=\phi_{>}\left(\boldsymbol{x}=\boldsymbol{0}\right)$ & $\dots\textnormal{the potential \ensuremath{\phi_{>}\:}at the origin}$\tabularnewline
\noalign{\vskip0.25cm}
$\boldsymbol{E}:=-\nabla\phi_{>}\left(\boldsymbol{x}=\boldsymbol{0}\right)$ & $\dots\textnormal{the field due to \ensuremath{\phi_{>}\:}at the origin}$\tabularnewline
\noalign{\vskip0.25cm}
$\phi_{ij}:=\frac{\partial^{2}}{\partial x_{i}\partial x_{j}}\phi_{>}\left(\boldsymbol{x}=\boldsymbol{0}\right)$ & $\dots\textnormal{the field gradients of \ensuremath{\phi_{>}\:}at the origin}$\tabularnewline
\noalign{\vskip0.25cm}
\end{tabular}
\end{defn}
\[
\implies\phi_{>}\left(\boldsymbol{x}\right)=\phi_{0}-\boldsymbol{x}\cdot\boldsymbol{E}+\frac{1}{2}\left(x_{i}x_{j}-\frac{1}{3}\boldsymbol{x}^{2}\delta_{ij}\right)\phi_{ij}+\dots
\]
 Now drop the subscripts, and denote $\rho:=\rho_{<}$, $\phi:=\phi_{>}$.
\begin{eqnarray*}
\implies U & = & \indefint{\boldsymbol{x}}\rho\left(\boldsymbol{x}\right)\phi\left(\boldsymbol{x}\right)\\
 & = & \phi_{0}\indefint{\boldsymbol{x}}\rho\left(\boldsymbol{x}\right)-\boldsymbol{E}\cdot\indefint{\boldsymbol{x}}\boldsymbol{x}\rho\left(\boldsymbol{x}\right)+\frac{1}{3}\phi_{ij}\frac{1}{2}\indefint{\boldsymbol{x}}\rho\left(\boldsymbol{x}\right)\left(3x_{i}x_{j}-\boldsymbol{x}^{2}\delta_{ij}\right)+\dots
\end{eqnarray*}
\[
\implies\boxed{U=\phi_{0}Q-\boldsymbol{E}\cdot\boldsymbol{d}+\frac{1}{3}\phi^{ij}Q_{ij}+\dots}
\]
where

~

\begin{tabular}{ll}
$\phi_{0}$, $\boldsymbol{E}$, $\phi^{ij}$ & ...are the potential, electric field, and field gradient tensor due
to $\rho>$ evaluated at the origin.\tabularnewline
\noalign{\vskip0.25cm}
$Q$, $\boldsymbol{d}$, $Q_{ij}$  & ...are the total charge, dipole moment, and quadrupole moments of
$\rho_{<}$.\tabularnewline
\noalign{\vskip0.25cm}
\end{tabular}
\begin{rem}
Alternatively, we can use the spherical harmonic expansion from \cref{sub:ch3sec4_4Expansion-of-harmonic}
to expand the potential:
\[
\phi\left(\boldsymbol{x}\right)=\summation{l,m}{}\phi_{lm}^{-}\left(\boldsymbol{x}\right)=\summation{l,m}{}q_{lm}^{-}r^{l}Y_{lm}\left(\Omega\right),
\]
 where we have disregarded the $\phi_{lm}^{+}$ terms because they
blow up at the origin.
\begin{eqnarray*}
\implies U & = & \indefint{\boldsymbol{x}}\rho\left(\boldsymbol{x}\right)\phi\left(\boldsymbol{x}\right)\\
 & = & \indefint{\boldsymbol{x}}\rho\left(\boldsymbol{x}\right)\summation{l,m}{}q_{lm}^{-}r^{l}Y_{lm}\left(\Omega\right)\\
 & = & \summation{l,m}{}q_{lm}^{-}\understuff{\indefint rr^{2}r^{l}\indefint{\Omega}\rho\left(r,\Omega\right)Y_{lm}\left(\Omega\right)}{=\sqrt{\frac{2l+1}{4\pi}}Q_{lm}^{*}}
\end{eqnarray*}
\[
\implies\boxed{U=\summation{l,m}{}q_{lm}^{-}\sqrt{\frac{2l+1}{4\pi}}Q_{lm}^{*}},
\]
 where, as per \cref{sub:ch3sec4_5Multipole-expansion-of}, $Q_{lm}$
are the multipole moments of the charge density $\rho\left(\boldsymbol{x}\right):=\rho_{<}\left(\boldsymbol{x}\right)$
and the $q_{lm}^{-}$ are the coefficients of the expansion of the
harmonic function $\phi\left(\boldsymbol{x}\right):=\phi_{>}\left(\boldsymbol{x}\right)$
in spherical harmonics.

Which one is used depends on the symmetry of the problem.
\end{rem}

\subsection{\label{sub:ch3sec4_7The-magnetic-moment}The magnetic moment}

From \cref{sub:ch3sec3_6The-law-of}, the Biot \& Savart law gives
the magnetic field resulting from a stationary current density. This
requires an interpretation, as currents are produced by moving charges
and hence are intrinsically time dependent.
\begin{defn}
\textbf{\emph{Stationary current density.}} By \emph{stationary current
density} $\boldsymbol{j}\left(\boldsymbol{x}\right)$, we mean the
time average taken over a time $T$ large compared to all microscopic
time-scales:
\[
\boldsymbol{j}\left(\boldsymbol{x}\right)=\overline{\boldsymbol{j}\left(\boldsymbol{x},t\right)}:=\frac{1}{T}\int_{0}^{T}dt\:\boldsymbol{j}\left(\boldsymbol{x},t\right)
\]
\end{defn}
\begin{example}
\textbf{\emph{Current in a wire loop.}}

$T$ must be much larger than the time it takes an electron to complete
one revolution.\end{example}
\begin{rem}
\label{rem:remark1_stationary_m4}With this definition, M4 reduces
to its static version upon time averaging, provided the electric field
$\boldsymbol{E}$ as a function of time is bounded. That is,
\[
\overline{\frac{\partial\boldsymbol{E}\left(\boldsymbol{x},t\right)}{\partial t}}=\frac{1}{T}\int_{0}^{T}dt\:\frac{\partial\boldsymbol{E}}{\partial t}=\frac{1}{T}\left[\boldsymbol{E}\left(\boldsymbol{x},T\right)-\boldsymbol{E}\left(\boldsymbol{x},0\right)\right]\overset{T\rightarrow\infty}{\longrightarrow}\boldsymbol{0},
\]
 if $\boldsymbol{E}\left(\boldsymbol{x},t\right)$ is bounded.
\[
\left(\textnormal{M4}\right)\implies\understuff{\overline{-\frac{1}{c}\partial_{t}\boldsymbol{E}}}0+\overline{\nabla\times\boldsymbol{B}}=\boxed{\overline{\nabla\times\boldsymbol{B}}=\overline{\frac{4\pi}{c}\boldsymbol{j}}}.
\]

\end{rem}
Now consider the time averaged vector potential $\boldsymbol{A}\left(\boldsymbol{x}\right):=\overline{\boldsymbol{A}\left(\boldsymbol{x},t\right)}$
at large distances from a localized static current density given by
\[
\boldsymbol{j}\left(\boldsymbol{y}\right)=\summation{\alpha}{}\overline{e_{\alpha}\boldsymbol{v}_{\alpha}\delta\left(\boldsymbol{y}-\boldsymbol{x}_{\alpha}\right)}.
\]
 From \cref{sub:ch3sec3_6The-law-of}, the Biot \& Savart law gives
\begin{eqnarray*}
\underline{\boldsymbol{A}\left(\boldsymbol{x}\right)}=\frac{1}{c}\indefint{\boldsymbol{y}}\frac{\boldsymbol{j}\left(\boldsymbol{y}\right)}{\left|\boldsymbol{x}-\boldsymbol{y}\right|} & = & \frac{1}{c}\summation{\alpha}{}\overline{\frac{e_{\alpha}\boldsymbol{v}_{\alpha}}{\left|\boldsymbol{x}-\boldsymbol{x}_{\alpha}\right|}}\\
 & \overset{1.}{=} & \frac{1}{c}\summation{\alpha}{}\overline{e_{\alpha}\boldsymbol{v}_{\alpha}\frac{1}{r}\left[1+\frac{\boldsymbol{x}\cdot\boldsymbol{x}_{\alpha}}{r^{2}}+\dots\right]}\\
 & \overset{2.}{\approx} & \underline{\frac{1}{c}\frac{1}{r^{3}}\summation{\alpha}{}e_{\alpha}\overline{\boldsymbol{v}_{\alpha}\left(\boldsymbol{x}\cdot\boldsymbol{x}_{\alpha}\right)}}
\end{eqnarray*}

\begin{enumerate}
\item Expanding $1/\left|\boldsymbol{x}-\boldsymbol{x}_{\alpha}\right|$
as per \cref{sub:ch3sec4_1The-electric-dipole}.
\item The monopole contribution is zero by \cref{rem:remark1_stationary_m4}:
\[
\summation{\alpha}{}\overline{e_{\alpha}\boldsymbol{v}_{\alpha}}=\overline{\frac{d}{dt}\summation{\alpha}{}e_{\alpha}\boldsymbol{x}_{\alpha}}=0
\]
 since a static current density is assumed to be bounded. Here we
also drop higher order terms.
\end{enumerate}
We can rewrite the dipole term as follows:
\begin{eqnarray*}
\summation{\alpha}{}e_{\alpha}\boldsymbol{v}_{\alpha}\left(\boldsymbol{x}_{\alpha}\cdot\boldsymbol{x}\right) & = & \summation{\alpha}{}e_{\alpha}\dot{\boldsymbol{x}}_{\alpha}\left(\boldsymbol{x}_{\alpha}\cdot\boldsymbol{x}\right)\\
 & = & \frac{1}{2}\frac{d}{dt}\summation{\alpha}{}e_{\alpha}\boldsymbol{x}_{\alpha}\left(\boldsymbol{x}_{\alpha}\cdot\boldsymbol{x}\right)+\frac{1}{2}\summation{\alpha}{}e_{\alpha}\left[\boldsymbol{v}_{\alpha}\left(\boldsymbol{x}_{\alpha}\cdot\boldsymbol{x}\right)-\boldsymbol{x}_{\alpha}\left(\boldsymbol{v}_{\alpha}\cdot\boldsymbol{x}\right)\right],
\end{eqnarray*}
 by the product rule. Taking the time average,
\begin{eqnarray*}
\implies\underline{\summation{\alpha}{}e_{\alpha}\overline{\boldsymbol{v}_{\alpha}\left(\boldsymbol{x}_{\alpha}\cdot\boldsymbol{x}\right)}} & = & \frac{1}{2}\summation{\alpha}{}e_{\alpha}\understuff{\overline{\frac{d}{dt}\boldsymbol{x}_{\alpha}\left(\boldsymbol{x}_{\alpha}\cdot\boldsymbol{x}\right)}}{0\textnormal{ (bounded)}}+\frac{1}{2}\summation{\alpha}{}e_{\alpha}\left[\overline{\boldsymbol{v}_{\alpha}\left(\boldsymbol{x}_{\alpha}\cdot\boldsymbol{x}\right)}-\overline{\boldsymbol{x}_{\alpha}\left(\boldsymbol{v}_{\alpha}\cdot\boldsymbol{x}\right)}\right]\\
 & = & \underline{\frac{1}{2}\summation{\alpha}{}e_{\alpha}\left[\overline{\boldsymbol{v}_{\alpha}\left(\boldsymbol{x}_{\alpha}\cdot\boldsymbol{x}\right)}-\overline{\boldsymbol{x}_{\alpha}\left(\boldsymbol{v}_{\alpha}\cdot\boldsymbol{x}\right)}\right]}
\end{eqnarray*}
\[
\implies\underline{\boldsymbol{A}\left(\boldsymbol{x}\right)}=\underline{\frac{1}{2c}\frac{1}{r^{3}}\summation{\alpha}{}e_{\alpha}\left[\overline{\boldsymbol{v}_{\alpha}\left(\boldsymbol{x}_{\alpha}\cdot\boldsymbol{x}\right)}-\overline{\boldsymbol{x}_{\alpha}\left(\boldsymbol{v}_{\alpha}\cdot\boldsymbol{x}\right)}\right]}.
\]

\begin{defn}
\textbf{\emph{Magnetic moment.}} The \emph{magnetic moment }of the
charges is defined as
\[
\boxed{\boldsymbol{m}:=\frac{1}{2c}\summation{\alpha}{}e_{\alpha}\overline{\left(\boldsymbol{x}_{\alpha}\times\boldsymbol{v}_{\alpha}\right)}}.
\]
\end{defn}
\begin{prop}
The vector potential for large distances from the current density
is given by the magnetic moment via
\[
\boxed{\boldsymbol{A}\left(\boldsymbol{x}\right)=\frac{1}{r^{3}}\boldsymbol{m}\times\boldsymbol{x}}+O\left(\frac{1}{r^{4}}\right).
\]
\end{prop}
\begin{proof}
\begin{eqnarray*}
\boldsymbol{m}\times\boldsymbol{x} & = & \frac{1}{2c}\summation{\alpha}{}e_{\alpha}\overline{\left(\boldsymbol{x}_{\alpha}\times\boldsymbol{v}_{\alpha}\right)}\times\boldsymbol{x}\\
 & = & \frac{1}{2c}\summation{\alpha}{}e_{\alpha}\left[\overline{\boldsymbol{v}_{\alpha}\left(\boldsymbol{x}_{\alpha}\cdot\boldsymbol{x}\right)}-\overline{\boldsymbol{x}\left(\boldsymbol{x}_{\alpha}\cdot\boldsymbol{v}_{\alpha}\right)}\right]=r^{3}\boldsymbol{A}.
\end{eqnarray*}
\end{proof}
\begin{cor}
The magnetic field for large distances from the current density is
\[
\boxed{\boldsymbol{B}\left(\boldsymbol{x}\right)=\frac{3\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{m}\right)\hat{\boldsymbol{x}}-\boldsymbol{m}}{r^{3}}}+O\left(\frac{1}{r^{4}}\right).
\]
\end{cor}
\begin{proof}
Analogous to \cref{sub:ch3sec4_1The-electric-dipole}.\end{proof}
\begin{prop}
If all of the moving charges have the same charge-to-mass ratio $\frac{e_{\alpha}}{m_{\alpha}}=:\frac{e}{m}$,
and if the motion is non-relativistic $\left(v_{\alpha}\ll c\right)$,
then the magnetic moment is proportional to the angular momentum of
the system:
\[
\boxed{\boldsymbol{m}=\frac{e}{2mc}\boldsymbol{L}}.\tag{*}
\]
\end{prop}
\begin{proof}
$\boldsymbol{L}:=\summation{\alpha}{}\boldsymbol{x}_{\alpha}\times\boldsymbol{p}_{\alpha}=\summation{\alpha}{}m_{\alpha}\boldsymbol{x}_{\alpha}\times\boldsymbol{v}_{\alpha}.$
\[
\implies\underline{\boldsymbol{m}}:=\frac{1}{2c}\summation{\alpha}{}e_{\alpha}\overline{\left(\boldsymbol{x}_{\alpha}\times\boldsymbol{v}_{\alpha}\right)}=\frac{1}{2c}\summation{\alpha}{}\frac{e_{\alpha}}{m_{\alpha}}m_{\alpha}\overline{\left(\boldsymbol{x}_{\alpha}\times\boldsymbol{v}_{\alpha}\right)}=\underline{\frac{e}{2mc}\boldsymbol{L}}.
\]
\end{proof}
\begin{rem}
The proportionality factor $\frac{e}{2mc}$ is called \emph{gyromagnetic
ratio}.
\end{rem}
~
\begin{rem}
$\left(*\right)$ holds for orbital momentum $\boldsymbol{L}$ of
particles, but \emph{not }for the magnetic moment related to spin.
For electrons,
\[
\boldsymbol{m}_{e}=\frac{ge}{2mc}\boldsymbol{S}_{e},
\]
 with $S_{e}=\frac{1}{2}\hbar$ the spin of the electron, $m$ the
electron mass, and $g=2.0023\cdots$ the so-called \emph{g-factor}.
\end{rem}
~
\begin{rem}
The g-factor was a mystery until the development of the Dirac equation,
which predicts $g=2$. The rest is accounted for by loop corrections
in QED.
\end{rem}

\chapter{\label{chap:Electromagnetic-waves-in}Electromagnetic waves in vacuum}


\section{Plane electromagnetic waves}


\subsection{\label{sub:ch4sec1_1The-wave-equation}The wave equation}

Consider vacuum: $J^{\mu}\left(x\right)=0$ everywhere.
\begin{rem}
Any solutions to Maxwell's Equations must be time-dependent since
in \cref{chap:Static-solutions-of} \cref{sub:ch3sec1_1Electrostatics},
\cref{sub:ch3sec1_2Magnetostatics}, we saw that in vacuum, static
potentials obey Laplace's equation, which has only the trivial (zero)
solution.\end{rem}
\begin{thm}
\textbf{\emph{Wave equation.}} In vacuum (and with the Lorentz gauge),
the 4-vector potential $A^{\mu}\left(x\right)$ obeys 
\[
\boxed{\partial_{\nu}\partial^{\nu}A^{\mu}\left(x\right)=0}.\tag{*}
\]
\end{thm}
\begin{proof}
From \cref{chap:Maxwell's-Equations} \cref{sub:ch2sec1_3The-field-equations},
\begin{eqnarray*}
\partial_{\mu}F^{\mu\nu} & = & \frac{4\pi}{c}J^{\nu}\overset{1.}{=}\underline{0}\\
 & = & \partial_{\mu}\left(\partial^{\mu}A^{\nu}-\partial^{\nu}A^{\mu}\right)\\
 & \overset{2.}{=} & \partial_{\mu}\partial^{\mu}A^{\nu}-\partial^{\nu}\understuff{\partial_{\mu}A^{\mu}}0\\
 & = & \underline{\partial_{\mu}\partial^{\mu}A^{\nu}}
\end{eqnarray*}

\begin{enumerate}
\item We are considering vacuum.
\item In Lorentz gauge, $\partial_{\mu}A^{\mu}=0$.
\end{enumerate}
\end{proof}
\begin{rem}
$\left(*\right)$ is called \emph{wave equation}.
\end{rem}
~
\begin{rem}
The operator 
\[
\boxed{\square:=\partial_{\nu}\partial^{\nu}}
\]
is called \emph{d'Alembert operator}. Explicitly,
\[
\underline{\partial_{\nu}\partial^{\nu}}=g^{\mu\nu}\partial_{\nu}\partial_{\mu}=g^{\mu\nu}\frac{\partial^{2}}{\partial x^{\mu}\partial x^{\nu}}=\underline{\frac{1}{c^{2}}\partial_{t}^{2}-\nabla^{2}}.
\]
Some books define it as the negative of this. 
\end{rem}
~
\begin{rem}
The Lorentz gauge implies a Lorentz invariant relation between $\phi$
and $\boldsymbol{A}$:
\[
\underline{0}=\partial_{\mu}A^{\mu}=\frac{\partial}{\partial x^{\mu}}A^{\mu}=\underline{\frac{1}{c}\partial_{t}\phi+\nabla\cdot\boldsymbol{A}}.
\]
\end{rem}
\begin{cor}
The electric and magnetic fields also obey the wave equation:
\[
\boxed{\square\boldsymbol{E}=\square\boldsymbol{B}=0}.\tag{**}
\]
\end{cor}
\begin{proof}
From \cref{chap:Maxwell's-Equations} \cref{sub:ch2sec3_4Relations-between-fields},
\begin{eqnarray*}
\underline{\square\boldsymbol{B}} & = & \square\left(\nabla\times\boldsymbol{A}\right)=\nabla\times\left(\square\boldsymbol{A}\right)=\underline{\boldsymbol{0}}\\
\underline{\square\boldsymbol{E}} & = & \square\left(-\nabla\phi-\frac{1}{c}\partial_{t}\boldsymbol{A}\right)=-\nabla\left(\square\phi\right)-\frac{1}{c}\partial_{t}\left(\square\boldsymbol{A}\right)=\underline{\boldsymbol{0}}
\end{eqnarray*}
\end{proof}
\begin{rem}
The Lorentz gauge still does not determine the potentials uniquely;
in vacuum, one can always choose a gauge such that
\[
\phi=0\implies\nabla\cdot\boldsymbol{A}=0
\]
 (see Problem \#39). However, this choice is \emph{not} Lorentz invariant.
\end{rem}

\subsection{\label{sub:ch4sec1_2Plane-waves}Plane waves}
\begin{defn}
\textbf{\emph{Plane waves.}} Solutions of the wave equation that depend
on only one spacial coordinate are called \emph{plane waves}.
\end{defn}
Let $f\left(x,t\right)$ be any component of $\boldsymbol{E}$ or
$\boldsymbol{B}$ or $A^{\mu}$. From \cref{sub:ch4sec1_1The-wave-equation}
$\left(*\right)$ or $\left(**\right)$, 
\[
\boxed{\left(\partial_{t}^{2}-c^{2}\partial_{x}^{2}\right)f\left(x,t\right)=0}.\tag{*}
\]
 This is called the \emph{plane wave equation} or \emph{1D wave equation}.
\begin{thm}
\textbf{\emph{d'Alembert solution.}} The most general solution of
$\left(*\right)$ is 
\[
\boxed{f\left(x,t\right)=f_{1}\left(x-ct\right)+f_{2}\left(x+ct\right)},
\]
where $f_{1}$, $f_{2}$ are arbitrary twice continuously differentiable
functions of their arguments.\end{thm}
\begin{proof}
We can write $\left(*\right)$ as 
\[
\underline{\left(\frac{1}{c}\partial_{t}-\partial_{x}\right)\left(\frac{1}{c}\partial_{t}+\partial_{x}\right)f\left(x,t\right)=0}.\tag{\ensuremath{\dagger}}
\]
 Define $\xi:=x-ct$, $\eta:=x+ct$. $\implies x=\frac{1}{2}\left(\xi+\eta\right)$,
$t=-\frac{1}{2c}\left(\xi-\eta\right)$. Also define $\psi\left(\xi,\eta\right):=f\left(x,t\right)$.
\begin{eqnarray*}
\implies\underline{\frac{1}{c}\partial_{t}f} & \overset{1.}{=} & \left(\partial_{\xi}\psi\right)\frac{1}{c}\partial_{t}\xi+\left(\partial_{\eta}\psi\right)\frac{1}{c}\partial_{t}\eta\\
 & = & \underline{-\partial_{\xi}\psi+\partial_{\eta}\psi},\\
\underline{\partial_{x}f} & = & \left(\partial_{\xi}\psi\right)\partial_{x}\xi+\left(\partial_{\eta}\psi\right)\partial_{x}\eta\\
 & = & \underline{\partial_{\xi}\psi+\partial_{\eta}\psi}.
\end{eqnarray*}

\begin{enumerate}
\item Inserting $f\left(x,t\right)=:\psi\left(\xi,\eta\right)$ and using
the chain rule.
\end{enumerate}
Inserting these relations into $\left(\dagger\right)$, we see
\[
0=-2\partial_{\xi}2\understuff{\partial_{\eta}\psi\left(\xi,\eta\right)}{=:a\left(\eta\right)}.
\]
 The bracketed term must not be a function of $\xi$ since, after
a $\xi-$derivative, the result is $0$. Integrate:
\[
\implies\psi\left(\xi,\eta\right)=\int_{\eta_{0}}^{\eta}d\tilde{\eta}\:a\left(\tilde{\eta}\right)+b\left(\xi\right).
\]
 Note that both terms above are arbitrary functions. Let $f_{1}\left(\xi\right):=b\left(\xi\right)$,
$f_{2}\left(\eta\right):=\int_{\eta_{0}}^{\eta}d\tilde{\eta}\:a\left(\tilde{\eta}\right)$.
\[
\implies\psi\left(\xi,\eta\right)=f_{1}\left(\xi\right)+f_{2}\left(\eta\right)=f_{1}\left(x-ct\right)+f_{2}\left(x+ct\right)=f\left(x,t\right).
\]
\end{proof}
\begin{rem}
PDEs in general have \emph{whole classes }of functions as their solutions,
in contrast to ODEs.
\end{rem}
~
\begin{rem}
(Gotta insert this figure)

$f_{1}$ moves in the $+x$ direction with velocity $c$,

$f_{2}$ moves in the $-x$ direction with velocity $c$.

$f$ is a superposition of $f_{1}$, $f_{2}$. 
\end{rem}

\subsection{\label{sub:ch4sec1_3Orientation-of-the}Orientation of the fields}
\begin{prop}
Consider a plane electromagnetic wave propagating in some direction
$\hat{\boldsymbol{n}}$. Then $\boldsymbol{E}$, $\boldsymbol{B}$,
$\hat{\boldsymbol{n}}$ are mutually perpendicular, and
\[
\boxed{\boldsymbol{B}=\hat{\boldsymbol{n}}\times\boldsymbol{E}}.
\]
\end{prop}
\begin{proof}
By Problem \#39, in vacuum we can always choose a gauge such that
\[
\nabla\cdot\boldsymbol{A}=0\textnormal{ and }\phi=0.
\]
 Let $\hat{\boldsymbol{n}}=\left(1,0,0\right)$. $\implies\boldsymbol{A}\left(\boldsymbol{x},t\right)=\boldsymbol{A}\left(x,t\right)$,
 and have the wave travel in the $+x$ direction.
\[
\implies\boldsymbol{A}\left(x,t\right)=\boldsymbol{A}\left(x-ct\right)=\boldsymbol{A}\left(t-\frac{x}{c}\right)=\boldsymbol{A}\left(u\right),
\]
 where $u:=t-\frac{x}{c}$. Now, $\nabla\cdot\boldsymbol{A}=0\implies\partial_{x}A_{x}=0$.
We also know
\begin{eqnarray*}
\square\boldsymbol{A}=0 & \implies & \partial_{t}^{2}A_{x}=0\\
 & \implies & \partial_{t}A_{x}=\textnormal{const.}
\end{eqnarray*}
 If the constant were not zero, $E_{x}$ would not be zero since $\boldsymbol{E}=-\partial_{t}\boldsymbol{A}$.
This would not fall off at infinity. Thus, assume $\partial_{t}A_{x}=0\implies A_{x}=0$.
 The wave solution does not fall off either, but its average vanishes.
\[
\implies\boldsymbol{A}\left(u\right)=\left(0,A_{y}\left(u\right),A_{z}\left(u\right)\right)\quad\cdots\quad\perp\hat{\boldsymbol{n}}
\]
\[
\implies\boldsymbol{E}=-\frac{1}{c}\partial_{t}\boldsymbol{A}=-\frac{1}{c}\partial_{u}\boldsymbol{A}\quad\cdots\quad\perp\hat{\boldsymbol{n}}
\]
\begin{eqnarray*}
\implies\underline{\boldsymbol{B}}=\nabla\times\boldsymbol{A} & = & \left(0,-\partial_{x}A_{z},\partial_{x}A_{y}\right)\\
 & = & -\frac{1}{c}\partial_{u}\left(0,-A_{z},A_{y}\right)\\
 & = & -\frac{1}{c}\partial_{u}\left(\hat{\boldsymbol{n}}\times\boldsymbol{A}\right)=-\frac{1}{c}\hat{\boldsymbol{n}}\times\partial_{\mu}\boldsymbol{A}=\underline{\hat{\boldsymbol{n}}\times\boldsymbol{E}}.
\end{eqnarray*}
\end{proof}
\begin{cor}
The Poynting vector is given by 
\[
\boxed{\boldsymbol{P}\left(\boldsymbol{x},t\right)=cu\left(\boldsymbol{x},t\right)\hat{\boldsymbol{n}}},
\]
 where $u\left(\boldsymbol{x},t\right)$ is the energy density of
the fields.\end{cor}
\begin{proof}
From \cref{chap:Maxwell's-Equations} \cref{sub:ch2sec3_6Poynting's-theorem},
\begin{eqnarray*}
\implies\underline{\boldsymbol{P}}=\frac{c}{4\pi}\boldsymbol{E}\times\boldsymbol{B} & = & \frac{c}{4\pi}\boldsymbol{E}\times\left(\hat{\boldsymbol{n}}\times\boldsymbol{E}\right)\\
 & \overset{1.}{=} & \frac{c}{4\pi}\boldsymbol{E}^{2}\hat{\boldsymbol{n}}\\
 & \overset{2.}{=} & \frac{c}{8\pi}\left(\boldsymbol{E}^{2}+\boldsymbol{B}^{2}\right)\hat{\boldsymbol{n}}\\
 & = & \underline{cu\left(\boldsymbol{x},t\right)\hat{\boldsymbol{n}}}.
\end{eqnarray*}

\begin{enumerate}
\item Since $\boldsymbol{E}\perp\hat{\boldsymbol{n}}$.
\item Since $\boldsymbol{E}^{2}=\boldsymbol{B}^{2}$. 
\end{enumerate}
\end{proof}
\begin{rem}
The energy contained in the wave propagates with velocity $c$ in
the direction $\hat{\boldsymbol{n}}$ perpendicular to the wave fronts.
\end{rem}

\subsection{\label{sub:ch4sec1_4Monochromatic-plane-waves}Monochromatic plane
waves}

Consider the wave equation:
\[
\left(\frac{1}{c^{2}}\partial_{t}^{2}-\nabla^{2}\right)f\left(\boldsymbol{x},t\right)=0.
\]

\begin{defn}
\textbf{\emph{Monochromatic plane wave.}} A solution of the form
\[
\boxed{f\left(\boldsymbol{x},t\right)=f_{0}e^{i\left(\boldsymbol{k}\cdot\boldsymbol{x}-\omega t\right)}},\quad f_{0}\in\mathbb{C},
\]
 is called a \emph{monochromatic plane wave} with frequency $\omega$.\end{defn}
\begin{rem}
Problem \#40 $\implies\omega^{2}=c^{2}\boldsymbol{k}^{2}\iff f$ solves
wave equation.
\end{rem}
~
\begin{rem}
By the superposition principle from \cref{chap:Maxwell's-Equations}
\ref{sec:ch2sec5The-superposition-principle}, if $f:\mathbb{R}^{4}\rightarrow\mathbb{C}$
is a solution, then so are $\textnormal{Re }f$, $\textnormal{Im }f$.
\end{rem}
What about $\boldsymbol{k}$?

\emph{Case 1:} $\underline{k_{x},k_{y},k_{z}\in\mathbb{R}}\quad\implies\omega\in\mathbb{R}$. 

We can write $f_{0}=\left|f_{0}\right|e^{-i\delta}$, where $\delta\in\mathbb{R}$.
Then $\textnormal{Re }f$, $\textnormal{Im }f$ yield the two solutions:
\[
\begin{aligned}\boxed{f\left(\boldsymbol{x},t\right)=\left|f_{0}\right|\cos\left(\boldsymbol{k}\cdot\boldsymbol{x}-\omega t-\delta\right)},\\
\boxed{f\left(\boldsymbol{x},t\right)=\left|f_{0}\right|\sin\left(\boldsymbol{k}\cdot\boldsymbol{x}-\omega t-\delta\right)}.
\end{aligned}
\]

\begin{rem}
For fixed $\boldsymbol{x}$, $f$ is periodic in $t$ with period
$T:=\frac{2\pi}{\omega}$.
\end{rem}
~
\begin{rem}
For fixed $t$, $f$ is periodic in space. Define
\[
\underline{\varphi:=\boldsymbol{k}\cdot\boldsymbol{x}-\omega t-\delta}
\]
 to be the \emph{phase }of the wave.
\[
f=\textnormal{const.}\iff\varphi=\textnormal{const.}\iff\underline{\boldsymbol{k}\cdot\boldsymbol{x}=\omega t+\delta}.
\]
 Thus, the surfaces of constant field are planes perpendicular to
$\boldsymbol{k}$. $\boldsymbol{k}$ is called \emph{wave vector};
$\lambda:=\frac{2\pi}{\left|\boldsymbol{k}\right|}$ is called \emph{wavelength}.
\end{rem}
~

\emph{Case 2:} 

$\underline{\textnormal{At least one of }k_{i}\textnormal{ is not real, e.g., }k_{x}=\alpha+i\beta}.$
\[
\implies f\left(\boldsymbol{x},t\right)=e^{i\alpha\lambda}e^{-\beta x}f_{0}e^{i\left(k_{y}y+k_{z}z-\omega t\right)}
\]
\[
\implies f\rightarrow\infty\textnormal{ if }x\rightarrow\mp\infty\textnormal{ for }\beta\gtrless0.
\]
 Thus, the solution is physically meaningful at most in a restricted
space (e.g., total reflection at a surface).

~

\emph{Case 3:} $\underline{\omega\notin\mathbb{R}\implies\boldsymbol{k}\notin\mathbb{R}^{3}}$. 

This is the same as case 2, since $\omega^{2}=c^{2}\boldsymbol{k}^{2}$.


\subsection{\label{sub:ch4sec1_5Polarization-of-electromagnetic}Polarization
of electromagnetic waves}

Nothing we have derived prohibits $\boldsymbol{E}$, $\boldsymbol{B}$
from rotating about $\boldsymbol{k}$. We can express a monochromatic
plane wave as
\[
\begin{aligned}\boxed{\boldsymbol{E}\left(\boldsymbol{x},t\right)=\boldsymbol{E}_{0}e^{i\left(\boldsymbol{k}\cdot\boldsymbol{x}-\omega t\right)}}\\
\boxed{\boldsymbol{B}\left(\boldsymbol{x},t\right)=\boldsymbol{B}_{0}e^{i\left(\boldsymbol{k}\cdot\boldsymbol{x}-\omega t\right)}}
\end{aligned}
\textnormal{, where }\omega^{2}=c^{2}k^{2}\textnormal{ with }k^{2}:=\left|\boldsymbol{k}\right|^{2}.
\]
 
\begin{rem}
\label{rem:ch4sec1_5rem1The-direction-of}The direction of propagation
$\hat{\boldsymbol{n}}$ from \cref{sub:ch4sec1_3Orientation-of-the}
is
\[
\hat{\boldsymbol{n}}=\hat{\boldsymbol{k}}:=\frac{\boldsymbol{k}}{\left|\boldsymbol{k}\right|}=\frac{\boldsymbol{k}}{\nicefrac{\omega}{c}}.
\]
 From \cref{sub:ch4sec1_3Orientation-of-the} we also have 
\[
\left|\boldsymbol{E}_{0}\right|=\left|\boldsymbol{B}_{0}\right|
\]
 and $\boldsymbol{E}_{0}$, $\boldsymbol{B}_{0}$, $\boldsymbol{k}$
form a right-handed coordinate system.

Consider $\boldsymbol{E}$.\footnote{This discussion is analogous for $\boldsymbol{B}-$field.}
Let $\boldsymbol{E}_{0}^{2}=\left|\boldsymbol{E}_{0}\right|^{2}e^{-i2\alpha}$,
and define $\boldsymbol{b}:=\boldsymbol{E}_{0}e^{i\alpha}$ with the
property $\boldsymbol{b}^{2}=\left|\boldsymbol{E}_{0}^{2}\right|\in\mathbb{R}$.
Consider the physical solution 
\[
\boldsymbol{E}\left(\boldsymbol{x},t\right)=\textnormal{Re }\left[\boldsymbol{b}e^{i\left(\boldsymbol{k}\cdot\boldsymbol{x}-\omega t-\alpha\right)}\right],
\]
 where $\boldsymbol{b}=\boldsymbol{b}_{1}+i\boldsymbol{b}_{2}$ with
$\boldsymbol{b}_{1}\perp\boldsymbol{b}_{2}$ since $\boldsymbol{b}^{2}\in\mathbb{R}$.
Let $\boldsymbol{k}=\left(k,0,0\right)$. $\implies\boldsymbol{b}_{1}=\left(0,b_{1},0\right)$,
$\boldsymbol{b}_{2}=\left(0,0,b_{2}\right)$,
\[
\implies\begin{aligned}E_{y} & = & b_{1}\cos\left(\boldsymbol{k}\cdot\boldsymbol{x}-\omega t-\alpha\right)\\
E_{z} & = & -b_{2}\sin\left(\boldsymbol{k}\cdot\boldsymbol{x}-\omega t-\alpha\right)
\end{aligned}
\implies\boxed{\frac{E_{y}^{2}}{b_{1}^{2}}+\frac{E_{z}^{2}}{b_{2}^{2}}=1}.
\]
\end{rem}
\begin{prop}
The $\boldsymbol{E}-$field vector moves on an ellipse; the same is
true for the $\boldsymbol{B}-$field. This is called \emph{elliptic
polarization}.\end{prop}
\begin{proof}
(above)\end{proof}
\begin{rem}
Monochromatic plane waves are, in general, elliptically polarized.
\end{rem}
~
\begin{rem}
Special cases:
\begin{lyxlist}{00.00.0000}
\item [{$b_{1}=b_{2}$}] \emph{circular polarization}
\item [{$b_{1}=0\textnormal{ or }b_{2}=0$}] \emph{linear polarization}
\end{lyxlist}
\end{rem}
~
\begin{rem}
Visualization:
\end{rem}

\subsection{The Doppler effect}

Define the 4-wavevector $k^{\mu}:=\left(\frac{\omega}{c},\boldsymbol{k}\right)=\left(k_{0},\boldsymbol{k}\right)$.
From problem \#40, $k^{\mu}$ transforms as a Minkowski vector.
\begin{prop}
The 4-wavevector has zero length in Minkowski space:
\[
\boxed{k_{\mu}k^{\mu}=0}.
\]
\end{prop}
\begin{proof}
$k_{\mu}k^{\mu}=\frac{\omega^{2}}{c^{2}}-\boldsymbol{k}^{2}=0$, from
the wave equation.\end{proof}
\begin{rem}
This implies our 4-wavevector lies on the \emph{light cone} given
by $k^{0}=\left|\boldsymbol{k}\right|$. .
\end{rem}
Consider an observer in a moving frame whose velocity forms some angle
$\theta$ with propagation direction $\boldsymbol{k}$.  What frequency
does the moving observer see?
\begin{thm}
\textbf{\emph{Doppler effect.}} If $\omega$ is the frequency of the
wave observed in the rest frame, then the moving observer measures
a different frequency $\omega'$ such that
\[
\boxed{\omega'=\gamma\omega\left(1-\frac{v}{c}\cos\theta\right)}.
\]
\end{thm}
\begin{proof}
Lorentz boost 4-wavevector along $x$:
\[
\implies\frac{\omega'}{c}=\gamma\left(\frac{\omega}{c}-\frac{v}{c}k_{x}\right).
\]
 But $k_{x}=\left|\boldsymbol{k}\right|\cos\theta=\frac{\omega}{c}\cos\theta$;
insert this and factor $\frac{\omega}{c}$.\end{proof}
\begin{rem}
The frequency shift given by $\left(1-\frac{v}{c}\cos\theta\right)$
is called \emph{linear Doppler effect}. The shift from $\gamma$ is
called \emph{quadratic Doppler effect}.
\end{rem}
~
\begin{rem}
The quadratic Doppler effect is nonzero even for $\cos\theta=0$;
a manifestation of time dilation. 
\end{rem}
~
\begin{rem}
Consider a non-relativistic wave, e.g. a sound wave (density wave)
in a fluid. The density fluctuation can be written
\[
\delta n\left(\boldsymbol{x},t\right)=ae^{i\left(\boldsymbol{k}\cdot\boldsymbol{x}-\omega t\right)},\textnormal{ where }\omega=c_{0}k
\]
 and $c_{0}$ is the phase velocity. Under a Galilean transformation,
\begin{eqnarray*}
x' & = & x-vt\\
y' & = & y\\
t' & = & t
\end{eqnarray*}
\begin{eqnarray*}
\implies\delta n\left(\boldsymbol{x},t\right) & = & ae^{i\left(k_{x}x'+k_{x}vt'+k_{y}y'-\omega t'\right)}\\
 & = & ae^{i\left(k_{x}x'+k_{y}y'-\left(\omega-k_{x}v\right)t'\right)}
\end{eqnarray*}
 But $\omega-k_{x}v=\omega'$
\[
\implies\omega'=\omega\left(1-\frac{v}{c_{0}}\cos\theta\right).
\]
 Only the linear Doppler effect is observed, and there is \emph{no}
frequency shift for motion perpendicular to $\boldsymbol{k}$.
\end{rem}

\section{The wave equation as an initial value problem}


\subsection{The wave equation in Fourier space}

From \cref{sub:ch4sec1_1The-wave-equation}, the general wave equation
is 
\[
\boxed{\square f\left(\boldsymbol{x},t\right)=\left(\frac{1}{c^{2}}\partial_{t}^{2}-\nabla^{2}\right)f\left(\boldsymbol{x},t\right)=0}.\tag{*}
\]
 Take a spacial Fourier transform (\cref{chap:Static-solutions-of}
\ref{sec:ch3sec2_Digression:-Fourier-transforms}), where
\[
\underline{\hat{f}\left(\boldsymbol{k},t\right)=\indefint{\boldsymbol{x}}e^{-i\boldsymbol{k}\cdot\boldsymbol{x}}f\left(\boldsymbol{x},t\right)}
\]
 with back transform
\[
\underline{f\left(\boldsymbol{x},t\right)=\frac{1}{\left(2\pi\right)^{3}}\int d\boldsymbol{k}\:e^{i\boldsymbol{k}\cdot\boldsymbol{x}}\hat{f}\left(\boldsymbol{k},t\right)}.
\]

\begin{rem}
The generalized function concept implies this can be done for a large
class of functions.
\begin{eqnarray*}
\left(*\right)\implies0 & = & \left(\frac{1}{c^{2}}\partial_{t}^{2}-\nabla^{2}\right)\frac{1}{\left(2\pi\right)^{3}}\int d\boldsymbol{k}\:e^{i\boldsymbol{k}\cdot\boldsymbol{x}}\hat{f}\left(\boldsymbol{k},t\right)\\
 & = & \frac{1}{\left(2\pi\right)^{3}}\int d\boldsymbol{k}\:\left(\frac{1}{c^{2}}\partial_{t}^{2}-\boldsymbol{k}^{2}\right)\hat{f}\left(\boldsymbol{k},t\right)
\end{eqnarray*}
 But this integrand is positive definite.
\[
\implies\boxed{\frac{d^{2}}{dt^{2}}\hat{f}\left(\boldsymbol{k},t\right)+c^{2}\boldsymbol{k}^{2}\hat{f}\left(\boldsymbol{k},t\right)=0}.\tag{**}
\]
 An alternative way to see this is to multiply $\left(*\right)$ by
$e^{-i\boldsymbol{k}\cdot\boldsymbol{x}}$ and take the $\boldsymbol{x}$
integral:
\begin{eqnarray*}
\implies0 & = & \indefint{\boldsymbol{x}}e^{-i\boldsymbol{k}\cdot\boldsymbol{x}}\left(\frac{1}{c^{2}}\partial_{t}^{2}-\nabla^{2}\right)f\left(\boldsymbol{x},t\right)\\
 & = & \frac{1}{c^{2}}\partial_{t}^{2}\hat{f}+\boldsymbol{k}^{2}\hat{f}
\end{eqnarray*}
 (integrating by parts twice).
\end{rem}
~
\begin{rem}
$\left(**\right)$ is an ODE for a harmonic oscillator with frequency\footnote{In this section, the notation $\omega_{\boldsymbol{k}}$ implies $\omega$
is a function of $\boldsymbol{k}$.} 
\[
\omega_{\boldsymbol{k}}=c\left|\boldsymbol{k}\right|=:ck.
\]

\end{rem}
~
\begin{rem}
The Fourier back transform theorem implies $\left(*\right)$ is equivalent
to $\left(**\right)$. 
\end{rem}

\subsection{The general solution of the wave equation}

The general solution of $\left(**\right)$ for $\hat{f}$ is 
\[
\boxed{\hat{f}\left(\boldsymbol{k},t\right)=a{}_{\boldsymbol{k}}^{0}\cos\left(\omega_{\boldsymbol{k}}t\right)+\frac{\overset{\bullet}{a}{}_{\boldsymbol{k}}^{0}}{\omega_{\boldsymbol{k}}}\sin\left(\omega_{\boldsymbol{k}}t\right)},
\]
 where 
\[
\boxed{\begin{aligned}a{}_{\boldsymbol{k}}^{0} & := & \hat{f}\left(\boldsymbol{k},t=0\right) & = & \indefint{\boldsymbol{x}}e^{-i\boldsymbol{k}\cdot\boldsymbol{x}}f\left(\boldsymbol{x},t=0\right)\\
\overset{\bullet}{a}{}_{\boldsymbol{k}}^{0} & := & \left.\partial_{t}\hat{f}\left(\boldsymbol{k},t\right)\right|_{t=0} & = & \indefint{\boldsymbol{x}}e^{-i\boldsymbol{k}\cdot\boldsymbol{x}}\left.\partial_{t}f\left(\boldsymbol{x},t\right)\right|_{t=0}
\end{aligned}
}
\]

\begin{thm}
The general solution of the wave equation is uniquely determined by
the field and its time derivative at some initial time (WLOG $t=0$),\footnote{That is, $f\left(\boldsymbol{x},t=0\right)$ and $\left.\partial_{t}f\left(\boldsymbol{x},t\right)\right|_{t=0}$.}
and is given by
\[
\boxed{f\left(\boldsymbol{x},t\right)=\frac{1}{\left(2\pi\right)^{3}}\int d\boldsymbol{k}\:e^{i\boldsymbol{k}\cdot\boldsymbol{x}}\left[a{}_{\boldsymbol{k}}^{0}\cos\left(\omega_{\boldsymbol{k}}t\right)+\frac{\overset{\bullet}{a}{}_{\boldsymbol{k}}^{0}}{\omega_{\boldsymbol{k}}}\sin\left(\omega_{\boldsymbol{k}}t\right)\right]},
\]
 with $\omega_{\boldsymbol{k}}=c\left|\boldsymbol{k}\right|$ and
$a{}_{\boldsymbol{k}}^{0},\,\overset{\bullet}{a}{}_{\boldsymbol{k}}^{0}$
defined above.\end{thm}
\begin{cor}
The solution can also be written
\[
\boxed{f\left(\boldsymbol{x},t\right)=\frac{1}{\left(2\pi\right)^{3}}\int d\boldsymbol{k}\:\left[f_{\boldsymbol{k}}^{+}e^{i\left(\boldsymbol{k}\cdot\boldsymbol{x}-\omega_{\boldsymbol{k}}t\right)}+f_{\boldsymbol{k}}^{-}e^{-i\left(\boldsymbol{k}\cdot\boldsymbol{x}-\omega_{\boldsymbol{k}}t\right)}\right]},
\]
 where 
\[
\boxed{f_{\boldsymbol{k}}^{\pm}:=\frac{1}{2}\left(a{}_{\pm\boldsymbol{k}}^{0}\pm i\frac{1}{\omega_{\boldsymbol{k}}}\overset{\bullet}{a}{}_{\pm\boldsymbol{k}}^{0}\right)}.
\]
\end{cor}
\begin{proof}
\begin{eqnarray*}
\underline{a{}_{\boldsymbol{k}}^{0}\cos\left(\omega_{\boldsymbol{k}}t\right)+\frac{\overset{\bullet}{a}{}_{\boldsymbol{k}}^{0}}{\omega_{\boldsymbol{k}}}\sin\left(\omega_{\boldsymbol{k}}t\right)} & = & a{}_{\boldsymbol{k}}^{0}\frac{1}{2}\left(e^{i\omega_{\boldsymbol{k}}t}+e^{-i\omega_{\boldsymbol{k}}t}\right)+\frac{\overset{\bullet}{a}{}_{\boldsymbol{k}}^{0}}{\omega_{\boldsymbol{k}}}\frac{1}{2i}\left(e^{i\omega_{\boldsymbol{k}}t}-e^{-i\omega_{\boldsymbol{k}}t}\right)\\
 & = & \understuff{\frac{1}{2}\left(a{}_{\boldsymbol{k}}^{0}+i\frac{\overset{\bullet}{a}{}_{\boldsymbol{k}}^{0}}{\omega_{\boldsymbol{k}}}\right)}{f_{\boldsymbol{k}}^{+}}e^{-i\omega_{\boldsymbol{k}}t}+\understuff{\frac{1}{2}\left(a{}_{\boldsymbol{k}}^{0}-i\frac{\overset{\bullet}{a}{}_{\boldsymbol{k}}^{0}}{\omega_{\boldsymbol{k}}}\right)}{f_{-\boldsymbol{k}}^{-}}e^{i\omega_{\boldsymbol{k}}t}
\end{eqnarray*}
\begin{eqnarray*}
\implies f\left(\boldsymbol{x},t\right) & = & \frac{1}{\left(2\pi\right)^{3}}\int d\boldsymbol{k}\:e^{i\boldsymbol{k}\cdot\boldsymbol{x}}\left[f_{\boldsymbol{k}}^{+}e^{-i\omega_{\boldsymbol{k}}t}+f_{-\boldsymbol{k}}^{-}e^{i\omega_{\boldsymbol{k}}t}\right]\\
 & = & \frac{1}{\left(2\pi\right)^{3}}\int d\boldsymbol{k}\:f_{\boldsymbol{k}}^{+}e^{i\boldsymbol{k}\cdot\boldsymbol{x}-i\omega_{\boldsymbol{k}}t}+\frac{1}{\left(2\pi\right)^{3}}\int d\boldsymbol{k}\:f_{\boldsymbol{k}}^{-}e^{-i\boldsymbol{k}\cdot\boldsymbol{x}+i\omega_{\boldsymbol{k}}t},
\end{eqnarray*}
 where in the last line, in the second term $\omega_{\boldsymbol{k}}=\omega_{-\boldsymbol{k}}$
is inserted.\end{proof}
\begin{rem}
The general solution of the wave equation is a linear superposition
of monochromatic plane waves with superposition amplitudes that are
uniquely determined by the initial conditions $f\left(\boldsymbol{x},t=0\right)$
and $\dot{f}\left(\boldsymbol{x},t=0\right)$
\end{rem}

\chapter{Electromagnetic radiation}
\begin{description}
\item [{idea:}] we have discussed\end{description}
\begin{itemize}
\item static solutions of Maxwell's equations with sources (\cref{chap:Static-solutions-of})
\item dynamic solutions of Maxwell's equations in vacuum (\cref{chap:Electromagnetic-waves-in}).
\end{itemize}
Now we discuss
\begin{itemize}
\item dynamic solutions of Maxwell's equations with sources.
\end{itemize}

\section{Review of potentials, gauges}


\subsection{\label{sub:ch5sec1_1Fields-and-potentials}Fields and potentials}

Recall in \cref{chap:Maxwell's-Equations} \cref{sub:ch2sec3_4Relations-between-fields},
the fields $\boldsymbol{E}$, $\boldsymbol{B}$ (which are observable)
can be obtained from potentials (that are not observable) via
\[
\boxed{\begin{alignedat}{1}\boldsymbol{E}\left(\boldsymbol{x},t\right) & =-\nabla\phi\left(\boldsymbol{x},t\right)-\frac{1}{c}\partial_{t}\boldsymbol{A}\left(\boldsymbol{x},t\right)\\
\boldsymbol{B}\left(\boldsymbol{x},t\right) & =\nabla\times\boldsymbol{A}\left(\boldsymbol{x},t\right)
\end{alignedat}
}
\]

\begin{rem}
The homogeneous Maxwell equations are automatically fulfilled by these.
\end{rem}
~
\begin{rem}
From \cref{chap:Maxwell's-Equations} \cref{sub:ch2sec3_1The-field-tensor},
$\phi$, $\boldsymbol{A}$ are the components of the 4-vector $A^{\mu}\left(x\right)=\left(\phi\left(x\right),\boldsymbol{A}\left(x\right)\right)$.\end{rem}
\begin{prop}
The inhomogeneous Maxwell equations (M3, M4) are equivalent to four
PDEs, which are the equations of motion (or field equations) for $A^{\mu}\left(x\right)$:\footnote{Compare with \cref{chap:Electromagnetic-waves-in} \cref{sub:ch4sec1_1The-wave-equation}.}
\[
\boxed{\partial_{\mu}\partial^{\mu}A^{\nu}\left(x\right)-\partial^{\nu}\partial_{\mu}A^{\mu}\left(x\right)=\frac{4\pi}{c}J^{\nu}\left(x\right)}\tag{\ensuremath{\ast}}
\]
\end{prop}
\begin{proof}
From \cref{chap:Maxwell's-Equations} \cref{sub:ch2sec1_3The-field-equations},
\begin{eqnarray*}
\frac{4\pi}{c}J^{\nu} & = & \partial_{\mu}F^{\mu\nu}\\
 & = & \partial_{\mu}\partial^{\mu}A^{\nu}-\partial^{\nu}\partial_{\mu}A^{\mu}.
\end{eqnarray*}
\end{proof}
\begin{cor}
In terms of $\phi$, $\boldsymbol{A}$, $\left(*\right)$ takes the
form
\[
\boxed{\begin{alignedat}{2}\square\boldsymbol{A}+\nabla\left(\frac{1}{c}\partial_{t}\phi+\nabla\cdot\boldsymbol{A}\right) & = & \frac{4\pi}{c}\boldsymbol{j}\\
-\nabla^{2}\phi-\frac{1}{c}\partial_{t}\nabla\cdot\boldsymbol{A} & = & 4\pi\rho
\end{alignedat}
}\tag{\ensuremath{\ast^{\prime}}}
\]
 where $\square:=\frac{1}{c^{2}}\partial_{t}^{2}-\nabla^{2}$.\end{cor}
\begin{proof}
$J^{\nu}=\left(c\rho,\boldsymbol{j}\right)$, $\partial^{\mu}:=\frac{\partial}{\partial x_{\mu}}=\left(\frac{1}{c}\partial_{t},-\nabla\right)$,
$\partial_{\mu}:=\frac{\partial}{\partial x^{\mu}}=\left(\frac{1}{c}\partial_{t},\nabla\right)$.
\begin{eqnarray*}
\implies\partial_{\mu}\partial^{\mu} & = & \frac{1}{c^{2}}\partial_{t}^{2}-\nabla^{2}=:\square,\\
\partial_{\mu}A^{\mu} & = & \frac{1}{c}\partial_{t}\phi+\nabla\cdot\boldsymbol{A}.
\end{eqnarray*}

\begin{lyxlist}{00.00.0000}
\item [{$\nu=1,\,2,\,3$}] in $\left(*\right)$ yields the first equation.
\item [{$\nu=0$}] in $\left(*\right)$ yields
\end{lyxlist}
\begin{eqnarray*}
\square\phi-\frac{1}{c}\partial_{t}\left(\frac{1}{c}\partial_{t}\phi+\nabla\cdot\boldsymbol{A}\right) & = & \frac{4\pi}{c}c\rho\\
=\bcancel{\frac{1}{c^{2}}\partial_{t}^{2}\phi}-\nabla^{2}\phi-\bcancel{\frac{1}{c^{2}}\partial_{t}^{2}\phi}-\frac{1}{c}\partial_{t}\nabla\cdot\boldsymbol{A} & = & 4\pi\rho
\end{eqnarray*}
\end{proof}
\begin{rem}
In the static case, $\left(*'\right)$ simplifies to 
\[
\begin{alignedat}{3}\nabla^{2}\phi & = & 4\pi\rho & \quad\dots\quad & \textnormal{ Poisson's equation (Ch3 1.1)} & \quad\checked\\
\understuff{-\nabla^{2}\boldsymbol{A}+\nabla\left(\nabla\cdot\boldsymbol{A}\right)}{=\nabla\times\left(\nabla\times\boldsymbol{A}\right)=\nabla\times\boldsymbol{B}} & = & \frac{4\pi}{c}\boldsymbol{j} & \quad\dots\quad & \textnormal{Fourth Maxwell equation} & \quad\checked
\end{alignedat}
\]

\end{rem}
~
\begin{rem}
In vacuum and using Lorenz gauge, $\left(*\right)$ simplifies to
\[
\begin{alignedat}{3}\square A^{\nu}-\partial^{\nu}\understuff{\partial_{\mu}A^{\mu}}{=0} & = & 0 & \quad\dots\quad & \textnormal{ wave equation (Ch4 1.1)} & \quad\checked\end{alignedat}
\]

\end{rem}

\subsection{\label{sub:ch5sec1_2Gauge-conventions}Gauge conventions}

From \cref{chap:Maxwell's-Equations} \cref{sub:ch2sec2_4Gauge-invariance},
the potentials are not unique. We can choose certain constraints,
called \emph{gauge conventions}.

~

\textbf{Popular choices:}

\begin{tabular}{cccc}
$\left(1\right)$ & \emph{Lorenz gauge} & $\boxed{\partial_{\mu}A^{\mu}\left(x\right)=0=\frac{1}{c}\partial_{t}\phi+\nabla\cdot\boldsymbol{A}}$ & Lorentz invariant\tabularnewline
$\left(2\right)$ & \emph{Coulomb gauge} & $\boxed{\nabla\cdot\boldsymbol{A}=0}$ (cf. Problem \#19) & not Lorentz invariant\tabularnewline
\end{tabular}
\begin{rem}
Some books call $\left(2\right)$ the \emph{transverse gauge}, since
$\boldsymbol{k}\cdot\boldsymbol{A}\left(\boldsymbol{k}\right)=0$
(from Fourier transforming), which implies $\boldsymbol{A}\perp\boldsymbol{k}$.
Others call it \emph{radiation gauge}. 
\end{rem}
~
\begin{rem}
Another possibility is to choose $\boxed{\phi\left(x\right)=0}.$
This is also sometimes called \emph{radiation gauge}. 
\end{rem}
~
\begin{rem}
4 potentials and 1 constraint (our choice of gauge) implies 3 potential
fields uniquely determine the 6 fields $\boldsymbol{E}$, $\boldsymbol{B}$. \end{rem}
\begin{prop}
In Lorenz gauge, the field equations for the potentials \textup{\cref{sub:ch5sec1_1Fields-and-potentials}
$\left(*\right)$ becomes
\[
\boxed{\begin{alignedat}{2}\square\boldsymbol{A} & = & \frac{4\pi}{c}\boldsymbol{j}\\
\square\phi & = & 4\pi\rho
\end{alignedat}
}\textnormal{ or, }\boxed{\square A^{\mu}=\frac{4\pi}{c}J^{\mu}}.\tag{\ensuremath{\ast}}
\]
}\end{prop}
\begin{proof}
Lorenz gauge $\implies\partial_{\mu}A^{\mu}=0,$ $\therefore$ \cref{sub:ch5sec1_1Fields-and-potentials}
$\left(*\right)$ $\implies\left(*\right)$.\end{proof}
\begin{cor}
Once we choose Lorenz gauge, it is maintained under time evolution.\end{cor}
\begin{proof}
$\square\partial_{\mu}A^{\mu}=\partial_{\mu}\square A^{\mu}=\frac{4\pi}{c}\understuff{\partial_{\mu}J^{\mu}=0}{\textnormal{continuity eq.}}.$\end{proof}
\begin{rem}
From \cref{chap:Maxwell's-Equations} \cref{sub:ch2sec2_1Continuity-equation-for},
$\partial_{\mu}J^{\mu}=0$ is not an independent condition; it follows
from the field equations.\end{rem}
\begin{prop}
In Coulomb gauge, the field equations become
\[
\boxed{\begin{alignedat}{1}\square\boldsymbol{A} & =\frac{4\pi}{c}\boldsymbol{j}-\frac{1}{c}\partial_{t}\nabla\phi\\
\nabla^{2}\phi & =-4\pi\rho
\end{alignedat}
}.\tag{\ensuremath{\ast\ast}}
\]
\end{prop}
\begin{proof}
\cref{sub:ch5sec1_1Fields-and-potentials} $\left(*'\right)$ with
$\nabla\cdot\boldsymbol{A}=0\implies\left(**\right)$.\end{proof}
\begin{cor}
Coulomb gauge is maintained under time evolution.\end{cor}
\begin{proof}
\begin{eqnarray*}
\underline{\square\left(\nabla\cdot\boldsymbol{A}\right)}=\nabla\cdot\left(\square\boldsymbol{A}\right) & \overset{1.}{=} & \frac{4\pi}{c}\nabla\cdot\boldsymbol{j}-\frac{1}{c}\partial_{t}\nabla^{2}\phi\\
 & \overset{2.}{=} & \frac{4\pi}{c}\nabla\cdot\boldsymbol{j}+\frac{4\pi}{c}\partial_{t}\rho\\
 & = & \frac{4\pi}{c}\left(\nabla\cdot\boldsymbol{j}+\frac{1}{c}\partial_{t}c\rho\right)\\
 & = & \frac{4\pi}{c}\understuff{\partial_{\mu}J^{\mu}=0}{\textnormal{continuity eq.}}.
\end{eqnarray*}

\begin{enumerate}
\item Inserting $\square\boldsymbol{A}$ from $\left(*'\right)$.
\item Inserting $-\nabla^{2}\phi$ from $\left(*'\right)$.
\end{enumerate}
\end{proof}
\begin{rem}
Which gauge to pick is a matter of choice. Different choices are convenient
for different problems.
\end{rem}

\section{Green's functions; the Lorenz gauge}


\subsection{\label{sub:ch5sec2_1The-concept-of}The concept of Green's functions}

Consider an inhomogeneous wave equation:
\[
\boxed{\square f\left(\boldsymbol{x},t\right)=i\left(\boldsymbol{x},t\right)},\tag{\ensuremath{\ast}}
\]
 with $i\left(\boldsymbol{x},t\right)$ a given inhomogeneity.
\begin{defn}
\textbf{\emph{Green's function.}} A \emph{Green's function} $G\left(\boldsymbol{x},t\right)$
for the PDE $\left(*\right)$ is a solution of
\[
\boxed{\square G\left(\boldsymbol{x},t\right)=\delta\left(\boldsymbol{x}\right)\delta\left(t\right)}.\tag{\ensuremath{\ast\ast}}
\]
\end{defn}
\begin{rem}
This is $\left(*\right)$ with a special inhomogeneity
\[
i\left(\boldsymbol{x},t\right)=\delta\left(\boldsymbol{x}\right)\delta\left(t\right)=\delta\left(x\right)\delta\left(y\right)\delta\left(z\right)\delta\left(t\right).
\]
\end{rem}
\begin{prop}
Let $G\left(\boldsymbol{x},t\right)$ be a solution of $\left(**\right)$.
Then
\[
\boxed{f\left(\boldsymbol{x},t\right)=\int d\boldsymbol{x}'\:dt'\:G\left(\boldsymbol{x}-\boldsymbol{x}',t-t'\right)i\left(\boldsymbol{x}',t'\right)=:\left(G\star i\right)\left(\boldsymbol{x},t\right)}
\]
 is a solution of $\left(*\right)$.\end{prop}
\begin{proof}
\begin{eqnarray*}
\square f\left(\boldsymbol{x},t\right) & = & \int d\boldsymbol{x}'\:dt'\:\square G\left(\boldsymbol{x}-\boldsymbol{x}',t-t'\right)i\left(\boldsymbol{x}',t'\right)\\
 & = & \int d\boldsymbol{x}'\:dt'\:\delta\left(\boldsymbol{x}-\boldsymbol{x}'\right)\delta\left(t-t'\right)i\left(\boldsymbol{x}',t'\right)\\
 & = & i\left(\boldsymbol{x},t\right).
\end{eqnarray*}
 Note that this assumes we can interchange $\square,\,\int$ which
is allowed if $G$ is sufficiently well behaved.
\end{proof}

\subsection{\label{sub:ch5sec2_2Green's-functions-for}Green's functions for
the wave equation}

To find the form of Green's functions, take the Fourier transform
of \cref{sub:ch5sec2_1The-concept-of} $\left(**\right)$ with respect
to time. That is, take $\indefint te^{i\omega t}\left(**\right)$:\footnote{Note that, by convention, we use $+i$ instead of $-i$ here.}
\[
\implies\delta\left(\boldsymbol{x}\right)\understuff{\indefint te^{i\omega t}\delta\left(t\right)}{=1\textnormal{ (Ch.3 2.5)}}=\indefint te^{i\omega t}\frac{1}{c^{2}}\partial_{t}^{2}G\left(\boldsymbol{x},t\right)-\nabla^{2}\understuff{\indefint te^{i\omega t}G\left(\boldsymbol{x},t\right)}{\textnormal{define }=:G_{\omega}\left(\boldsymbol{x}\right)}
\]
\begin{eqnarray*}
\implies\delta\left(\boldsymbol{x}\right)+\nabla^{2}G_{\omega}\left(\boldsymbol{x}\right) & = & \frac{1}{c^{2}}\indefint te^{i\omega t}\partial_{t}^{2}G\left(\boldsymbol{x},t\right)\\
 & \overset{1.}{=} & \frac{\left(i\omega\right)^{2}}{c^{2}}\understuff{\indefint te^{i\omega t}G\left(\boldsymbol{x},t\right)}{=G_{\omega}\left(\boldsymbol{x}\right)}
\end{eqnarray*}

\begin{enumerate}
\item Integrating by parts twice and assuming $G\left(\boldsymbol{x},t\right)$
falls off at $\pm\infty$, or, using \cref{chap:Static-solutions-of}
\cref{sub:ch3sec2_1The-Fourier-transform} \cref{prop:fourier_of_derivativeLet--be}.
\end{enumerate}
Thus, $G_{\omega}\left(\boldsymbol{x}\right)$ obeys
\[
\underline{-\left(\nabla^{2}+\frac{\omega^{2}}{c^{2}}\right)G_{\omega}\left(\boldsymbol{x}\right)=\delta\left(\boldsymbol{x}\right)}.
\]


We solve this by taking the spacial Fourier transforms. Define
\[
G_{\omega}\left(\boldsymbol{k}\right):=\indefint{\boldsymbol{x}}e^{-i\boldsymbol{k}\cdot\boldsymbol{x}}G_{\omega}\left(\boldsymbol{x}\right).
\]
\[
\implies\left(\boldsymbol{k}^{2}-\frac{\omega^{2}}{c^{2}}\right)G_{\omega}\left(\boldsymbol{k}\right)=1
\]
\[
\implies\boxed{G_{\omega}\left(\boldsymbol{k}\right)=\frac{1}{\boldsymbol{k}^{2}-\frac{\omega^{2}}{c^{2}}}}.
\]
 To find $G\left(\boldsymbol{x},t\right)$, we must back transform.
\begin{lyxlist}{00.00.0000}
\item [{\textbf{spatial:}}] 
\begin{eqnarray*}
\underline{G_{\omega}\left(\boldsymbol{x}\right)} & = & \int\frac{d\boldsymbol{k}}{\left(2\pi\right)^{3}}\:e^{i\boldsymbol{k}\cdot\boldsymbol{x}}G_{\omega}\left(\boldsymbol{k}\right)\\
 & = & \int\frac{d\boldsymbol{k}}{\left(2\pi\right)^{3}}\:e^{i\boldsymbol{k}\cdot\boldsymbol{x}}\frac{1}{\boldsymbol{k}^{2}+\left(\frac{i\omega}{c}\right)^{2}}\\
 & \overset{1.}{=} & \underline{\frac{1}{4\pi}\frac{e^{\pm\frac{i\omega r}{c}}}{r}}
\end{eqnarray*}
\end{lyxlist}
\begin{enumerate}
\item From Problem \#28, $\int\frac{d\boldsymbol{k}}{\left(2\pi\right)^{3}}\:e^{i\boldsymbol{k}\boldsymbol{x}}\frac{4\pi}{\boldsymbol{k}^{2}+\left(\frac{1}{r_{0}}\right)^{2}}=\frac{e^{-\frac{r}{r_{0}}}}{r}$,
where $r_{0}=\pm\frac{c}{i\omega}$, $r=\left|\boldsymbol{x}\right|$.\end{enumerate}
\begin{lyxlist}{00.00.0000}
\item [{\textbf{temporal:}}] \textbf{
\begin{eqnarray*}
\underline{G\left(\boldsymbol{x},t\right)} & = & \int\frac{d\omega}{2\pi}\:e^{-i\omega t}G_{\omega}\left(\boldsymbol{x}\right)\\
 & = & \frac{1}{4\pi}\frac{1}{r}\int\frac{d\omega}{2\pi}\:e^{-i\omega t\pm\frac{i\omega r}{c}}\\
 & = & \frac{1}{4\pi r}\int\frac{d\omega}{2\pi}\:e^{-i\omega\left(t\mp\frac{r}{c}\right)}\\
 & = & \underline{\frac{1}{4\pi r}\delta\left(t\mp\frac{r}{c}\right)}.
\end{eqnarray*}
}\end{lyxlist}
\begin{thm}
The defining equation for the Green's functions (\textup{\cref{sub:ch5sec2_1The-concept-of}
$\left(**\right)$) has two solutions:
\[
\boxed{G_{\pm}\left(\boldsymbol{x},t\right)=\frac{1}{4\pi r}\delta\left(t\mp\frac{r}{c}\right)}\textnormal{ where }r:=\left|\boldsymbol{x}\right|.
\]
}\end{thm}
\begin{proof}
(above).\end{proof}
\begin{rem}
Consider a point-like, time-dependent source
\[
\underline{i\left(\boldsymbol{x},t\right)=\delta\left(\boldsymbol{x}\right)i\left(t\right)}.
\]
 From the proposition in \cref{sub:ch5sec2_1The-concept-of}, the
two solutions of the wave equation with this source are
\begin{eqnarray*}
\underline{f_{\pm}\left(\boldsymbol{x},t\right)} & = & \int d\boldsymbol{x}'\:dt'\:\frac{1}{4\pi\left|\boldsymbol{x}-\boldsymbol{x}'\right|}\delta\left(t-t'\mp\frac{1}{c}\left|\boldsymbol{x}-\boldsymbol{x}'\right|\right)\delta\left(\boldsymbol{x}'\right)i\left(t'\right)\\
 & = & \frac{1}{4\pi r}\int dt'\:\delta\left(t-t'\mp\frac{r}{c}\right)i\left(t'\right)\\
 & = & \underline{\frac{1}{4\pi r}i\left(t\pm\frac{r}{c}\right)}.
\end{eqnarray*}
 This implies that if the source $i\left(t'\right)$ does something
at a time $t'$, then the 4-potential response at position $\boldsymbol{x}$
occurs at a time
\[
t=t'\pm\frac{r}{c}
\]
 for the solutions $f_{\pm}$.\end{rem}
\begin{defn}
Define: 
\[
\begin{alignedat}{2}G_{+} & \textnormal{ as } & \textnormal{"retarded Green's function"}\\
G_{-} & \textnormal{ as } & \textnormal{"advanced Green's function"}
\end{alignedat}
\]

\end{defn}
\doublebox{\begin{minipage}[t]{1\columnwidth}%
\textbf{Axiom} \textbf{4.} Causality; a physical response cannot
precede the action of the source.
\begin{lyxlist}{00.00.0000}
\item [{\textbf{consequence:}}] only the retarded solution is physical.\end{lyxlist}
%
\end{minipage}}


\subsection{\label{sub:ch5sec2_3The-retarded-potentials}The retarded potentials}

We can obtain the potentials by applying the proposition from \cref{sub:ch5sec2_1The-concept-of}
and results from \cref{sub:ch5sec2_2Green's-functions-for} to the
wave equations for $\boldsymbol{A}$, $\phi$:
\[
\phi\left(\boldsymbol{x},t\right)=\int d\boldsymbol{x}'\:dt'\:\frac{1}{4\pi\left|\boldsymbol{x}-\boldsymbol{x}'\right|}\delta\left(t-t'-\frac{1}{c}\left|\boldsymbol{x}-\boldsymbol{x}'\right|\right)4\pi\rho\left(\boldsymbol{x}',t'\right)
\]
\[
\implies\boxed{\phi\left(\boldsymbol{x},t\right)=\indefint{\boldsymbol{y}}\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\rho\left(\boldsymbol{y},t-\frac{1}{c}\left|\boldsymbol{x}-\boldsymbol{y}\right|\right)}.\tag{\ensuremath{\ast}}
\]
 Analogously,
\[
\implies\boxed{\boldsymbol{A}\left(\boldsymbol{x},t\right)=\frac{1}{c}\indefint{\boldsymbol{y}}\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\boldsymbol{j}\left(\boldsymbol{y},t-\frac{1}{c}\left|\boldsymbol{x}-\boldsymbol{y}\right|\right)}.\tag{\ensuremath{\ast\ast}}
\]

\begin{rem}
$\left(*\right)$, $\left(**\right)$ are called \emph{retarded potentials}.
\end{rem}
~
\begin{rem}
The time delay $\Delta t=\frac{\left|\boldsymbol{x}-\boldsymbol{y}\right|}{c}$
corresponds to the time it takes the wave to travel from point $\boldsymbol{y}$
to $\boldsymbol{x}$ with velocity $c$.
\end{rem}
~
\begin{rem}
$\left(*\right)$, $\left(**\right)$ are analogous to Poisson's formula
in the static case (cf. \cref{chap:Static-solutions-of} \cref{sub:ch3sec3_6The-law-of}).
The new concept from time dependence is retardation from finite speed
of propagation.
\end{rem}

\section{\label{sec:ch5sec3Radiation-by-time-dependent}Radiation by time-dependent
sources}


\subsection{\label{sub:ch5sec3_1Asymptotic-potentials-and}Asymptotic potentials
and fields}

Consider retarded potentials (\cref{sub:ch5sec2_3The-retarded-potentials}
$\left(*\right)$, $\left(**\right)$) at large distances $r=\left|\boldsymbol{x}\right|$
from the sources.

We can expand $\left|\boldsymbol{x}-\boldsymbol{y}\right|$:
\begin{eqnarray*}
\left|\boldsymbol{x}-\boldsymbol{y}\right| & = & \sqrt{r^{2}-2\boldsymbol{x}\cdot\boldsymbol{y}+\boldsymbol{y}^{2}}\\
 & = & r\sqrt{1-2\frac{\hat{\boldsymbol{x}}\cdot\boldsymbol{y}}{r}+O\left(\frac{1}{r^{2}}\right)}\\
 & = & r-\hat{\boldsymbol{x}}\cdot\boldsymbol{y}+O\left(\frac{1}{r}\right),
\end{eqnarray*}
 where $\hat{\boldsymbol{x}}:=\frac{\boldsymbol{x}}{r}$. 
\[
\implies\boxed{\phi\left(\boldsymbol{x},t\right)=\frac{1}{r}\indefint{\boldsymbol{y}}\rho\left(\boldsymbol{y},t_{r}\right)}+O\left(\frac{1}{r^{2}}\right),
\]
 where $\underline{t_{r}:=t-\frac{1}{c}\left|\boldsymbol{x}-\boldsymbol{y}\right|\approx t-\frac{r}{c}+\frac{1}{c}\hat{\boldsymbol{x}}\cdot\boldsymbol{y}}$.
Analogously,
\[
\implies\boxed{\boldsymbol{A}\left(\boldsymbol{x},t\right)=\frac{1}{cr}\indefint{\boldsymbol{y}}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)}+O\left(\frac{1}{r^{2}}\right).
\]
 
\begin{rem}
We keep only leading terms for $r\rightarrow\infty$, which are of
$O\left(\frac{1}{r}\right)$.
\end{rem}
~
\begin{rem}
How many terms to keep in the time argument $t_{r}$ of $\rho$, $\boldsymbol{j}$
depends on how rapidly the sources are changing. If $L$ is the linear
dimension of the source, and the source changes appreciably on a time
scale $\Delta t=\frac{L}{c}$, then the term $\frac{1}{c}\hat{\boldsymbol{x}}\cdot\boldsymbol{y}$
may be important.
\end{rem}
Before deriving the asymptotic forms of $\boldsymbol{E}$, $\boldsymbol{B}$,
we prove two useful lemmas.
\begin{lem}
\[
\boxed{\nabla\frac{1}{r}f\left(t_{r}\right)=-\frac{1}{c}\hat{\boldsymbol{x}}\frac{1}{r}\partial_{t}f\left(t_{r}\right)}+O\left(\frac{1}{r^{2}}\right)
\]
 Note: In this section, $\partial_{t}f\left(t_{r}\right):=\left.\left(\partial_{t}f\right)\right|_{t_{r}}$
and $\nabla\frac{1}{r}f\left(t_{r}\right):=\nabla\left(\frac{1}{r}f\left(t_{r}\right)\right)$.\end{lem}
\begin{proof}
\begin{eqnarray*}
\nabla\frac{1}{r}f\left(t_{r}\right) & \overset{1.}{=} & \understuff{\left(\nabla\frac{1}{r}\right)f\left(t_{r}\right)}{O\left(1/r^{2}\right)}+\frac{1}{r}\nabla\left(f\left(t_{r}\right)\right)\\
 & \overset{2.}{=} & \frac{1}{r}\left(\partial_{t}f\right)\left(t_{r}\right)\nabla t_{r}+O\left(\frac{1}{r^{2}}\right)\\
 & \overset{3.}{=} & \frac{1}{r}\left(\partial_{t}f\right)\left(t_{r}\right)\left(-\frac{1}{c}\right)\nabla\sqrt{x^{2}+y^{2}+z^{2}}+O\left(\frac{1}{r^{2}}\right)\\
 & = & -\frac{1}{cr}\understuff{\frac{\boldsymbol{x}}{r}}{\hat{\boldsymbol{x}}}\partial_{t}f\left(t_{r}\right)+O\left(\frac{1}{r^{2}}\right)
\end{eqnarray*}

\begin{enumerate}
\item Product rule.
\item Chain rule. 
\item $\nabla t_{r}=-\frac{1}{c}\nabla\sqrt{x^{2}+y^{2}+z^{2}}$
\end{enumerate}
\end{proof}
\begin{lem}
\[
\boxed{\partial_{t}\rho\left(\boldsymbol{y},t_{r}\right)=-\nabla_{\boldsymbol{y}}\cdot\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)+\frac{1}{c}\hat{\boldsymbol{x}}\cdot\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)}+O\left(\frac{1}{r}\right)
\]
\end{lem}
\begin{proof}
By the continuity equation (\cref{chap:Maxwell's-Equations} \cref{sub:ch2sec2_1Continuity-equation-for}),
\[
\implies\partial_{t}\rho\left(\boldsymbol{x},t\right)=-\nabla_{\boldsymbol{x}}\cdot\boldsymbol{j}\left(\boldsymbol{x},t\right)
\]
\[
\implies\left(\partial_{t}\rho\left(\boldsymbol{y},t\right)\right)_{t=t_{r}}=-\left(\nabla_{\boldsymbol{y}}\cdot\boldsymbol{j}\left(\boldsymbol{y},t\right)\right)_{t=t_{r}}
\]
 But
\begin{eqnarray*}
\nabla_{\boldsymbol{y}}\cdot\left(\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)\right) & \overset{1.}{=} & \left(\nabla_{\boldsymbol{y}}\cdot\boldsymbol{j}\left(\boldsymbol{y},t\right)\right)_{t=t_{r}}+\left(\partial_{t}\boldsymbol{j}\right)\left(\boldsymbol{y},t_{r}\right)\cdot\nabla_{\boldsymbol{y}}t_{r}\\
 & \overset{2.}{=} & \left(\nabla_{\boldsymbol{y}}\cdot\boldsymbol{j}\left(\boldsymbol{y},t\right)\right)_{t=t_{r}}+\frac{1}{c}\hat{\boldsymbol{x}}\cdot\left(\partial_{t}\boldsymbol{j}\right)\left(\boldsymbol{y},t_{r}\right)+O\left(\frac{1}{r}\right)
\end{eqnarray*}
\begin{eqnarray*}
\implies\left(\partial_{t}\rho\left(\boldsymbol{y},t\right)\right)_{t=t_{r}}=:\partial_{t}\rho\left(\boldsymbol{y},t_{r}\right) & = & -\nabla_{\boldsymbol{y}}\cdot\left(\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)\right)+\frac{1}{c}\hat{\boldsymbol{x}}\cdot\left(\partial_{t}\boldsymbol{j}\right)\left(\boldsymbol{y},t_{r}\right)+O\left(\frac{1}{r}\right)\\
 & =: & -\nabla_{\boldsymbol{y}}\cdot\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)+\frac{1}{c}\hat{\boldsymbol{x}}\cdot\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)+O\left(\frac{1}{r}\right).
\end{eqnarray*}

\begin{enumerate}
\item Chain rule. 
\item Recall $t_{r}:=t-\frac{1}{c}\left|\boldsymbol{x}-\boldsymbol{y}\right|$
\begin{eqnarray*}
\implies\nabla_{\boldsymbol{y}}t_{r} & = & -\frac{1}{c}\nabla_{\boldsymbol{y}}\left|\boldsymbol{y}-\boldsymbol{x}\right|\\
 & = & -\frac{1}{c}\frac{\boldsymbol{y}-\boldsymbol{x}}{\left|\boldsymbol{y}-\boldsymbol{x}\right|}=\frac{1}{c}\frac{\boldsymbol{x}}{r\sqrt{1-2\hat{\boldsymbol{x}}\cdot\boldsymbol{y}+\frac{\boldsymbol{y^{2}}}{r^{2}}}}-\frac{\boldsymbol{y}}{r\sqrt{\cdots}}=\frac{1}{c}\hat{\boldsymbol{x}}\left(1+O\left(\frac{1}{r}\right)\right)+O\left(\frac{1}{r}\right).
\end{eqnarray*}

\end{enumerate}
\end{proof}
\begin{prop}
Far from the sources, the fields are
\[
\boxed{\begin{alignedat}{1}\boldsymbol{B}\left(\boldsymbol{x},t\right) & =-\frac{1}{c^{2}}\frac{\hat{\boldsymbol{x}}}{r}\times\indefint{\boldsymbol{y}}\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)\\
\boldsymbol{E}\left(\boldsymbol{x},t\right) & =-\hat{\boldsymbol{x}}\times\boldsymbol{B}\left(\boldsymbol{x},t\right)
\end{alignedat}
}\dots+O\left(\frac{1}{r^{2}}\right).
\]
\end{prop}
\begin{rem}
This implies $\boldsymbol{E}^{2}=\boldsymbol{B}^{2}$, and $\hat{\boldsymbol{x}}\perp\boldsymbol{E}\perp\boldsymbol{B}$,
forming a right-handed orthogonal set. 
\end{rem}
~
\begin{rem}
The fields fall off as $\frac{1}{r}$ as opposed to $\frac{1}{r^{2}}$
in static solutions.\end{rem}
\begin{proof}
\emph{(of proposition).}

From \cref{sub:ch5sec1_1Fields-and-potentials}, $\boldsymbol{B}=\nabla\times\boldsymbol{A}$,
so by the equation for asymptotic $\boldsymbol{A}$,
\begin{eqnarray*}
\implies B_{i} & = & \varepsilon_{ijk}\partial_{j}\frac{1}{r}\frac{1}{c}\indefint{\boldsymbol{y}}j_{k}\left(\boldsymbol{y},t_{r}\right)\\
 & \overset{1.}{=} & \varepsilon_{ijk}\left(-\frac{1}{c^{2}}\right)\hat{x}_{j}\frac{1}{r}\indefint{\boldsymbol{y}}\left(\partial_{t}j_{k}\left(\boldsymbol{y},t\right)\right)_{t=t_{r}}\\
\implies\boldsymbol{B}\left(\boldsymbol{x},t\right) & = & -\frac{1}{c^{2}}\frac{\hat{\boldsymbol{x}}}{r}\times\indefint{\boldsymbol{y}}\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)
\end{eqnarray*}

\begin{enumerate}
\item By lemma 1, $\partial_{j}\frac{1}{r}f\left(t_{r}\right)=-\frac{1}{c}\hat{x}_{j}\frac{1}{r}\partial_{t}f\left(t_{r}\right)$.
\end{enumerate}
From \cref{sub:ch5sec1_1Fields-and-potentials}, $\boldsymbol{E}=-\nabla\phi-\frac{1}{c}\partial_{t}\boldsymbol{A}$,
so by the equations for asymptotic $\phi$, $\boldsymbol{A}$,
\begin{eqnarray*}
\implies\boldsymbol{E} & = & -\nabla\frac{1}{r}\indefint{\boldsymbol{y}}\rho\left(\boldsymbol{y},t_{r}\right)-\frac{1}{c}\partial_{t}\frac{1}{cr}\indefint{\boldsymbol{y}}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)\\
 & \overset{1.}{=} & \frac{1}{c}\frac{\hat{\boldsymbol{x}}}{r}\indefint{\boldsymbol{y}}\partial_{t}\rho\left(\boldsymbol{y},t_{r}\right)-\frac{1}{c^{2}r}\indefint{\boldsymbol{y}}\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)\\
 & \overset{2.}{=} & -\frac{1}{c}\frac{\hat{\boldsymbol{x}}}{r}\understuff{\indefint{\boldsymbol{y}}\nabla_{\boldsymbol{y}}\cdot\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)}{=\int_{\mathbb{R}^{3}}d\boldsymbol{S}\cdot\boldsymbol{j}\rightarrow0}+\frac{1}{c}\frac{\hat{\boldsymbol{x}}}{r}\indefint{\boldsymbol{y}}\frac{1}{c}\hat{\boldsymbol{x}}\cdot\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)-\frac{1}{c^{2}r}\indefint{\boldsymbol{y}}\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)\\
 & = & \frac{1}{c^{2}r}\indefint{\boldsymbol{y}}\left[\hat{\boldsymbol{x}}\left(\hat{\boldsymbol{x}}\cdot\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)\right)-\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)\right]\\
 & \overset{3.}{=} & \frac{1}{c^{2}r}\indefint{\boldsymbol{y}}\hat{\boldsymbol{x}}\times\left(\hat{\boldsymbol{x}}\times\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)\right)\\
 & = & -\hat{\boldsymbol{x}}\times\boldsymbol{B}\left(\boldsymbol{x},t\right).
\end{eqnarray*}

\begin{enumerate}
\item By lemma 1, and since $\partial_{t}=\partial_{t_{r}}$. 
\item By lemma 2.
\item Using the vector identity:
\begin{eqnarray*}
\left(\boldsymbol{a}\times\left(\boldsymbol{a}\times\boldsymbol{b}\right)\right)_{i} & = & \varepsilon_{ijk}a_{j}\varepsilon_{klm}a_{l}b_{m}\\
 & = & \left(\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}\right)a_{j}a_{l}b_{m}\\
 & = & a_{i}\left(\boldsymbol{a}\cdot\boldsymbol{b}\right)-\boldsymbol{a}^{2}b_{i}
\end{eqnarray*}

\end{enumerate}
\end{proof}
\begin{rem}
A time-dependent localized current density leads to time-dependent
fields everywhere in space (with proper retardation to account for
signal travel time). This phenomenon is called \emph{radiation}. 
\end{rem}
~
\begin{rem}
Far from the source, the radiation fields $\boldsymbol{E}$, $\boldsymbol{B}$
...

(i)\hphantom{(ii)}\quad{}falls off as $\frac{1}{r}$

(ii)\hphantom{(i)}\quad{}are perpendicular to one another and to
the radius vector from source to observer (because we are far enough
away that the waves are approximately plane waves).
\end{rem}
~
\begin{rem}
The source must provide the field energy; there is steady power loss
at the source.
\end{rem}

\subsection{\label{sub:ch5sec3_2The-radiated-power}The radiated power}

From \cref{chap:Maxwell's-Equations} \cref{sub:ch2sec3_6Poynting's-theorem},
the energy-current density of the fields is given by the Poynting
vector:
\[
\underline{\boldsymbol{P}\left(\boldsymbol{x},t\right)=\frac{c}{4\pi}\boldsymbol{E}\left(\boldsymbol{x},t\right)\times\boldsymbol{B}\left(\boldsymbol{x},t\right)}.
\]

\begin{rem}
$\boldsymbol{E}\perp\boldsymbol{B}\perp\hat{\boldsymbol{x}}\implies\boldsymbol{P}\parallel\hat{\boldsymbol{x}}$.
\end{rem}
~
\begin{rem}
$\left[\boldsymbol{P}\right]=\textnormal{energy per time and area}=\si{erg.cm^{-2}.s^{-1}}=\si{g.cm.s^{-3}}.$
\end{rem}
~
\begin{rem}
$\hat{\boldsymbol{x}}\cdot\boldsymbol{P}=\textnormal{power per unit area}$.
Denote by $\mathcal{\mathscr{P}}$ the total radiated power. Then
the power radiated per solid angle is given by
\begin{eqnarray*}
\frac{d\mathscr{P}}{d\Omega} & = & \hat{\boldsymbol{x}}\cdot\boldsymbol{P}dA\\
 & = & \left(\hat{\boldsymbol{x}}\cdot\frac{c}{4\pi}\boldsymbol{E}\times\boldsymbol{B}\right)\left(r^{2}d\Omega\right)\\
 & = & \frac{c}{4\pi}r^{2}\hat{\boldsymbol{x}}\cdot\left(\boldsymbol{B}\times\left(\hat{\boldsymbol{x}}\times\boldsymbol{B}\right)\right)
\end{eqnarray*}
But 
\begin{eqnarray*}
\hat{\boldsymbol{x}}\cdot\left(\boldsymbol{B}\times\left(\hat{\boldsymbol{x}}\times\boldsymbol{B}\right)\right) & = & \hat{x}_{i}\varepsilon_{ijk}B_{j}\varepsilon_{klm}\hat{x}_{l}B_{m}\\
 & = & \left(\delta_{il}\delta_{jm}-\delta_{im}\delta_{jl}\right)\hat{x}_{i}B_{j}\hat{x}_{l}B_{m}\\
 & = & \boldsymbol{B}^{2}-\understuff{\bcancel{\left(\boldsymbol{B}\cdot\hat{\boldsymbol{x}}\right)^{2}}}{\boldsymbol{B}\perp\hat{\boldsymbol{x}}}\\
 & = & \boldsymbol{B}^{2}
\end{eqnarray*}
\begin{eqnarray*}
\implies\frac{d\mathscr{P}}{d\Omega} & = & \frac{c}{4\pi}r^{2}\boldsymbol{B}^{2}\\
 & = & \frac{c}{4\pi}\cancel{r^{2}}\left(\frac{1}{c^{2}\cancel{r}}\right)^{2}\left(\hat{\boldsymbol{x}}\times\indefint{\boldsymbol{y}}\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)\right)^{2}\\
 & = & \frac{1}{4\pi c^{3}}\left(\hat{\boldsymbol{x}}\times\indefint{\boldsymbol{y}}\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)\right)^{2}
\end{eqnarray*}
\end{rem}
\begin{thm}
The power radiated by the source per solid angle is
\[
\boxed{\frac{d\mathscr{P}}{d\Omega}=\frac{1}{4\pi c^{3}}\left(\hat{\boldsymbol{x}}\times\indefint{\boldsymbol{y}}\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)\right)^{2}}.
\]
\end{thm}
\begin{proof}
\emph{(above).}\end{proof}
\begin{rem}
$\textnormal{Power}\propto\left(\textnormal{fields}\right)^{2},$
and $\textnormal{fields}\propto\frac{1}{r}$: there is nonzero power
per solid angle even as $r\rightarrow\infty$.\end{rem}
\begin{cor}
The total power radiated is
\[
\boxed{\mathscr{P}=\int d\Omega\frac{d\mathscr{P}}{d\Omega}}.
\]

\end{cor}

\subsection{\label{sub:ch5sec3_3Radiation-by-an}Radiation by an accelerated
charged point particle}

Consider a point particle with charge $e$ moving with non-relativistic
velocity $v\ll c$, on a trajectory $\boldsymbol{R}\left(t\right)$.
\begin{lyxlist}{00.00.0000}
\item [{\textbf{current~density:}}] $\underline{\boldsymbol{j}\left(\boldsymbol{y},t\right)=e\boldsymbol{v}\left(t\right)\delta\left(\boldsymbol{y}-\boldsymbol{R}\left(t\right)\right)}\quad\textnormal{ where }\boldsymbol{v}\left(t\right):=\dot{\boldsymbol{R}}\left(t\right)$
\item [{\textbf{retarded~time:}}] 
\begin{eqnarray*}
\underline{t_{r}} & := & t-\frac{1}{c}\left|\boldsymbol{x}-\boldsymbol{y}\right|\\
 & \overset{1.}{\approx} & t-\frac{r}{c}+\frac{1}{c}\hat{\boldsymbol{x}}\cdot\boldsymbol{R}\left(t'\right)\\
 & \overset{2.}{\approx} & \underline{t-\frac{r}{c}=:t_{e}}
\end{eqnarray*}
\end{lyxlist}
\begin{enumerate}
\item $t'$ is the solution of $t'=t-\frac{1}{c}\left|\boldsymbol{x}-\boldsymbol{R}\left(t'\right)\right|$
(from \cref{sub:ch5sec2_3The-retarded-potentials})
\item $\frac{1}{c}\hat{\boldsymbol{x}}\cdot\boldsymbol{R}\left(t'\right)$
is small of order $\frac{v}{c}$ if $v\ll c$. \end{enumerate}
\begin{rem}
$t_{e}$ is the time of emission for a signal received at time $t$.
\end{rem}
To find the power radiated, we will need the following quantity:
\begin{eqnarray*}
\underline{\indefint{\boldsymbol{y}}\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)} & \approx & \indefint{\boldsymbol{y}}\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{e}\right)\\
 & = & \frac{d}{dt}e\indefint{\boldsymbol{y}}\boldsymbol{v}\left(t_{e}\right)\delta\left(\boldsymbol{y}-\boldsymbol{R}\left(t_{e}\right)\right)\\
 & = & e\left.\frac{d\boldsymbol{v}}{dt}\right|_{t=t_{e}}\\
 & = & \underline{e\dot{\boldsymbol{v}}\left(t_{e}\right)}.
\end{eqnarray*}
Inserting into $\left(\hat{\boldsymbol{x}}\times\indefint{\boldsymbol{y}}\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right)\right)^{2}$
yields (disregarding $e$): 
\begin{eqnarray*}
\left(\hat{\boldsymbol{x}}\times\dot{\boldsymbol{v}}\left(t_{e}\right)\right)^{2} & = & \varepsilon_{ijk}\hat{x}_{j}\dot{v}_{k}\varepsilon_{ilm}\hat{x}_{l}\dot{v}_{m}\\
 & = & \left(\delta_{jl}\delta_{km}-\delta_{jm}\delta_{kl}\right)\hat{x}_{j}\hat{x}_{l}\dot{v}_{k}\dot{v}_{m}\\
 & = & \dot{\boldsymbol{v}}^{2}-\left(\hat{\boldsymbol{x}}\cdot\dot{\boldsymbol{v}}\right)^{2}
\end{eqnarray*}
\[
\implies\underline{\frac{d\mathscr{P}}{d\Omega}=\frac{e^{2}}{4\pi c^{3}}\left[\left(\dot{\boldsymbol{v}}\left(t_{e}\right)\right)^{2}-\left(\hat{\boldsymbol{x}}\cdot\dot{\boldsymbol{v}}\left(t_{e}\right)\right)^{2}\right]}.
\]
 Let $\theta$ be the angle between the acceleration at time $t_{e}$
and the radius vector to the observer.
\[
\implies\hat{\boldsymbol{x}}\cdot\dot{\boldsymbol{v}}\left(t_{e}\right)=\dot{\boldsymbol{v}}^{2}\cos^{2}\theta
\]
\[
\implies\boxed{\frac{d\mathscr{P}}{d\Omega}=\frac{e^{2}}{4\pi c^{3}}\left(\dot{\boldsymbol{v}}\left(t_{e}\right)\right)^{2}\sin^{2}\left[\theta\left(t_{e}\right)\right]}.
\]

\begin{prop}
\textbf{\emph{Larmor formula. }}The total power radiated by the accelerated
charge is
\[
\boxed{\mathscr{P}=\frac{2e^{2}}{3c^{3}}\dot{\boldsymbol{v}}^{2}}\quad\left(\textnormal{for }v\ll c\right).
\]
 This is called the \emph{Larmor formula}.\end{prop}
\begin{proof}
$\indefint{\Omega}\sin^{2}\theta=2\pi\int_{-1}^{1}d\eta\:\left(1-\eta^{2}\right)=\frac{8\pi}{3}$.\end{proof}
\begin{rem}
This is called the \emph{Larmor formula}, valid for non-relativistic
particles.
\end{rem}
~
\begin{rem}
This is the physics behind synchrotron radiation (see Problem \#46).
\end{rem}
~
\begin{rem}
This implies that a classical atom cannot be stable (see Problems
\#47, 48).
\end{rem}

\subsection{\label{sub:ch5sec3_4Dipole-radiation}Dipole radiation}

Consider a system of many slow moving ($v\ll c$) charges that is
still small compared to $r$. We will still use the approximation
$t_{r}\approx t_{e}$.
\begin{prop}
In this situation the radiated power per solid angle is
\[
\boxed{\frac{d\mathscr{P}}{d\Omega}=\frac{1}{4\pi c^{3}}\left(\hat{\boldsymbol{x}}\times\ddot{\boldsymbol{d}}\right)^{2}},
\]
 where $\boldsymbol{d}$ is the dipole moment of the charge distribution,
given by
\[
\boldsymbol{d}\left(t\right):=\indefint{\boldsymbol{y}}\boldsymbol{y}\rho\left(\boldsymbol{y},t\right)
\]
 and $\ddot{\boldsymbol{d}}$ is its second time derivative.\end{prop}
\begin{rem}
With $\theta$ the angle between $\ddot{\boldsymbol{d}}$ and $\hat{\boldsymbol{x}}$,
this becomes
\[
\underline{\frac{d\mathscr{P}}{d\Omega}=\frac{1}{4\pi c^{3}}\sin^{2}\theta\left(\ddot{\boldsymbol{d}}\right)^{2}}.
\]

\end{rem}
~
\begin{rem}
For one point charge, $\rho\left(\boldsymbol{y},t\right)=e\delta\left(\boldsymbol{y}-\boldsymbol{R}\left(t\right)\right)$
\[
\implies\boldsymbol{d}\left(t\right)=\indefint{\boldsymbol{y}}\boldsymbol{y}e\delta\left(\boldsymbol{y}-\boldsymbol{R}\left(t\right)\right)=e\boldsymbol{R}\left(t\right)
\]
\[
\implies\ddot{\boldsymbol{d}}=e\dot{\boldsymbol{v}},
\]
 so it works for one particle.\end{rem}
\begin{lem}
\[
\underline{\frac{d}{dt}\boldsymbol{d}\left(t\right)=\indefint{\boldsymbol{y}}\boldsymbol{j}\left(\boldsymbol{y},t\right)}
\]
\end{lem}
\begin{proof}
Charge conservation implies
\[
\partial_{t}\rho+\nabla\cdot\boldsymbol{j}=0.
\]
 Integrating over space,
\begin{eqnarray*}
\implies0 & = & \indefint{\boldsymbol{y}}\boldsymbol{y}\left[\nabla_{\boldsymbol{y}}\cdot\boldsymbol{j}\left(\boldsymbol{y},t\right)+\partial_{t}\rho\left(\boldsymbol{y},t\right)\right]\\
 & \overset{1.}{=} & \indefint{\boldsymbol{y}}\left[\nabla_{\boldsymbol{y}}\left(\boldsymbol{y}\cdot\boldsymbol{j}\right)-\boldsymbol{j}+\boldsymbol{y}\partial_{t}\rho\left(\boldsymbol{y},t\right)\right]\\
 & \overset{2.}{=} & -\indefint{\boldsymbol{y}}\boldsymbol{j}\left(\boldsymbol{y},t\right)+\frac{d}{dt}\indefint{\boldsymbol{y}}\boldsymbol{y}\rho\left(\boldsymbol{y},t\right)\\
 & = & -\indefint{\boldsymbol{y}}\boldsymbol{j}\left(\boldsymbol{y},t\right)+\frac{d}{dt}\boldsymbol{d}\left(t\right).
\end{eqnarray*}

\begin{enumerate}
\item Product rule
\item $\indefint{\boldsymbol{y}}\nabla_{\boldsymbol{y}}\left(\boldsymbol{y}\cdot\boldsymbol{j}\right)\rightarrow0$
if $\boldsymbol{y}\cdot\boldsymbol{j}$ falls off fast enough.
\end{enumerate}
\end{proof}
~
\begin{proof}
\emph{(of proposition)}

From \cref{sub:ch5sec3_2The-radiated-power}, 
\begin{eqnarray*}
4\pi c^{3}\frac{d\mathscr{P}}{d\Omega} & \overset{1.}{\approx} & \left(\hat{\boldsymbol{x}}\times\indefint{\boldsymbol{y}}\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t_{e}\right)\right)^{2}\\
 & = & \left(\hat{\boldsymbol{x}}\times\frac{d}{dt}\indefint{\boldsymbol{y}}\boldsymbol{j}\left(\boldsymbol{y},t_{e}\right)\right)^{2}\\
 & \overset{2.}{=} & \left(\hat{\boldsymbol{x}}\times\frac{d}{dt}\left(\frac{d}{dt}\boldsymbol{d}\right)\right)^{2}\\
 & = & \left(\hat{\boldsymbol{x}}\times\ddot{\boldsymbol{d}}\right)^{2}
\end{eqnarray*}

\begin{enumerate}
\item Replacing $t_{r}$ with $t_{e}$.
\item From the lemma above.
\end{enumerate}
\end{proof}
\begin{rem}
This contribution is called \emph{electric dipole radiation}.
\end{rem}
Now we keep corrections to the approximation we made ($t_{r}\approx t_{e}$).
From \cref{sub:ch5sec3_2The-radiated-power}, to find $\frac{d\mathscr{P}}{d\Omega}$
we need
\begin{eqnarray*}
\indefint{\boldsymbol{y}}\boldsymbol{j}\left(\boldsymbol{y},t_{r}\right) & = & \indefint{\boldsymbol{y}}\boldsymbol{j}\Bigl(\boldsymbol{y},\understuff{t-\frac{r}{c}}{t_{e}}+\frac{1}{c}\hat{\boldsymbol{x}}\cdot\boldsymbol{y}+\dots\Bigr)\\
 & \overset{1.}{\approx} & \indefint{\boldsymbol{y}}\boldsymbol{j}\left(\boldsymbol{y},t_{e}\right)+\frac{1}{c}\indefint{\boldsymbol{y}}\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{y}\right)\left.\partial_{t}\boldsymbol{j}\left(\boldsymbol{y},t\right)\right|_{t=t_{e}}\\
 & \begin{array}{c}
\overset{2.}{=}\\
\\
\end{array} & \begin{array}{cccccc}
\dot{\boldsymbol{d}}\left(t_{e}\right)+\frac{1}{c}\frac{d}{dt}\Bigr|_{t_{e}}\int d\boldsymbol{y} & \Bigr[ & \frac{1}{2}\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{y}\right)\boldsymbol{j} & + & \frac{1}{2}\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{j}\right)\boldsymbol{y}\\
 & + & \frac{1}{2}\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{y}\right)\boldsymbol{j} & - & \frac{1}{2}\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{j}\right)\boldsymbol{y} & \Bigl]
\end{array}\\
 & = & \dot{\boldsymbol{d}}\left(t_{e}\right)-\frac{1}{2c}\frac{d}{dt}\Bigr|_{t_{e}}\indefint{\boldsymbol{y}}\left[\boldsymbol{y}\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{j}\right)-\boldsymbol{j}\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{y}\right)\right]+\left(\textnormal{other term}\right)\\
 & \overset{3.}{=} & \dot{\boldsymbol{d}}\left(t_{e}\right)-\frac{1}{2c}\frac{d}{dt}\Bigr|_{t_{e}}\indefint{\boldsymbol{y}}\hat{\boldsymbol{x}}\times\left(\boldsymbol{y}\times\boldsymbol{j}\right)+\left(\textnormal{other term}\right)\\
 & = & \dot{\boldsymbol{d}}\left(t_{e}\right)-\hat{\boldsymbol{x}}\times\frac{d}{dt}\Bigr|_{t_{e}}\frac{1}{2c}\indefint{\boldsymbol{y}}\boldsymbol{y}\times\boldsymbol{j}\left(\boldsymbol{y},t\right)+\left(\textnormal{other term}\right)\\
 & \overset{4.}{=} & \dot{\boldsymbol{d}}\left(t_{e}\right)-\hat{\boldsymbol{x}}\times\frac{d}{dt}\Bigr|_{t_{e}}\boldsymbol{m}+\left(\textnormal{other term}\right)\\
 & \overset{5.}{=} & \dot{\boldsymbol{d}}\left(t_{e}\right)-\hat{\boldsymbol{x}}\times\dot{\boldsymbol{m}}\left(t_{e}\right)+\left(\textnormal{other term}\right)
\end{eqnarray*}

\begin{enumerate}
\item Taylor expanded.
\item Used the lemma to replace $\indefint{\boldsymbol{y}}\boldsymbol{j}\left(\boldsymbol{y},t_{e}\right)$,
split the integrand $\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{y}\right)\boldsymbol{j}$
into $\frac{1}{2}\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{y}\right)\boldsymbol{j}+\frac{1}{2}\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{y}\right)\boldsymbol{j}$,
and added to the integrand $0=\frac{1}{2}\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{j}\right)\boldsymbol{y}-\frac{1}{2}\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{j}\right)\boldsymbol{y}$.
\item Used vector identity $\boldsymbol{a}\times\left(\boldsymbol{b}\times\boldsymbol{c}\right)=\boldsymbol{b}\left(\boldsymbol{a}\cdot\boldsymbol{c}\right)-\boldsymbol{c}\left(\boldsymbol{a}\cdot\boldsymbol{b}\right)$.
\item By definition (Ch \ref{chap:Static-solutions-of} \cref{sub:ch3sec4_7The-magnetic-moment}),
the magnetic dipole moment is $\boldsymbol{m}\left(t\right):=\frac{1}{2c}\indefint{\boldsymbol{y}}\boldsymbol{y}\times\boldsymbol{j}\left(\boldsymbol{y},t\right)$.
\item In this and the following sections, we use the notation $\frac{d}{dt}\Bigr|_{t_{e}}\boldsymbol{m}=:\dot{\boldsymbol{m}}\left(t_{e}\right)$.
\end{enumerate}
Therefore, in this approximation, the power per solid angle is
\[
\boxed{\frac{d\mathscr{P}}{d\Omega}=\frac{1}{4\pi c^{3}}\left[\hat{\boldsymbol{x}}\times\left(\ddot{\boldsymbol{d}}-\hat{\boldsymbol{x}}\times\ddot{\boldsymbol{m}}\right)\right]^{2}},
\]
 with $\boldsymbol{d}$ and $\boldsymbol{m}$ the electric and magnetic
(respectively) dipole moments of the source. Note that the ``other
term'' in the proof that we have neglected is of the same order as
the $\hat{\boldsymbol{x}}\times\left(\hat{\boldsymbol{x}}\times\ddot{\boldsymbol{m}}\right)$
term in $v/c$. We will discuss this later.
\begin{cor}
The total radiated power is
\[
\boxed{\mathscr{P}=\frac{2}{3c^{3}}\left[\left(\ddot{\boldsymbol{d}}\right)^{2}+\left(\ddot{\boldsymbol{m}}\right)^{2}\right]}.
\]
\end{cor}
\begin{proof}
\begin{eqnarray*}
4\pi c^{3}\mathscr{P} & = & 4\pi c^{3}\int d\Omega\frac{d\mathscr{P}}{d\Omega}\\
 & = & \int d\Omega\left[\hat{\boldsymbol{x}}\times\left(\ddot{\boldsymbol{d}}-\hat{\boldsymbol{x}}\times\ddot{\boldsymbol{m}}\right)\right]^{2}\\
 & = & \int d\Omega\left[\left(\hat{\boldsymbol{x}}\times\ddot{\boldsymbol{d}}\right)^{2}-2\left(\hat{\boldsymbol{x}}\times\ddot{\boldsymbol{d}}\right)\cdot\left(\hat{\boldsymbol{x}}\times\left(\hat{\boldsymbol{x}}\times\ddot{\boldsymbol{m}}\right)\right)+\left(\hat{\boldsymbol{x}}\times\left(\hat{\boldsymbol{x}}\times\ddot{\boldsymbol{m}}\right)\right)^{2}\right]
\end{eqnarray*}
We consider these terms separately:
\begin{eqnarray*}
\underline{\int d\Omega\left(\hat{\boldsymbol{x}}\times\ddot{\boldsymbol{d}}\right)^{2}} & \overset{1.}{=} & 2\pi\int_{-1}^{1}d\eta\left(1-\eta^{2}\right)\ddot{\boldsymbol{d}}^{2}\\
 & = & 4\pi\left(1-\frac{1}{3}\right)\ddot{\boldsymbol{d}}^{2}\\
 & = & \underline{\frac{8\pi}{3}\ddot{\boldsymbol{d}}^{2}}
\end{eqnarray*}

\begin{enumerate}
\item Choosing our coordinate system such that $\ddot{\boldsymbol{d}}\parallel\hat{\boldsymbol{z}}$
(using notation $\eta:=\cos\theta$).
\end{enumerate}
\begin{eqnarray*}
\underline{\int d\Omega\left(\hat{\boldsymbol{x}}\times\left(\hat{\boldsymbol{x}}\times\ddot{\boldsymbol{m}}\right)\right)^{2}} & \overset{1.}{=} & \int d\Omega\left[\hat{\boldsymbol{x}}\left(\hat{\boldsymbol{x}}\cdot\ddot{\boldsymbol{m}}\right)-\ddot{\boldsymbol{m}}\right]^{2}\\
 & \overset{2.}{=} & \int d\Omega\left[\eta^{2}\ddot{\boldsymbol{m}}^{2}-2\eta^{2}\ddot{\boldsymbol{m}}^{2}+\ddot{\boldsymbol{m}}^{2}\right]^{2}\\
 & = & 2\pi\int_{-1}^{1}d\eta\left(1-\eta^{2}\right)\ddot{\boldsymbol{m}}^{2}\\
 & = & \underline{\frac{8\pi}{3}\ddot{\boldsymbol{m}}^{2}}
\end{eqnarray*}

\begin{enumerate}
\item Vector identity $\boldsymbol{a}\times\left(\boldsymbol{b}\times\boldsymbol{c}\right)=\boldsymbol{b}\left(\boldsymbol{a}\cdot\boldsymbol{c}\right)-\boldsymbol{c}\left(\boldsymbol{a}\cdot\boldsymbol{b}\right)$.
\item Choosing our coordinate system such that $\ddot{\boldsymbol{m}}\parallel\hat{\boldsymbol{z}}$
(using notation $\eta:=\cos\theta$).
\end{enumerate}
\[
\int d\Omega\left(\hat{\boldsymbol{x}}\times\ddot{\boldsymbol{d}}\right)\cdot\left(\hat{\boldsymbol{x}}\times\left(\hat{\boldsymbol{x}}\times\ddot{\boldsymbol{m}}\right)\right)=0,
\]
 since the integral is odd in $\eta$ (to see this, let $\hat{\boldsymbol{x}}\rightarrow-\hat{\boldsymbol{x}}$).
\end{proof}
Now, what of the other term we've been ignoring?
\begin{rem}
The other term, given by
\[
\dots+\frac{1}{2c}\frac{d}{dt}\Bigr|_{t_{e}}\indefint{\boldsymbol{y}}\left[\boldsymbol{y}\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{j}\right)+\boldsymbol{j}\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{y}\right)\right]+\dots
\]
 has the structure (for the $i^{\textnormal{th}}$ component): 
\begin{eqnarray*}
\indefint{\boldsymbol{y}}\left(y_{i}j_{j}+j_{i}y_{j}\right) & \overset{1.}{=} & -\indefint{\boldsymbol{y}}y_{i}y_{j}\nabla_{\boldsymbol{y}}\cdot\boldsymbol{j}\\
 & \overset{2.}{=} & \indefint{\boldsymbol{y}}y_{i}y_{j}\partial_{t}\rho\\
 & = & \frac{d}{dt}\indefint{\boldsymbol{y}}y_{i}y_{j}\rho\left(\boldsymbol{y},t\right)\\
 & = & \frac{d}{dt}Q_{ij}\left(t\right),
\end{eqnarray*}
where $Q_{ij}\left(t\right):=\indefint{\boldsymbol{y}}y_{i}y_{j}\rho\left(\boldsymbol{y},t\right)$
is the electric quadrupole moment of the charge distribution.\end{rem}
\begin{enumerate}
\item Integrating by parts ``in reverse''. 
\item Continuity equation.
\end{enumerate}
Thus, the contribution to $\mathscr{P}$ from this term is of order
$\frac{1}{c^{5}}\dddot{Q}^{2}$.
\begin{rem}
The magnetic dipole moment has an extra $1/c$ in its definition.
Thus, the magnetic dipole and electric quadrupole radiation terms
are of the same order in $v/c$ (see Landau \& Lifshitz 71).
\end{rem}

\section{Spectral distribution of radiated energy}

In \ref{sec:ch5sec3Radiation-by-time-dependent} we calculated the
\emph{total} power radiated by a time-dependent source.
\begin{description}
\item [{question:}] How is this energy distributed over different frequencies?
\end{description}

\subsection{Retarded potentials in frequency space}

From \cref{sub:ch5sec2_3The-retarded-potentials},
\[
\phi\left(\boldsymbol{x},t\right)=\indefint{\boldsymbol{y}}\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\rho\left(\boldsymbol{y},t-\frac{\left|\boldsymbol{x}-\boldsymbol{y}\right|}{c}\right).
\]
 Define a temporal Fourier transform (cf. \cref{sub:ch5sec2_2Green's-functions-for})\footnote{In this notation, the argument of the function indicates if it is
a Fourier transform or not (the same symbol $f$ is used to refer
to the Fourier transformed function and the original function).}
\begin{eqnarray*}
f\left(\boldsymbol{x},\omega\right) & := & \indefint te^{i\omega t}f\left(\boldsymbol{x},t\right)\\
\implies f\left(\boldsymbol{x},t\right) & = & \indefintfrac{\omega}{2\pi}e^{-i\omega t}f\left(\boldsymbol{x},\omega\right).
\end{eqnarray*}
\begin{eqnarray*}
\implies\underline{\phi\left(\boldsymbol{x},\omega\right)} & = & \indefint te^{i\omega t}\indefint{\boldsymbol{y}}\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\understuff{\indefintfrac{\omega'}{2\pi}e^{-i\omega'\left(t-\left|\boldsymbol{x}-\boldsymbol{y}\right|/c\right)}\rho\left(\boldsymbol{y},\omega'\right)}{=\rho\left(\boldsymbol{y},t-\left|\boldsymbol{x}-\boldsymbol{y}\right|/c\right)}\\
 & = & \indefint{\boldsymbol{y}}\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}\indefintfrac{\omega'}{2\pi}\rho\left(\boldsymbol{y},\omega'\right)\understuff{\indefint te^{i\left(\omega-\omega'\right)t}}{=2\pi\delta\left(\omega-\omega'\right)}e^{i\omega'\left|\boldsymbol{x}-\boldsymbol{y}\right|/c}\\
 & = & \underline{\indefint{\boldsymbol{y}}\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}e^{i\omega\left|\boldsymbol{x}-\boldsymbol{y}\right|/c}\rho\left(\boldsymbol{y},\omega\right)}
\end{eqnarray*}

\begin{prop}
The retarded potentials in frequency space are
\[
\boxed{\phi\left(\boldsymbol{x},\omega\right)=\indefint{\boldsymbol{y}}\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}e^{i\omega\left|\boldsymbol{x}-\boldsymbol{y}\right|/c}\rho\left(\boldsymbol{y},\omega\right)}
\]
 and, analogously,
\[
\boxed{\boldsymbol{A}\left(\boldsymbol{x},\omega\right)=\frac{1}{c}\indefint{\boldsymbol{y}}\frac{1}{\left|\boldsymbol{x}-\boldsymbol{y}\right|}e^{i\omega\left|\boldsymbol{x}-\boldsymbol{y}\right|/c}\boldsymbol{j}\left(\boldsymbol{y},\omega\right)},
\]
 where $\rho\left(\boldsymbol{y},\omega\right)$ and $\boldsymbol{j}\left(\boldsymbol{y},\omega\right)$
are the temporal Fourier transforms of $\rho\left(\boldsymbol{y},t\right)$
and $\boldsymbol{j}\left(\boldsymbol{y},t\right)$, respectively.\end{prop}
\begin{proof}
(above)
\end{proof}

\subsection{\label{sub:ch5sec4_2Asymptotic-potentials-and}Asymptotic potentials
and fields}

For large distances $r:=\left|\boldsymbol{x}\right|$ from the sources,
the expansion from \cref{sub:ch5sec3_1Asymptotic-potentials-and}
applies:
\[
\left|\boldsymbol{x}-\boldsymbol{y}\right|\approx r-\hat{\boldsymbol{x}}\cdot\boldsymbol{y}
\]
\begin{eqnarray*}
\implies\phi\left(\boldsymbol{x},\omega\right) & = & \indefint{\boldsymbol{y}}\frac{1}{r}\left[1+O\left(\frac{1}{r}\right)\right]e^{i\omega\left(r-\hat{\boldsymbol{x}}\cdot\boldsymbol{y}+\dots\right)/c}\rho\left(\boldsymbol{y},\omega\right)\\
 & \approx & \frac{1}{r}e^{i\omega r/c}\indefint{\boldsymbol{y}}e^{-i\omega\hat{\boldsymbol{x}}\cdot\boldsymbol{y}/c}\rho\left(\boldsymbol{y},\omega\right)+O\left(1/r^{2}\right)
\end{eqnarray*}

\begin{description}
\item [{definition:}] $\boxed{\boldsymbol{k}:=\frac{\omega}{c}\hat{\boldsymbol{x}}}$
is called \emph{wave vector}.\end{description}
\begin{rem}
This is consistent with Ch. \ref{chap:Electromagnetic-waves-in} \cref{sub:ch4sec1_5Polarization-of-electromagnetic}
\cref{rem:ch4sec1_5rem1The-direction-of}.
\end{rem}
~
\begin{rem}
Far from the source, the wave fronts are approximately plane waves,
so Ch. \ref{chap:Electromagnetic-waves-in} applies.
\end{rem}
\begin{eqnarray*}
\implies\underline{\phi\left(\boldsymbol{x},\omega\right)} & \approx & \frac{1}{r}e^{ikr}\indefint{\boldsymbol{y}}e^{-i\boldsymbol{k}\cdot\boldsymbol{y}}\rho\left(\boldsymbol{y},\omega\right)\\
 & = & \underline{\frac{1}{r}e^{ikr}\rho\left(\boldsymbol{k},\omega\right)},
\end{eqnarray*}
with $\rho\left(\boldsymbol{k},\omega\right)$ the spacial Fourier
transform of $\rho\left(\boldsymbol{x},\omega\right)$, and $k:=\left|\boldsymbol{k}\right|$.
Note that $\frac{1}{r}e^{ikr}$ represents a spherical wave. 

Analogously,
\[
\underline{\boldsymbol{A}\left(\boldsymbol{x},\omega\right)}\approx\underline{\frac{1}{r}e^{ikr}\frac{1}{c}\boldsymbol{j}\left(\boldsymbol{k},\omega\right)}.
\]

\begin{prop}
Far from the sources, the fields are
\[
\boxed{\boldsymbol{B}\left(\boldsymbol{x},\omega\right)\approx i\frac{\omega}{c}\frac{e^{i\omega r/c}}{r}\hat{\boldsymbol{x}}\times\frac{1}{c}\boldsymbol{j}\left(\boldsymbol{k},\omega\right)},
\]
\[
\boxed{\boldsymbol{E}\left(\boldsymbol{x},\omega\right)\approx-\hat{\boldsymbol{x}}\times\boldsymbol{B}\left(\boldsymbol{x},\omega\right)}.
\]
 \end{prop}
\begin{rem}
The expression for $\boldsymbol{E}$ in terms of $\boldsymbol{B}$
follows instantly from the proposition in \cref{sub:ch5sec3_1Asymptotic-potentials-and}
(by taking Fourier transform).\end{rem}
\begin{proof}
\emph{(of proposition)}

Taking the temporal Fourier transform of $\boldsymbol{B}\left(\boldsymbol{x},t\right)=\nabla\times\boldsymbol{A}\left(\boldsymbol{x},t\right)$
yields

\[
\boldsymbol{B}\left(\boldsymbol{x},\omega\right)=\nabla\times\boldsymbol{A}\left(\boldsymbol{x},\omega\right)
\]
\begin{eqnarray*}
\implies\underline{B_{l}\left(\boldsymbol{x},\omega\right)} & = & \varepsilon_{lmn}\partial_{m}A_{n}\left(\boldsymbol{x},\omega\right)\\
 & = & \varepsilon_{lmn}\partial_{m}\left(\frac{1}{r}e^{ikr}\right)\frac{1}{c}j_{n}\left(\boldsymbol{k},\omega\right)\\
 & \overset{1.}{\approx} & \varepsilon_{lmn}ik\frac{e^{ikr}}{r}\hat{x}_{m}\frac{1}{c}j_{n}\left(\boldsymbol{k},\omega\right)\\
 & = & \underline{ik\frac{e^{ikr}}{r}\left(\hat{\boldsymbol{x}}\times\frac{1}{c}\boldsymbol{j}\left(\boldsymbol{k},\omega\right)\right)_{l}}.
\end{eqnarray*}

\begin{enumerate}
\item The product rule yields two terms:
\[
\underline{\partial_{m}\frac{1}{r}}=-\frac{\hat{x}_{m}}{r^{2}}+O\left(1/r^{3}\right)=\underline{O\left(1/r^{2}\right)}\dots\textnormal{ discard this term}
\]
\[
\underline{\frac{1}{r}\partial_{m}e^{ikr}}=\frac{1}{r}e^{ikr}ik\partial_{m}r=\frac{e^{ikr}}{r}ik\frac{1}{2r}2x_{m}=\underline{ik\frac{e^{ikr}}{r}\hat{x}_{m}}=O\left(1/r\right)
\]

\end{enumerate}
\end{proof}

\subsection{\label{sub:ch5sec4_3The-spectral-distribution}The spectral distribution
of the radiated energy}
\begin{thm}
The total radiated energy per solid angle $d\Omega$ and frequency
$d\omega$ is
\[
\boxed{\frac{d^{2}U}{\infinitesimal{\Omega}d\omega}=\frac{\omega^{2}}{4\pi^{2}c^{3}}\left|\hat{\boldsymbol{x}}\times\boldsymbol{j}\left(\boldsymbol{k},\omega\right)\right|^{2}}.
\]
\end{thm}
\begin{rem}
Check a static source: $\boldsymbol{j}\left(\boldsymbol{k},t\right)=\boldsymbol{j}\left(\boldsymbol{k}\right)$
\[
\implies\boldsymbol{j}\left(\boldsymbol{k},\omega\right)\propto\delta\left(\omega\right)
\]
\[
\implies\frac{d^{2}U}{\infinitesimal{\Omega}d\omega}=0
\]
\end{rem}
\begin{proof}
\emph{(of theorem)}

The instantaneous flux of energy is given by the Poynting vector (ch.
\ref{chap:Maxwell's-Equations} \cref{sub:ch2sec3_6Poynting's-theorem}):
\[
\boldsymbol{P}\left(\boldsymbol{x},t\right):=\frac{c}{4\pi}\boldsymbol{E}\left(\boldsymbol{x},t\right)\times\boldsymbol{B}\left(\boldsymbol{x},t\right).
\]
 Then the total energy $U$ radiated into a solid angle is (see \cref{sub:ch5sec3_2The-radiated-power}):
\begin{eqnarray*}
\frac{dU}{d\Omega} & = & \indefint tr^{2}\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{P}\left(\boldsymbol{x},t\right)\right)\\
 & = & r^{2}\frac{c}{4\pi}\indefint t\hat{\boldsymbol{x}}\cdot\left[\boldsymbol{E}\left(\boldsymbol{x},t\right)\times\boldsymbol{B}\left(\boldsymbol{x},t\right)\right]\\
 & = & \frac{cr^{2}}{4\pi}\indefint t\hat{\boldsymbol{x}}\cdot\left[\left(\indefintfrac{\omega}{2\pi}e^{-i\omega t}\boldsymbol{E}\left(\boldsymbol{x},\omega\right)\right)\times\left(\indefintfrac{\omega'}{2\pi}e^{-i\omega't}\boldsymbol{B}\left(\boldsymbol{x},\omega'\right)\right)\right]\\
 & \overset{1.}{=} & \frac{cr^{2}}{4\pi}\indefintfrac{\omega}{2\pi}\indefintfrac{\omega'}{2\pi}\hat{\boldsymbol{x}}\cdot\left[\boldsymbol{E}\left(\boldsymbol{x},\omega\right)\times\boldsymbol{B}\left(\boldsymbol{x},\omega'\right)\right]2\pi\delta\left(\omega+\omega'\right)\\
 & = & \frac{cr^{2}}{4\pi}\indefintfrac{\omega}{2\pi}\hat{\boldsymbol{x}}\cdot\left[\boldsymbol{E}\left(\boldsymbol{x},\omega\right)\times\boldsymbol{B}\left(\boldsymbol{x},-\omega\right)\right]\\
 & \overset{2.}{=} & -\frac{cr^{2}}{4\pi}\indefintfrac{\omega}{2\pi}\hat{\boldsymbol{x}}\cdot\left[\left(\hat{\boldsymbol{x}}\times\boldsymbol{B}\left(\boldsymbol{x},\omega\right)\right)\times\boldsymbol{B}\left(\boldsymbol{x},\omega\right)^{*}\right]\\
 & \overset{3.}{=} & \frac{cr^{2}}{4\pi}\indefintfrac{\omega}{2\pi}\left|\boldsymbol{B}\left(\boldsymbol{x},\omega\right)\right|^{2}\\
 & \overset{4.}{=} & \frac{cr^{2}}{4\pi^{2}}\defint{\omega}0{\infty}\left|\boldsymbol{B}\left(\boldsymbol{x},\omega\right)\right|^{2}\\
 & \overset{5.}{=} & \frac{1}{4\pi^{2}c^{3}}\defint{\omega}0{\infty}\omega^{2}\left|\hat{\boldsymbol{x}}\times\boldsymbol{j}\left(\boldsymbol{k},\omega\right)\right|^{2}
\end{eqnarray*}

\begin{enumerate}
\item $\indefint te^{-i\left(\omega+\omega'\right)t}=2\pi\delta\left(\omega+\omega'\right)$.
\item Since $B_{i}\left(\boldsymbol{x},t\right)\in\mathbb{R}$, $\boldsymbol{B}\left(\boldsymbol{x},-\omega\right)=\boldsymbol{B}\left(\boldsymbol{x},\omega\right)^{*}$.
Also, by \cref{sub:ch5sec4_2Asymptotic-potentials-and}, $\boldsymbol{E}\left(\boldsymbol{x},\omega\right)\approx-\hat{\boldsymbol{x}}\times\boldsymbol{B}\left(\boldsymbol{x},\omega\right)$.
\item Since $\hat{\boldsymbol{x}}\perp\boldsymbol{B}$.
\item Since integrand is even in $\omega$.
\item From \cref{sub:ch5sec4_2Asymptotic-potentials-and} proposition, $\boldsymbol{B}\left(\boldsymbol{x},\omega\right)\approx i\frac{\omega}{c}\frac{e^{i\omega r/c}}{r}\hat{\boldsymbol{x}}\times\frac{1}{c}\boldsymbol{j}\left(\boldsymbol{k},\omega\right)$.
\end{enumerate}
\[
\implies\frac{d^{2}U}{\infinitesimal{\Omega}d\omega}=\frac{\omega^{2}}{4\pi^{2}c^{3}}\left|\hat{\boldsymbol{x}}\times\boldsymbol{j}\left(\boldsymbol{k},\omega\right)\right|^{2}.
\]

\end{proof}

\subsection{Spectral distribution for dipole radiation}

From \cref{sub:ch5sec4_3The-spectral-distribution}, $\frac{d^{2}U}{\infinitesimal{\Omega}d\omega}$
is given by the Fourier transform of the current density:
\[
\boldsymbol{j}\left(\boldsymbol{k},\omega\right)\textnormal{, where }k=\left|\boldsymbol{k}\right|=\frac{\omega}{c}=\frac{2\pi}{\lambda},
\]
 with $\lambda$ the wavelength of the radiation.

Consider small sources in the limit that $\left|\boldsymbol{y}\right|\ll\lambda$.
\begin{example}
For an atom radiating visible light, we have 
\begin{eqnarray*}
\left|\boldsymbol{y}\right| & \lesssim & \textnormal{a few }\si{\angstrom}\\
\lambda & \approx & \textnormal{thousands of }\si{\angstrom}
\end{eqnarray*}

\end{example}
In this limit,
\begin{eqnarray*}
\underline{\boldsymbol{j}\left(\boldsymbol{k},\omega\right)} & = & \indefint{\boldsymbol{y}}e^{-i\boldsymbol{k}\cdot\boldsymbol{y}}\indefint te^{i\omega t}\boldsymbol{j}\left(\boldsymbol{y},t\right)\\
 & \overset{1.}{=} & \indefint{\boldsymbol{y}}\left[1-i\boldsymbol{k}\cdot\boldsymbol{y}+\dots\right]\indefint te^{i\omega t}\boldsymbol{j}\left(\boldsymbol{y},t\right)\\
 & \overset{2.}{=} & \indefint te^{i\omega t}\indefint{\boldsymbol{y}}\boldsymbol{j}\left(\boldsymbol{y},t\right)+O\left(a/\lambda\right)\\
 & \overset{3.}{=} & \indefint te^{i\omega t}\frac{d}{dt}\boldsymbol{d}\left(t\right)+O\left(a/\lambda\right)\\
 & = & \underline{-i\omega\boldsymbol{d}\left(\omega\right)+O\left(a/\lambda\right)}
\end{eqnarray*}

\begin{enumerate}
\item Taylor expand $e^{-i\boldsymbol{k}\cdot\boldsymbol{y}}$.
\item Define $a:=\left|\boldsymbol{y}\right|$.
\item By \cref{sub:ch5sec3_4Dipole-radiation} lemma, $\frac{d}{dt}\boldsymbol{d}\left(t\right)=\indefint{\boldsymbol{y}}\boldsymbol{j}\left(\boldsymbol{y},t\right)$.\end{enumerate}
\begin{prop}
If $a$ is the linear dimension of the source, and $\lambda$ the
wavelength of the radiation, then to lowest order in $a/\lambda\ll1$
the energy radiated per unit solid angle and unit frequency is given
by the \emph{Larmor formula}: 
\[
\boxed{\frac{d^{2}U}{\infinitesimal{\Omega}d\omega}=\frac{\omega^{2}}{4\pi^{2}c^{3}}\sin^{2}\theta\left|\dot{\boldsymbol{d}}\left(\omega\right)\right|^{2}},
\]
 where $\theta$ is the angle between $\boldsymbol{d}$, $\hat{\boldsymbol{x}}$
\[
\theta=\varangle\left(\boldsymbol{d},\hat{\boldsymbol{x}}\right)
\]
 and $\dot{\boldsymbol{d}}\left(\omega\right)$ is the Fourier transform
of $\dot{\boldsymbol{d}}\left(t\right)$. That is,
\[
\dot{\boldsymbol{d}}\left(\omega\right):=-i\omega\boldsymbol{d}\left(\omega\right)=\mathcal{F}_{t}\left[\dot{\boldsymbol{d}}\left(t\right)\right]\left(\omega\right).
\]
\end{prop}
\begin{proof}
In the dipole approximation, $\boldsymbol{d}\parallel\boldsymbol{j}\implies\left|\hat{\boldsymbol{x}}\times\boldsymbol{j}\right|^{2}=\sin^{2}\theta\left|\boldsymbol{j}\right|^{2}$.\end{proof}
\begin{cor}
The total energy per unit frequency is
\[
\boxed{\frac{dU}{d\omega}=\frac{2}{3}\frac{\omega^{2}}{\pi c^{3}}\left|\dot{\boldsymbol{d}}\left(\omega\right)\right|^{2}}.
\]
\end{cor}
\begin{proof}
$\indefint{\Omega}\sin^{2}\theta=2\pi\defint{\eta}{-1}1\left(1-\eta^{2}\right)=4\pi\left(\frac{2}{3}\right)=\frac{8\pi}{3}$.\end{proof}
\begin{example}
Consider a point charge $e$ on trajectory $\boldsymbol{y}\left(t\right)$
with velocity $\boldsymbol{v}\left(t\right)=\dot{\boldsymbol{y}}\left(t\right)\ll c$.
\[
\implies\boldsymbol{j}\left(\boldsymbol{x},t\right)=e\boldsymbol{v}\left(t\right)\delta\left(\boldsymbol{x}-\boldsymbol{y}\left(t\right)\right)
\]
\[
\implies\dot{\boldsymbol{d}}\left(t\right)=\indefint{\boldsymbol{x}}\boldsymbol{j}\left(\boldsymbol{x},t\right)=e\boldsymbol{v}\left(t\right)
\]
\[
\implies\dot{\boldsymbol{d}}\left(\omega\right)=\mathcal{F}\left[e\boldsymbol{v}\left(t\right)\right]\left(\omega\right)=e\boldsymbol{v}\left(\omega\right)
\]
\begin{eqnarray*}
\implies\underline{\frac{dU}{d\omega}} & = & \frac{2}{3}\frac{\omega^{2}e^{2}}{\pi c^{3}}\left|\boldsymbol{v}\left(\omega\right)\right|^{2}\\
 & = & \underline{\frac{2}{3}\frac{e^{2}}{\pi c^{3}}\left|\dot{\boldsymbol{v}}\left(\omega\right)\right|^{2}}
\end{eqnarray*}
$dU/d\omega$ is given by the Fourier transform of the acceleration.\end{example}
\begin{rem}
This is consistent with the Larmor formula from \cref{sub:ch5sec3_3Radiation-by-an}
(see Problem \#52).
\end{rem}
~
\begin{example}
Consider a slowly moving charge $\left(v\ll c\right)$ on a circle.

$\implies\dot{\boldsymbol{v}}$ is purely radial

$\implies$ power is maximal in the direction perpendicular to $\dot{\boldsymbol{v}}$
$\left(\theta=\pm\frac{\pi}{2}\right)$

$\implies$ \emph{no} radiation emitted in direction of $\dot{\boldsymbol{v}}$
$\left(\theta=0\right)$

$\implies$ in the orbital plane, the radiation has a butterfly shape

$\implies$ in 3-D, it has the shape of a torus
\end{example}

\subsection{Example: radiation by a damped harmonic oscillator}

Consider a charge $e$ in a harmonic potential (oscillator frequency
$\omega_{0}$) with damping constant $\gamma$.
\begin{description}
\item [{equation~of~motion:}] 
\[
\boxed{\ddot{y}=-\omega_{0}^{2}y-\gamma\dot{y}}\tag{\ensuremath{\ast}}
\]
\end{description}
\begin{rem}
We think of the damping as due to the radiation emitted.
\end{rem}
~
\begin{rem}
This is a simple model for an electron in a classical atom.\end{rem}
\begin{description}
\item [{initial~conditions:}] 
\[
y\left(t=0\right)=a,\quad\dot{y}\left(t=0\right)=0.
\]
\end{description}
\begin{lem}
For weak damping $\left(\gamma\ll\omega_{0}\right)$, the solution
of $\left(*\right)$ is
\[
\boxed{y\left(t\right)\approx a\cos\left(\omega_{0}t\right)e^{-\gamma t/2}}\quad\left(t>0\right).
\]
\end{lem}
\begin{proof}
See Problem \#53.
\end{proof}
\[
\implies\dot{y}\left(t\right)=-a\omega_{0}\sin\left(\omega_{0}t\right)e^{-\gamma t/2}\left[1+O\left(\gamma/\omega_{0}\right)\right]=:v\left(t\right)
\]
\begin{eqnarray*}
\implies v\left(\omega\right) & \approx & -a\omega_{0}\defint t0{\infty}e^{i\omega t}\sin\left(\omega_{0}t\right)e^{-\gamma t/2}\\
 & = & -\frac{a\omega_{0}}{2i}\defint t0{\infty}e^{i\omega t}\left[e^{i\omega_{0}t-\gamma t/2}-e^{-i\omega_{0}t-\gamma t/2}\right]\\
 & = & -\frac{a\omega_{0}}{2i}\left[\frac{-1}{i\left(\omega+\omega_{0}\right)-\gamma/2}-\frac{-1}{i\left(\omega-\omega_{0}\right)-\gamma/2}\right]\\
 & = & \frac{a\omega_{0}}{2}\left[\frac{1}{\omega-\omega_{0}+i\gamma/2}-\frac{1}{\omega+\omega_{0}+i\gamma/2}\right]
\end{eqnarray*}
Let $\omega>0$ (discussion for $\omega<0$ is analogous). Then $v\left(\omega\right)$
is dominated by the first term when $\omega\approx\omega_{0}$. 
\[
\implies\left|v\left(\omega\right)\right|^{2}\approx\frac{a^{2}\omega_{0}^{2}}{4}\frac{1}{\left(\omega-\omega_{0}\right)^{2}+\gamma^{2}/4}
\]
\begin{eqnarray*}
\implies\underline{\frac{dU}{d\omega}} & = & \frac{2e^{2}}{3\pi c^{3}}\left|\dot{\boldsymbol{v}}\left(\omega\right)\right|^{2}\\
 & = & \frac{2e^{2}}{3\pi c^{3}}\frac{a^{2}\omega_{0}^{2}}{4}\frac{\omega^{2}}{\left(\omega-\omega_{0}\right)^{2}+\gamma^{2}/4}\\
 & \approx & \underline{\frac{2e^{2}}{3\pi c^{3}}\frac{a^{2}\omega_{0}^{4}}{4}\frac{1}{\left(\omega-\omega_{0}\right)^{2}+\gamma^{2}/4}}\quad\left(\omega\approx\omega_{0}\right).
\end{eqnarray*}
 This is sometimes called \emph{susceptibility of oscillator}.
\begin{description}
\item [{discussion~(1):}] Spectrum is a Lorentzian centered on $\omega_{0}$
with width $\gamma$.
\item [{discussion~(2):}] Total energy radiated:
\begin{eqnarray*}
\underline{U} & = & 2\defint{\omega}0{\infty}\frac{dU}{d\omega}\\
 & \approx & 2\frac{e^{2}a^{2}\omega_{0}^{4}}{6\pi c^{3}}\defint{\omega}0{\infty}\frac{1}{\left(\omega-\omega_{0}\right)^{2}+\gamma^{2}/4}\\
 & \overset{1.}{=} & \frac{e^{2}a^{2}\omega_{0}^{4}}{3\pi c^{3}}\defint{\omega}{-\omega_{0}}{\infty}\frac{1}{\omega^{2}+\gamma^{2}/4}\\
 & \overset{2.}{=} & \frac{e^{2}a^{2}\omega_{0}^{4}}{3\pi c^{3}}\left(\frac{2}{\gamma}\right)\defint x{-\frac{2}{\gamma}\omega_{0}}{\infty}\frac{1}{x^{2}+1}\\
 & \overset{3.}{\approx} & \frac{2e^{2}a^{2}\omega_{0}^{4}}{3\pi c^{3}\gamma}\understuff{\defint x{-\infty}{\infty}\frac{1}{x^{2}+1}}{\pi}\\
 & = & \underline{\frac{2e^{2}a^{2}\omega_{0}^{4}}{3c^{3}\gamma}}.
\end{eqnarray*}
 Let's compare with initial oscillator energy:
\[
U_{\textnormal{osc}}^{t=0}=\frac{m}{2}\omega_{0}^{2}a^{2}
\]
\begin{eqnarray*}
\implies U & = & \frac{U_{\textnormal{osc}}^{t=0}}{\frac{m}{2}\omega_{0}^{2}a^{2}}\frac{2e^{2}a^{2}\omega_{0}^{4}}{3c^{3}\gamma}\\
 & = & U_{\textnormal{osc}}^{t=0}\frac{4e^{2}\omega_{0}^{2}}{3mc^{3}\gamma}.
\end{eqnarray*}
 Now, assuming the oscillator energy has totally gone into $U$, $\implies U=U_{\textnormal{osc}}^{t=0}$
\[
\implies\boxed{\gamma=\frac{4}{3}\frac{e^{2}\omega_{0}^{2}}{mc^{3}}}.
\]

\item [{discussion~(3):}] Compare this result with Problem \#47:
\[
\implies U_{\textnormal{osc}}=U_{\textnormal{osc}}^{t=0}e^{-t/\tau}
\]
 where we found $\tau=2/\gamma$. So the two approaches are consistent.
\item [{discussion~(4):}] See Problem \#53 for a more thorough discussion
of the approximations made above.
\end{description}

\section{Cherenkov radiation}


\subsection{\label{sub:ch5sec5_1The-time-Wigner-function,}The time-Wigner function,
and the macroscopic power spectrum}

From \cref{sub:ch5sec4_3The-spectral-distribution}, the spectral
distribution of radiation from a time-dependent current density:
\begin{eqnarray*}
\frac{d^{2}U}{\infinitesimal{\Omega}d\omega} & = & \frac{\omega^{2}}{4\pi^{2}c^{3}}\left|\hat{\boldsymbol{x}}\times\boldsymbol{j}\left(\boldsymbol{k},\omega\right)\right|^{2}\\
 & = & \frac{\omega^{2}}{4\pi^{2}c^{3}}\left(\hat{\boldsymbol{x}}\times\indefint te^{+i\omega t}\boldsymbol{j}\left(\boldsymbol{k},t\right)\right)\cdot\left(\hat{\boldsymbol{x}}\times\indefint{t'}e^{-i\omega t'}\boldsymbol{j}\left(\boldsymbol{k},t'\right)^{*}\right)\\
 & = & \frac{\omega^{2}}{4\pi^{2}c^{3}}\varepsilon_{ijk}\hat{x}_{j}\varepsilon_{ilm}\hat{x}_{l}\indefint t\indefint{t'}e^{i\omega\left(t-t'\right)}j_{k}\left(\boldsymbol{k},t\right)j_{m}\left(\boldsymbol{k},t'\right)^{*}
\end{eqnarray*}
We can rewrite the integrals using the substitutions
\begin{eqnarray*}
t & = & T+\frac{\tau}{2}\\
t' & = & T-\frac{\tau}{2}
\end{eqnarray*}
\begin{eqnarray*}
\implies\indefint t\indefint{t'}e^{i\omega\left(t-t'\right)}j_{k}\left(\boldsymbol{k},t\right)j_{m}\left(\boldsymbol{k},t'\right)^{*} & = & \indefint T\indefint{\tau}e^{i\omega\tau}j_{k}\left(\boldsymbol{k},T+\frac{\tau}{2}\right)j_{m}\left(\boldsymbol{k},T-\frac{\tau}{2}\right)^{*}\\
 & = & \indefint T\indefint{\tau}e^{i\omega\tau}W_{km}\left(\boldsymbol{k};T,\tau\right),
\end{eqnarray*}
 where 
\[
\underline{W_{km}:=j_{k}\left(\boldsymbol{k},T+\frac{\tau}{2}\right)j_{m}\left(\boldsymbol{k},T-\frac{\tau}{2}\right)^{*}}.
\]

\begin{rem}
$W_{km}$ is an example of what is called a Wigner function (in our
case a time-Wigner function). It separates the two times into \emph{average
time} (or \emph{macroscopic time}) $T$ and \emph{relative time} (or
\emph{microscopic time}) $\tau$.
\end{rem}
~
\begin{rem}
Only relative times $\left|\tau\right|\apprle1/\omega$ will appreciably
contribute to the $\tau-$integral, whereas all times $T$ during
which the source is active contribute to the $T-$integral.
\end{rem}
~
\begin{rem}
This makes sense if the two time-scales are well separated. E.g.,
a laser pulse of duration $T\gg1/\omega$.\end{rem}
\begin{defn}
The spectral distribution at time $T$ is 
\[
\boxed{\frac{d^{2}\mathscr{P}\left(T\right)}{\infinitesimal{\Omega}d\omega}:=\frac{\omega^{2}}{4\pi^{2}c^{3}}\varepsilon_{ijk}\hat{x}_{j}\varepsilon_{ilm}\hat{x}_{l}\indefint{\tau}e^{i\omega\tau}W_{km}\left(\boldsymbol{k};T,\tau\right)},
\]
called the \emph{macroscopic power spectrum}.\end{defn}
\begin{rem}
We recover $\frac{d^{2}U}{\infinitesimal{\Omega}d\omega}$ as $\frac{d^{2}U}{\infinitesimal{\Omega}d\omega}=\indefint T\frac{d^{2}\mathscr{P}\left(T\right)}{\infinitesimal{\Omega}d\omega}$.
\end{rem}

\subsection{\label{sub:ch5sec5_2Cherenkov-radiation}Cherenkov radiation}

Consider a point particle as in \cref{sub:ch5sec3_3Radiation-by-an}:
\[
\boldsymbol{j}\left(\boldsymbol{y},t\right)=e\boldsymbol{v}\left(t\right)\delta\left(\boldsymbol{y}-\boldsymbol{R}\left(t\right)\right),\textnormal{ where }\boldsymbol{v}\left(t\right):=\dot{\boldsymbol{R}}\left(t\right).
\]
 We specialize to uniform motion along a straight line:
\[
\boldsymbol{R}\left(t\right)=\boldsymbol{v}t,\quad\boldsymbol{v}\left(t\right)=\boldsymbol{v}=\textnormal{const.}
\]

\begin{rem}
We know that in vacuum this does not result in radiation.
\end{rem}
\begin{eqnarray*}
\implies\boldsymbol{j}\left(\boldsymbol{k},t\right) & = & \indefint{\boldsymbol{y}}e^{-i\boldsymbol{k}\cdot\boldsymbol{y}}e\boldsymbol{v}\delta\left(\boldsymbol{y}-\boldsymbol{v}t\right)\\
 & = & e\boldsymbol{v}e^{-i\boldsymbol{k}\cdot\boldsymbol{v}t}\\
 & = & e\boldsymbol{v}e^{-i\hat{\boldsymbol{x}}\cdot\boldsymbol{v}\frac{\omega}{c}t}
\end{eqnarray*}
\begin{eqnarray*}
\implies\underline{W_{km}\left(\boldsymbol{k};T,\tau\right)} & = & j_{k}\left(\boldsymbol{k},T+\frac{\tau}{2}\right)j_{m}\left(\boldsymbol{k},T-\frac{\tau}{2}\right)^{*}\\
 & = & e^{2}v_{k}v_{m}e^{-i\hat{\boldsymbol{x}}\cdot\boldsymbol{v}\frac{\omega}{c}\left(T+\frac{\tau}{2}\right)}e^{+i\hat{\boldsymbol{x}}\cdot\boldsymbol{v}\frac{\omega}{c}\left(T-\frac{\tau}{2}\right)}\\
 & = & \underline{e^{2}v_{k}v_{m}e^{-i\hat{\boldsymbol{x}}\cdot\boldsymbol{v}\frac{\omega}{c}\tau}}.
\end{eqnarray*}

\begin{rem}
\label{rem:ch5sec5_2The-Wigner-function}The Wigner function is independent
of $T$ here, as expected from uniform motion.
\end{rem}
\begin{eqnarray*}
\implies\underline{\frac{d^{2}\mathscr{P}\left(T\right)}{\infinitesimal{\Omega}d\omega}} & := & \frac{\omega^{2}}{4\pi^{2}c^{3}}\varepsilon_{ijk}\hat{x}_{j}\varepsilon_{ilm}\hat{x}_{l}\indefint{\tau}e^{i\omega\tau}W_{km}\left(\boldsymbol{k};T,\tau\right)\\
 & = & \frac{\omega^{2}e^{2}}{4\pi^{2}c^{3}}\varepsilon_{ijk}\varepsilon_{ilm}\hat{x}_{j}\hat{x}_{l}v_{k}v_{m}\indefint{\tau}e^{i\omega\tau}e^{-i\hat{\boldsymbol{x}}\cdot\boldsymbol{v}\frac{\omega}{c}\tau}\\
 & \overset{1.}{=} & \frac{\omega^{2}e^{2}}{4\pi^{2}c^{3}}v^{2}\sin^{2}\theta\indefint{\tau}e^{i\omega\left(1-\frac{v}{c}\cos\theta\right)\tau}\\
 & \overset{2.}{=} & \frac{\omega^{2}e^{2}}{4\pi^{2}c}\left(\frac{v}{c}\right)^{2}\sin^{2}\theta\delta\left(\omega\left(1-\frac{v}{c}\cos\theta\right)\right)\\
 & = & \underline{\frac{\left|\omega\right|e^{2}}{4\pi^{2}c}\left(\frac{v}{c}\right)^{2}\sin^{2}\theta\delta\left(1-\frac{v}{c}\cos\theta\right)}
\end{eqnarray*}

\begin{enumerate}
\item Defining $\theta$ to be the angle between $\hat{\boldsymbol{x}}$
and $\boldsymbol{v}$:
\begin{eqnarray*}
\varepsilon_{ijk}\varepsilon_{ilm}\hat{x}_{j}\hat{x}_{l}v_{k}v_{m} & = & \left(\delta_{jl}\delta_{km}-\delta_{jm}\delta_{kl}\right)\hat{x}_{j}\hat{x}_{l}v_{k}v_{m}\\
 & = & \boldsymbol{v}^{2}-\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{v}\right)^{2}\\
 & = & v^{2}\sin^{2}\theta
\end{eqnarray*}

\item $\indefint{\tau}e^{i\omega\left(1-\frac{v}{c}\cos\theta\right)\tau}=\delta\left(\omega\left(1-\frac{v}{c}\cos\theta\right)\right)$\end{enumerate}
\begin{rem}
If $v/c<1$, $\left|\cos\theta\right|<1\implies1+\frac{v}{c}\cos\theta>0$

$\implies$ no radiation, in agreement with \cref{sub:ch5sec3_3Radiation-by-an}
unless $v>c$ (``tachyonic particle'').
\end{rem}
~
\begin{rem}
In matter, $c\rightarrow c/n$, with $n$ the index of refraction
\[
\implies\frac{v}{c}\rightarrow n\frac{v}{c}\implies n\frac{v}{c}>1\textnormal{ is possible!}
\]

\end{rem}
~
\begin{rem}
Strictly speaking, this requires a theory for electromagnetic radiation
in matter. Here we assume $c\rightarrow c/n$ and $e^{2}\rightarrow e^{2}/n^{2}$
suffices to catch the main effects (the charge is screened; $F_{\textnormal{coulomb}}=e^{2}/r\rightarrow e^{2}/\left(\varepsilon r\right)$
where $n=\sqrt{\varepsilon}$). 

Also keep in mind we are applying a nonrelativistic approximation
to a situation in which $v/c$ is no longer small (see Problem \#54). 
\end{rem}
~
\begin{rem}
$n$ is frequency dependent $\left(n=n\left(\omega\right)\right)$

\begin{eqnarray*}
\implies\underline{\frac{d^{2}\mathscr{P}\left(T\right)}{\infinitesimal{\Omega}d\omega}} & = & \frac{\left|\omega\right|e^{2}/n^{2}}{4\pi^{2}c/n}\left(n\frac{v}{c}\right)^{2}\sin^{2}\theta\delta\left(1-n\frac{v}{c}\cos\theta\right)\\
 & = & \underline{\frac{\left|\omega\right|e^{2}}{4\pi^{2}cn\left(\omega\right)}\left(n\left(\omega\right)\frac{v}{c}\right)^{2}\sin^{2}\theta\delta\left(1-n\left(\omega\right)\frac{v}{c}\cos\theta\right)}.
\end{eqnarray*}
\end{rem}
\begin{description}
\item [{conclusion:}] a particle moving in a medium faster than the speed
of light in that medium emits radiation (\emph{Cherenkov radiation})
on a cone with angle $\theta$ where
\[
\cos\theta=\frac{c}{vn\left(\omega\right)}.
\]
\end{description}
\begin{prop}
The total power emitted is 
\[
\boxed{\frac{d\mathscr{P}}{d\omega}=\left|\omega\right|\frac{e^{2}v}{2\pi c^{2}}\left(1-\frac{c^{2}}{n^{2}\left(\omega\right)v^{2}}\right)}.
\]
\end{prop}
\begin{proof}
\begin{eqnarray*}
\frac{d\mathscr{P}}{d\omega} & = & \indefint{\Omega}\frac{d^{2}\mathscr{P}}{\infinitesimal{\Omega}d\omega}\\
 & = & \frac{\left|\omega\right|e^{2}}{4\pi^{2}cn}\left(n\frac{v}{c}\right)^{2}\left(2\pi\right)\defint{\eta}{-1}1\left(1-\eta^{2}\right)\delta\left(1-n\frac{v}{c}\eta\right)\\
 & = & \frac{\left|\omega\right|e^{2}}{2\pi cn}\left(n\frac{v}{c}\right)^{2}\frac{c}{nv}\defint{\eta}{-1}1\left(1-\eta^{2}\right)\delta\left(\eta-\frac{c}{vn}\right)\\
 & = & \frac{\left|\omega\right|e^{2}v}{2\pi c^{2}}\left(1-\left(\frac{c}{nv}\right)^{2}\right).
\end{eqnarray*}
\end{proof}
\begin{rem}
This is nonzero only for the range of frequencies (if they exist)
such that $vn\left(\omega\right)>c$.

$\implies$ total radiated power, $\mathscr{P}=\indefint{\omega}\frac{d\mathscr{P}}{d\omega}$
is finite.
\end{rem}
~
\begin{rem}
This is radiated energy per time and frequency, whereas a Cherenkov
radiation detector observes the energy radiated per distance traveled
by the particle. Now,
\begin{eqnarray*}
\mathscr{P}=\frac{dE}{dt} & = & \indefint{\omega}\frac{d\mathscr{P}}{d\omega}\\
 & = & \frac{e^{2}v}{2\pi c^{2}}2\defint{\omega}0{\infty}\omega\left(1-\frac{c^{2}}{n^{2}\left(\omega\right)v^{2}}\right)\Theta\left(\frac{c^{2}}{n^{2}\left(\omega\right)v^{2}}<1\right),
\end{eqnarray*}
 where $\Theta\left(\frac{c^{2}}{n^{2}\left(\omega\right)v^{2}}<1\right)$
is the theta function.
\[
\implies\underline{\frac{dE}{dx}}=\frac{dE}{dt}\frac{dt}{dx}=\underline{\frac{e^{2}}{\pi c^{2}}\defint{\omega}0{\infty}\omega\left(1-\frac{c^{2}}{n^{2}\left(\omega\right)v^{2}}\right)\Theta\left(\frac{c^{2}}{n^{2}\left(\omega\right)v^{2}}<1\right)}.
\]

\end{rem}
~
\begin{rem}
Each photon has energy $E=\hbar\omega$

$\implies$ the number of photons per distance and frequency is 
\begin{eqnarray*}
\underline{\frac{d^{2}N}{\infinitesimal xd\omega}} & = & \frac{e^{2}}{\hbar\pi c^{2}}\left(1-\frac{c^{2}}{n^{2}\left(\omega\right)v^{2}}\right)\Theta\left(\frac{c^{2}}{n^{2}\left(\omega\right)v^{2}}<1\right)\\
 & = & \underline{\frac{\alpha}{\pi c}\left(1-\frac{c^{2}}{n^{2}\left(\omega\right)v^{2}}\right)\Theta\left(\frac{c^{2}}{n^{2}\left(\omega\right)v^{2}}<1\right)},
\end{eqnarray*}
 with $\alpha:=\frac{e^{2}}{\hbar c}\approx\frac{1}{137}$ the \emph{fine
structure constant.}
\end{rem}

\section{Synchrotron radiation}
\begin{description}
\item [{idea:}] Discuss motion of charged particle in a homogeneous $B-$field,
as in Problem \#46, but\end{description}
\begin{itemize}
\item do it relativistically
\item discuss the power spectrum
\end{itemize}

\subsection{\label{sub:ch5sec6_1Relativistic-motion-of}Relativistic motion of
a charged particle in a $B-$field}

From PHYS611,
\[
\boxed{\frac{d\boldsymbol{p}}{dt}=\frac{e}{c}\boldsymbol{v}\times\boldsymbol{B}},\tag{\ensuremath{\ast}}
\]
 with $\boldsymbol{p}=\gamma m\boldsymbol{v}$ the momentum $\left(\gamma:=1/\sqrt{1-\left(v/c\right)^{2}}\right)$.
\begin{rem}
$\left(\ast\right)$ holds for relativistic motion.
\end{rem}
~
\begin{rem}
Force is purely transverse $\implies$ $E=\gamma mc^{2}=\textnormal{const.}$,
and $\boldsymbol{p}=\frac{E}{c^{2}}\boldsymbol{v}$ with $E$ the
particle's energy.

$\implies$ $\left(\ast\right)$ can be written
\[
\frac{E}{c^{2}}\frac{d\boldsymbol{v}}{dt}=\frac{e}{c}\boldsymbol{v}\times\boldsymbol{B}\implies\underline{\frac{d\boldsymbol{v}}{dt}}=\frac{ec}{E}\boldsymbol{v}\times\boldsymbol{B}=\underline{-\frac{ec}{E}\boldsymbol{B}\times\boldsymbol{v}}.
\]
\end{rem}
\begin{defn}
\textbf{\emph{Larmor frequency.}} 
\[
\omega_{0}:=\frac{\left|e\right|cB}{E},
\]
 is called \emph{Larmor frequency}.\end{defn}
\begin{rem}
In nonrelativistic limit, $\omega_{0}\approx\frac{\left|e\right|cB}{mc^{2}}=\frac{\left|e\right|B}{mc},$
called \emph{cyclotron frequency}.\end{rem}
\begin{description}
\item [{initial~condition:}] $\boldsymbol{v}\perp\boldsymbol{B}\implies\boldsymbol{v}\perp\boldsymbol{B}$
for all times.
\item [{conclusion:}] particle moves on a circle perpendicular to $B-$field
of radius 
\[
\boxed{R=\frac{v}{\omega_{0}}=\frac{v}{c}\frac{E}{\left|e\right|B}},
\]
 and the momentum is related to the radius by 
\[
\boxed{p=\frac{E}{c^{2}}v=\frac{1}{c}\left|e\right|BR}.
\]

\end{description}

\subsection{The power spectrum of synchrotron radiation}

Consider motion in the $x-y$ plane with an observer at point $\boldsymbol{x}$
and $\theta=\varangle\left(\boldsymbol{x},\hat{\boldsymbol{z}}\right)$.
Choose coordinate system such that $\boldsymbol{x}=\left(x,0,z\right)$
\[
\implies\boxed{\hat{\boldsymbol{x}}=\left(\sin\theta,0,\cos\theta\right)}.
\]
and initial conditions such that $\boldsymbol{y}\left(t\right)=R\left(\cos\omega_{0}t,\sin\omega_{0}t,0\right)$
\[
\implies\boxed{\boldsymbol{v}\left(t\right)=v\left(-\sin\omega_{0}t,\cos\omega_{0}t,0\right)}\textnormal{ with }v=\omega_{0}R.
\]

\begin{description}
\item [{current~density:}] \textbf{$\boldsymbol{j}\left(\boldsymbol{y},t\right)=e\boldsymbol{v}\left(t\right)\delta\left(\boldsymbol{y}-\boldsymbol{y}\left(t\right)\right)$
\begin{eqnarray*}
\implies\underline{\boldsymbol{j}\left(\boldsymbol{k},t\right)} & = & \indefint{\boldsymbol{y}}e^{-i\boldsymbol{k}\cdot\boldsymbol{y}}e\boldsymbol{v}\left(t\right)\delta\left(\boldsymbol{y}-\boldsymbol{y}\left(t\right)\right)\\
 & = & e\boldsymbol{v}\left(t\right)e^{-i\boldsymbol{k}\cdot\boldsymbol{y}\left(t\right)}\\
 & = & \underline{e\boldsymbol{v}\left(t\right)e^{-i\frac{\omega}{c}\hat{\boldsymbol{x}}\cdot\boldsymbol{y}\left(t\right)}}.
\end{eqnarray*}
}
\item [{charge~density:}] $\rho\left(\boldsymbol{y},t\right)=e\delta\left(\boldsymbol{y}-\boldsymbol{y}\left(t\right)\right)$
\begin{eqnarray*}
\implies\underline{\rho\left(\boldsymbol{k},t\right)} & = & ee^{-i\boldsymbol{k}\cdot\boldsymbol{y}\left(t\right)}\\
 & = & \underline{ee^{-i\frac{\omega}{c}\hat{\boldsymbol{x}}\cdot\boldsymbol{y}\left(t\right)}}
\end{eqnarray*}
\end{description}
\begin{lem}
The power spectrum from \textup{\cref{sub:ch5sec5_1The-time-Wigner-function,}
}\textup{\emph{can be written 
\[
\boxed{\frac{d^{2}\mathscr{P}\left(T\right)}{\infinitesimal{\Omega}d\omega}=\frac{\omega^{2}}{4\pi^{2}c^{3}}\indefint{\tau}e^{i\omega\tau}\left[\boldsymbol{j}\left(\boldsymbol{k},T+\frac{\tau}{2}\right)\cdot\boldsymbol{j}\left(\boldsymbol{k},T-\frac{\tau}{2}\right)^{*}-c^{2}\rho\left(\boldsymbol{k},T+\frac{\tau}{2}\right)\rho\left(\boldsymbol{k},T-\frac{\tau}{2}\right)^{*}\right]}
\]
}}\end{lem}
\begin{proof}
From \cref{sub:ch5sec5_1The-time-Wigner-function,}, the integrand
(ignoring $e^{i\omega\tau}$ factor and coefficients) is 
\begin{eqnarray*}
\varepsilon_{ijk}\varepsilon_{ilm}\hat{x}_{j}\hat{x}_{l}W_{km}\left(\boldsymbol{k};T,\tau\right) & \overset{1.}{=} & \varepsilon_{ijk}\hat{x}_{j}\varepsilon_{ilm}\hat{x}_{l}j_{k}\left(\boldsymbol{k},T+\frac{\tau}{2}\right)j_{m}\left(\boldsymbol{k},T-\frac{\tau}{2}\right)^{*}\\
 & \overset{2.}{=} & \boldsymbol{j}\left(\boldsymbol{k},T+\frac{\tau}{2}\right)\cdot\boldsymbol{j}\left(\boldsymbol{k},T-\frac{\tau}{2}\right)^{*}-\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{j}\left(\boldsymbol{k},T+\frac{\tau}{2}\right)\right)\left(\hat{\boldsymbol{x}}\cdot\boldsymbol{j}\left(\boldsymbol{k},T-\frac{\tau}{2}\right)^{*}\right)\\
 & \overset{3.}{=} & \boldsymbol{j}\left(\boldsymbol{k},T+\frac{\tau}{2}\right)\cdot\boldsymbol{j}\left(\boldsymbol{k},T-\frac{\tau}{2}\right)^{*}-\left(c\rho\left(\boldsymbol{k},T+\frac{\tau}{2}\right)\right)\left(c\rho\left(\boldsymbol{k},T-\frac{\tau}{2}\right)^{*}\right)
\end{eqnarray*}

\begin{enumerate}
\item By definition (see \cref{sub:ch5sec5_1The-time-Wigner-function,}).
\item From \cref{sub:ch5sec5_2Cherenkov-radiation} \cref{rem:ch5sec5_2The-Wigner-function}. 
\item At asymptotic distances away from source, $\hat{\boldsymbol{x}}\approx\hat{\boldsymbol{k}}$.
Using $\left|\boldsymbol{k}\right|=\frac{\omega}{c}$ and continuity
eq. yields
\begin{eqnarray*}
\partial_{t}\rho\left(\boldsymbol{x},t\right) & = & -\nabla\cdot\boldsymbol{j}\left(\boldsymbol{x},t\right)\\
\overset{\mathcal{F}}{\longrightarrow}i\omega\rho\left(\boldsymbol{k},\omega\right) & = & i\frac{\omega}{c}\hat{\boldsymbol{x}}\cdot\boldsymbol{j}\left(\boldsymbol{k},\omega\right)\\
\overset{\mathcal{F}^{-1}}{\longrightarrow}c\rho\left(\boldsymbol{k},t\right) & = & \hat{\boldsymbol{x}}\cdot\boldsymbol{j}\left(\boldsymbol{k},t\right)
\end{eqnarray*}

\end{enumerate}
\end{proof}
\begin{lem}
\[
\boxed{\boldsymbol{v}\left(T+\frac{\tau}{2}\right)\cdot\boldsymbol{v}\left(T-\frac{\tau}{2}\right)=v^{2}\cos\omega_{0}\tau}
\]
\end{lem}
\begin{proof}
\begin{eqnarray*}
\frac{1}{v^{2}}\boldsymbol{v}\left(T+\frac{\tau}{2}\right)\cdot\boldsymbol{v}\left(T-\frac{\tau}{2}\right) & \overset{1.}{=} & \sin\left(\omega_{0}\left(T+\frac{\tau}{2}\right)\right)\sin\left(\omega_{0}\left(T-\frac{\tau}{2}\right)\right)+\cos\left(\omega_{0}\left(T+\frac{\tau}{2}\right)\right)\cos\left(\omega_{0}\left(T-\frac{\tau}{2}\right)\right)\\
 & \overset{2.}{=} & \cos\left(\omega_{0}\left(T+\frac{\tau}{2}\right)-\omega_{0}\left(T-\frac{\tau}{2}\right)\right)\\
 & = & \cos\omega_{0}\tau
\end{eqnarray*}

\begin{enumerate}
\item $\frac{\boldsymbol{v}\left(t\right)}{v}=\left(-\sin\omega_{0}t,\cos\omega_{0}t,0\right)$
\item Angle difference formula.
\end{enumerate}
\end{proof}
\begin{lem}
\[
\boxed{e^{\mp\frac{\omega}{c}\hat{\boldsymbol{x}}\cdot\boldsymbol{y}\left(T\pm\frac{\tau}{2}\right)}=\summation{m=-\infty}{\infty}\left(\mp i\right)^{m}e^{\mp im\omega_{0}\left(T\pm\frac{\tau}{2}\right)}J_{m}\left(\frac{\omega}{c}R\sin\theta\right)},
\]
 with $J_{m}\left(x\right)$ a Bessel function of the first kind.\end{lem}
\begin{proof}
The Bessel functions obey 
\[
e^{iz\cos\varphi}=\summation{m=-\infty}{\infty}i^{m}e^{im\varphi}J_{m}\left(z\right)
\]
 and $\hat{\boldsymbol{x}}\cdot\boldsymbol{y}\left(t\right)=R\sin\theta\cos\omega_{0}\tau$
\begin{eqnarray*}
\implies e^{\mp i\frac{\omega}{c}\hat{\boldsymbol{x}}\cdot\boldsymbol{y}\left(t\right)} & = & e^{\mp i\frac{\omega}{c}R\sin\theta\cos\omega_{0}t}\\
 & = & \summation{m=-\infty}{\infty}\left(\mp i\right)^{m}e^{\mp im\omega_{0}t}J_{m}\left(\frac{\omega}{c}R\sin\theta\right).
\end{eqnarray*}

\end{proof}
\begin{eqnarray*}
\implies\underline{\frac{d^{2}\mathscr{P}\left(T\right)}{\infinitesimal{\Omega}d\omega}} & \overset{1.}{=} & \frac{\omega^{2}}{4\pi^{2}c^{3}}\indefint{\tau}e^{i\omega\tau}\left[\boldsymbol{j}\left(\boldsymbol{k},T+\frac{\tau}{2}\right)\cdot\boldsymbol{j}\left(\boldsymbol{k},T-\frac{\tau}{2}\right)^{*}-c^{2}\rho\left(\boldsymbol{k},T+\frac{\tau}{2}\right)\rho\left(\boldsymbol{k},T-\frac{\tau}{2}\right)^{*}\right]\\
 & = & \frac{\omega^{2}e^{2}}{4\pi^{2}c^{3}}\indefint{\tau}e^{i\omega\tau}\left[\boldsymbol{v}\left(T+\frac{\tau}{2}\right)\cdot\boldsymbol{v}\left(T+\frac{\tau}{2}\right)-c^{2}\right]e^{-i\frac{\omega}{c}\hat{\boldsymbol{x}}\cdot\left[\boldsymbol{y}\left(T+\frac{\tau}{2}\right)-\boldsymbol{y}\left(T-\frac{\tau}{2}\right)\right]}\\
 & \overset{2.}{=} & \frac{\omega^{2}e^{2}}{4\pi^{2}c}\indefint{\tau}e^{i\omega\tau}\left[\frac{v^{2}}{c^{2}}\cos\omega_{0}\tau-1\right]\\
 &  & \quad\summation{m=-\infty}{\infty}\left(-i\right)^{m}e^{-im\omega_{0}\left(T+\frac{\tau}{2}\right)}J_{m}\left(\frac{\omega}{c}R\sin\theta\right)\summation{n=-\infty}{\infty}i^{n}e^{in\omega_{0}\left(T-\frac{\tau}{2}\right)}J_{n}\left(\frac{\omega}{c}R\sin\theta\right)\\
 & = & \underline{\frac{\omega^{2}e^{2}}{4\pi^{2}c}\summation{m,n=-\infty}{\infty}i^{n-m}e^{-i\left(m-n\right)\omega_{0}T}}\\
 &  & \quad\underline{\indefint{\tau}e^{i\omega\tau}\left[\frac{v^{2}}{c^{2}}\cos\omega_{0}\tau-1\right]e^{-i\left(m+n\right)\omega_{0}\tau/2}J_{m}\left(\frac{\omega}{c}R\sin\theta\right)J_{n}\left(\frac{\omega}{c}R\sin\theta\right)}
\end{eqnarray*}

\begin{enumerate}
\item Lemma 1.
\item Lemmas 2, 3.\end{enumerate}
\begin{rem}
For the macroscopic power spectrum, we are not interested in how the
emission varies on the microscopic time scale given by $1/\omega_{0}$.

$\implies$ average over one oscillation period.\end{rem}
\begin{lem}
Let $\overline{f\left(T\right)}$ be a time average over one oscillation
period. Then
\[
\boxed{\overline{e^{-i\left(m-n\right)T\omega_{0}}}=\delta_{mn}}.
\]
\end{lem}
\begin{proof}
\begin{eqnarray*}
\overline{e^{-i\left(m-n\right)T\omega_{0}}} & = & \frac{\omega_{0}}{2\pi}\defint T0{2\pi/\omega_{0}}e^{-i\left(m-n\right)\omega_{0}T}\\
 & = & \frac{1}{2\pi}\defint x0{2\pi}e^{-i\left(m-n\right)x}\\
 & = & \delta_{mn}
\end{eqnarray*}

\end{proof}
\begin{eqnarray*}
\implies\overline{\frac{d^{2}\mathscr{P}\left(T\right)}{\infinitesimal{\Omega}d\omega}} & = & \frac{\omega^{2}e^{2}}{4\pi^{2}c}\summation{m,n=-\infty}{\infty}i^{n-m}\overline{e^{-i\left(m-n\right)\omega_{0}T}}\\
 &  & \quad\indefint{\tau}e^{i\omega\tau}\left[\frac{v^{2}}{c^{2}}\cos\omega_{0}\tau-1\right]e^{-i\left(m+n\right)\omega_{0}\tau/2}J_{m}\left(\frac{\omega}{c}R\sin\theta\right)J_{n}\left(\frac{\omega}{c}R\sin\theta\right)\\
 & = & \frac{\omega^{2}e^{2}}{4\pi^{2}c}\summation{m=-\infty}{\infty}\indefint{\tau}e^{i\omega\tau}\left[\frac{v^{2}}{c^{2}}\frac{1}{2}\left(e^{i\omega_{0}\tau}+e^{-i\omega_{0}\tau}\right)-1\right]e^{-im\omega_{0}\tau}\left(J_{m}\left(\frac{\omega}{c}R\sin\theta\right)\right)^{2}\\
 & = & \frac{\omega^{2}e^{2}}{2\pi c}\summation{m=-\infty}{\infty}\left[\frac{v^{2}}{2c^{2}}\left(\delta\left(\omega-\left(m-1\right)\omega_{0}\right)+\delta\left(\omega-\left(m+1\right)\omega_{0}\right)\right)-\delta\left(\omega-m\omega_{0}\right)\right]\left(J_{m}\left(\frac{\omega}{c}R\sin\theta\right)\right)^{2}\\
 & \overset{1.}{=} & \frac{\omega^{2}e^{2}}{2\pi c}\summation{m=-\infty}{\infty}\\
 &  & \quad\left[\frac{v^{2}}{2c^{2}}\left(\left(J_{m+1}\left(\frac{\omega}{c}R\sin\theta\right)\right)^{2}+\left(J_{m-1}\left(\frac{\omega}{c}R\sin\theta\right)\right)^{2}\right)-\left(J_{m}\left(\frac{\omega}{c}R\sin\theta\right)\right)^{2}\right]\delta\left(\omega-m\omega_{0}\right)\\
 & \overset{2.}{=} & \frac{\omega^{2}e^{2}}{2\pi c}\left(\summation{m=1}{\infty}+\summation{m=-1}{-\infty}+\bcancel{\summation m{}\delta_{m0}}\right)\left[\cdots\right]\delta\left(\omega-m\omega_{0}\right)\\
 & = & \underline{\frac{\omega^{2}e^{2}}{2\pi c}\summation{m=1}{\infty}}\\
 &  & \quad\underline{\left[\frac{v^{2}}{2c^{2}}\left(\left(J_{m+1}\left(\frac{\omega}{c}R\sin\theta\right)\right)^{2}+\left(J_{m-1}\left(\frac{\omega}{c}R\sin\theta\right)\right)^{2}\right)-\left(J_{m}\left(\frac{\omega}{c}R\sin\theta\right)\right)^{2}\right]\delta\left(\omega-m\omega_{0}\right)}
\end{eqnarray*}
\[
\,\tag{\ensuremath{\ast}}
\]

\begin{enumerate}
\item Distributed the $J_{m}$ factor, shifted the sum indices so that $\delta\left(\omega-\left(m\pm1\right)\omega_{0}\right)\left(J_{m}\left(\frac{\omega}{c}R\sin\theta\right)\right)^{2}\rightarrow\delta\left(\omega-m\omega_{0}\right)\left(J_{m\mp1}\left(\frac{\omega}{c}R\sin\theta\right)\right)^{2}$.
\item The summation can be split into three summations.

\begin{enumerate}
\item The $\summation m{}\delta_{m0}$ term does not contribute since $\omega^{2}\delta\left(\omega\right)=0$.
\item $J_{-m}\left(x\right)=\left(-\right)^{m}J_{m}\left(x\right)$, so
the remaining two summations yield equivalent contributions.
\end{enumerate}
\end{enumerate}
\begin{rem}
The frequencies emitted are the Larmor frequency $\left(\omega_{0}\right)$
and all of its harmonics.
\end{rem}
\appendix

\chapter{Glossary of notation}

\begin{tabular}{ccc}
Scalars &  & \tabularnewline
\hline 
\hline 
\noalign{\vskip2mm}
$\boldsymbol{x}\cdot\boldsymbol{y}:=g\left(\boldsymbol{x},\boldsymbol{y}\right)$ & $=$ & $x_{j}y^{j}$\tabularnewline
\noalign{\vskip2mm}
\hline 
\noalign{\vskip2mm}
 &  & \tabularnewline
\noalign{\vskip2mm}
\end{tabular}~%
\begin{tabular}{ccc}
Vectors &  & \tabularnewline
\hline 
\hline 
\noalign{\vskip2mm}
$\left(\boldsymbol{x}\times\boldsymbol{y}\right)_{j}$ & $:=$ & $\varepsilon_{jkl}x^{k}y^{l}$\tabularnewline
\noalign{\vskip2mm}
\hline 
\noalign{\vskip2mm}
 &  & \tabularnewline
\noalign{\vskip2mm}
\end{tabular}~%
\begin{tabular}{ccc}
Tensors &  & \tabularnewline
\hline 
\hline 
\noalign{\vskip2mm}
$(\mathbbm{1}_{n}){}^{j}{}_{k}$ & $:=$ & $\delta_{k}^{j}$\tabularnewline
\noalign{\vskip2mm}
\hline 
\noalign{\vskip2mm}
$\left(\boldsymbol{x}\otimes\boldsymbol{y}\right)^{jk}$ & $:=$ & $x^{j}y^{k}$\tabularnewline
\noalign{\vskip2mm}
\end{tabular}~%
\begin{tabular}{ccc}
Matrices &  & \tabularnewline
\hline 
\hline 
\noalign{\vskip2mm}
$(A^{T}){}^{j}{}_{k}$ & $:=$ & $A{}_{k}{}^{j}$\tabularnewline
\noalign{\vskip2mm}
\hline 
\noalign{\vskip2mm}
 &  & \tabularnewline
\noalign{\vskip2mm}
\end{tabular}

\bigskip{}


\noindent %
\begin{tabular}{cccc}
S. fields &  &  & transforms as...\tabularnewline
\hline 
\hline 
\noalign{\vskip2mm}
$\left(\nabla\cdot\boldsymbol{v}\right)\left(\boldsymbol{x}\right)$ & $:=$ & $\partial_{j}v^{j}\left(\boldsymbol{x}\right)$ & \emph{scalar}\tabularnewline
\noalign{\vskip2mm}
\hline 
\noalign{\vskip2mm}
 &  &  & \tabularnewline
\noalign{\vskip2mm}
\hline 
\noalign{\vskip2mm}
 &  &  & \tabularnewline
\noalign{\vskip2mm}
\end{tabular}~%
\begin{tabular}{cccc}
V. fields &  &  & transforms as...\tabularnewline
\hline 
\hline 
\noalign{\vskip2mm}
$\left(\nabla f\right)_{j}\left(\boldsymbol{x}\right)$ & $:=$ & $\frac{\partial}{\partial x^{j}}f\left(\boldsymbol{x}\right)=:\delta_{j}f\left(\boldsymbol{x}\right)$ & \emph{covector}\tabularnewline
\noalign{\vskip2mm}
\hline 
\noalign{\vskip2mm}
$\left(\nabla\times\boldsymbol{v}\right)^{j}\left(\boldsymbol{x}\right)$ & $:=$ & $\varepsilon^{jkl}\partial_{k}v_{l}\left(\boldsymbol{x}\right)$ & \emph{pseudovector}\tabularnewline
\noalign{\vskip2mm}
\hline 
\noalign{\vskip2mm}
 &  &  & \tabularnewline
\noalign{\vskip2mm}
\end{tabular}


\chapter{Transformation identities}

Let $D$ be a coordinate transformation. By \cref{claim:3_2_trafo_partial},
\[
\boxed{D{}^{j}{}_{k}=\frac{\partial\tilde{x}^{j}}{\partial x^{k}},\quad\left(D^{-1}\right){}^{j}{}_{k}=\frac{\partial x^{j}}{\partial\tilde{x}^{k}}}.
\]
 In what follows, transformation identities have been tabulated for
various mathematical objects.


\section{Scalar fields}

\begin{tabular}{ccc}
$CS$ &  & $\widetilde{CS}$\tabularnewline
\hline 
\hline 
\noalign{\vskip2mm}
$\left(\nabla\cdot\boldsymbol{v}\right)\left(\boldsymbol{x}\right)=\widetilde{\left(\nabla\cdot\boldsymbol{v}\right)}\left(\tilde{\boldsymbol{x}}\right)$ &  & ``''\tabularnewline
\noalign{\vskip2mm}
\end{tabular}


\section{Vectors}

\begin{tabular}{ccc}
$CS$ &  & $\widetilde{CS}$\tabularnewline
\hline 
\hline 
\noalign{\vskip2mm}
$\boldsymbol{e}_{j}=D{}^{k}{}_{j}\tilde{\boldsymbol{e}}_{k}$ &  & $\tilde{\boldsymbol{e}}_{j}=(D^{-1}){}^{k}{}_{j}\boldsymbol{e}_{k}$\tabularnewline
\noalign{\vskip2mm}
\hline 
\noalign{\vskip2mm}
$x^{j}=(D^{-1}){}^{j}{}_{k}\tilde{x}^{k}$ &  & $\tilde{x}^{j}=D{}^{j}{}_{k}x^{k}$\tabularnewline
\noalign{\vskip2mm}
\hline 
\noalign{\vskip2mm}
$x_{j}=D{}^{k}{}_{j}\tilde{x}_{k}$ &  & $\tilde{x}_{j}=\left(D^{-1}\right){}^{k}{}_{j}x_{k}$\tabularnewline
\noalign{\vskip2mm}
\end{tabular}


\section{Vector fields}

\begin{tabular}{ccc}
$CS$ &  & $\widetilde{CS}$\tabularnewline
\hline 
\hline 
\noalign{\vskip2mm}
$\partial_{j}f\left(\boldsymbol{x}\right)=D{}^{k}{}_{j}\tilde{\partial}_{k}\tilde{f}\left(\tilde{\boldsymbol{x}}\right)$ &  & $\tilde{\partial}_{j}\tilde{f}\left(\tilde{\boldsymbol{x}}\right)=\left(D^{-1}\right){}^{k}{}_{j}\partial_{k}f\left(\boldsymbol{x}\right)$\tabularnewline
\noalign{\vskip2mm}
\hline 
\noalign{\vskip2mm}
$\left(\nabla\times\boldsymbol{v}\right)^{j}\left(\boldsymbol{x}\right)=\left(\det D\right)\left(D^{-1}\right){}^{j}{}_{k}\widetilde{\left(\nabla\times\boldsymbol{v}\right)}^{k}\left(\tilde{\boldsymbol{x}}\right)$ &  & $\widetilde{\left(\nabla\times\boldsymbol{v}\right)}^{j}\left(\tilde{\boldsymbol{x}}\right)=\left(\det D\right)D{}^{j}{}_{k}\left(\nabla\times\boldsymbol{v}\right)^{k}\left(\boldsymbol{x}\right)$\tabularnewline
\noalign{\vskip2mm}
\end{tabular}


\section{Tensors}

\begin{tabular}{ccc}
$CS$ &  & $\widetilde{CS}$\tabularnewline
\hline 
\hline 
\noalign{\vskip2mm}
$g_{jk}=D{}^{m}{}_{j}\tilde{g}_{ml}D{}^{l}{}_{k}$ &  & $\tilde{g}_{jk}=(D^{-1}){}^{m}{}_{j}g_{ml}(D^{-1}){}^{l}{}_{k}$\tabularnewline
\noalign{\vskip2mm}
\hline 
\noalign{\vskip2mm}
$\varepsilon^{jkl}=\left(\det D\right)\left(D^{-1}\right){}^{j}{}_{\alpha}\left(D^{-1}\right){}^{k}{}_{\beta}\left(D^{-1}\right){}^{l}{}_{\gamma}\varepsilon^{\alpha\beta\gamma}$ &  & $\tilde{\varepsilon}^{jkl}=\left(\det D\right)D{}^{j}{}_{\alpha}D{}^{k}{}_{\beta}D{}^{l}{}_{\gamma}\varepsilon^{\alpha\beta\gamma}$\tabularnewline
\noalign{\vskip2mm}
\end{tabular}


\chapter{Electromagnetic field tensor}

In what follows, we define
\[
\boldsymbol{E}:=\left(E^{1},E^{2},E^{3}\right)=:\left(E_{x},E_{y},E_{z}\right)\quad\textnormal{and}\quad B^{jk}:=\begin{pmatrix}0 & -B^{3} & B^{2}\\
B^{3} & 0 & -B^{1}\\
-B^{2} & B^{1} & 0
\end{pmatrix}=:\begin{pmatrix}0 & -B_{z} & B_{y}\\
B_{z} & 0 & -B_{x}\\
-B_{y} & B_{x} & 0
\end{pmatrix}.
\]
 Note the (confusing) convention that upper numerical indices correspond
to lower ``Cartesian'' indices. Also note that $B^{jk}=B_{jk}.$


\section{Covariant components $F_{\mu\nu}$}

\[F_{\mu\nu} =: 
\left(\begin{array}{c;{2pt/2pt}c}
0      		 & \boldsymbol{E}\\\hdashline[2pt/2pt]
-\boldsymbol{E} & B_{jk}\\
\end{array}\right)\\
\]


\section{Contravariant components $F^{\mu\nu}$}

\[  F^{\mu\nu}=g^{\mu\alpha}g^{\nu\beta}F_{\alpha\beta}=\begin{pmatrix}+\\ -\\ -\\ - \end{pmatrix}_{\mu}\begin{pmatrix}+\\ -\\ -\\ - \end{pmatrix}_{\nu}F_{\mu\nu} =  
\left(\begin{array}{c;{2pt/2pt}c}
0      		 & -\boldsymbol{E}\\\hdashline[2pt/2pt]
\boldsymbol{E} & B_{jk}\\
\end{array}\right)\\
\]


\section{Mixed components $F{}^{\mu}{}_{\nu}$}

\[  F{}^{\mu}{}_{\nu}=g^{\mu\alpha}F_{\alpha\nu} = 
\left(\begin{array}{c;{2pt/2pt}c}
1      		& 0\\\hdashline[2pt/2pt]
0 & -\mathbbm{1}_{3}\\
\end{array}\right)\\
\left(\begin{array}{c;{2pt/2pt}c}
0      		 & \boldsymbol{E}\\\hdashline[2pt/2pt]
-\boldsymbol{E} & B_{jk}\\
\end{array}\right)\\
 =  
\left(\begin{array}{c;{2pt/2pt}c}
0      		& \boldsymbol{E}\\\hdashline[2pt/2pt]
\boldsymbol{E} & B_{jk}\\
\end{array}\right)\\
\]


\section{Mixed components $F{}_{\mu}{}^{\nu}$}

\[  F{}_{\mu}{}^{\nu}=g_{\mu\alpha}F^{\alpha\nu} = 
\left(\begin{array}{c;{2pt/2pt}c}
1      		& 0\\\hdashline[2pt/2pt]
0 & -\mathbbm{1}_{3}\\
\end{array}\right)\\
\left(\begin{array}{c;{2pt/2pt}c}
0      		 & -\boldsymbol{E}\\\hdashline[2pt/2pt]
\boldsymbol{E} & B^{jk}\\
\end{array}\right)\\
 =  
\left(\begin{array}{c;{2pt/2pt}c}
0      		& -\boldsymbol{E}\\\hdashline[2pt/2pt]
-\boldsymbol{E} & -B_{jk}\\
\end{array}\right)\\
\]
\end{document}
